 Hi!
 What is your favorite holiday?
 one where I get to meet lots of different people.
 What was the most number of people you have ever met during a holiday?
 Hard to keep a count. Maybe 25.
 Which holiday was that?
 I think it was Australia
 Do you still talk to the people you met?
 Not really. The interactions are usually short-lived but it's fascinating to learn where people are coming from and what matters to them
 Yea, me too. I feel like God often puts strangers in front of you, and gives you an opportunity to connect with them in that moment in deeply meaningful ways. Do you ever feel like you know things about strangers without them telling you?
 what do you mean?
 I think it's like a 6th sense, often seen as "cold readings" to people, but can be remarkably accurate. I once sat next to a man in a coffee and I felt a pain in my back. I asked the stranger if he had a pain. It turns out that he did in the exact spot, and said he pulled a muscle while dancing at a party. I had never met the man before and never saw him again.
 Wow! That's interesting, borderline spooky
 There's this practice called "Treasure Hunting" that's kind of a fun game you play in a public place. There's a book called "The Ultimate Treasure Hunt" that talks about it. You use your creativity to imagine people you will meet, and you write down a description, then you associate them with a positive message or encouraging word. Maybe you saw a teenage boy in a red hat at the shopping mall in your imagination, then while at the mall, you may find someone who matches that description. You show that you have a message for him and that you have a message for a boy in a red hat. You then give him a message of kindness or whatever was on your heart. You have no idea, sometimes you meet someone who is having a really hard day, and it brings them to tears to have a stranger show them love.
 So, do you do treasure hunting often?
 I did more when I was in grad school (and had more time). I would usually go with friends. For a while I would go to the farmers market in Santa Cruz every week and try to feel if there is something I am supposed to tell a stranger. Usually, they are vague hope-filled messages, but it's weird when I blurt out something oddly specific.
 Hi
 Any plans for the weekend?
 my friends are gonna visit me this weekend. we might go hiking!
 That's great! How's the weather over the weekend? I hope its warm.
 Should be very sunny! you?
 Cool! very depressing plans ... stay home and work üòû I have a project deadline very close.
 üòê hope you get your work done very soon! a bug free weekend!
 Right, very anxious! where do you plan to go for a hike?
 I am going to Diablo!
 Nice, where is that place? I haven't been there
 hours drive from here. still in bay area
 That's cool! How long is the hike?
  Actually no idea, but it will take the entire day for that.
 nice! sounds fun!
 Hi!
 Hey there! What's up???
 Nothing much, how you doin?
 I'm in New York this week for Thanksgiving. I'm squatting in the office today and I caught up with an old friend of mine :D
 Oh wow! Sounds like fun! When was the last time you had seen this friend?
 The last time in New York, back in June.
 Ohh okay. I was going to say if it had been a long time maybe it'd be awkward...
 Haha, I guess if it's been a very long time there's almost too many life events to catch up on.. especially recently
 Oh really? Has a lot changed in your life recently?
 Haha it's probably too much to go into at the moment. Let's just say life is an exciting experience. How about you?
 Ahhh sounds exciting indeed! My life is pretty bland. I like routine, but sometimes I wish I had more time for adventures!
 What kinds of adventures?? Any ones that I would be able to join you on?
 Hmmmm. I really want to try bull riding. Do you have any interest in that?
 I'd love to try! Can we schedule something for next week?
 Sure! What does your Saturday look like?
 Saturday looks pretty good, shall we shoot for something in the morning?
 Hi!
 hey
 is it raining pretty bad today?
 yeah, can walk too far to see all the foodtruck options
 surprising that the rain started early this year... I don't like them too much. They make days gloomy
 yeah but I think it's good to have some rainy days in bay area, it's pretty dry here üòõ
 Where I grew up, we had lots of water trouble too...
 yeah like wise, I've seen a pretty bad snowstorm when I was at my undergrad school, all flights canceled and traffics went down
 Haha... I don't think I can survive in that weather ever. Just the rains at 50 degrees make me want to sit in heated rroms
 yeah how do you like it in bay area though? I think we need more rain here
 people say there is drought here... but we have 24 hours water supply here ... lol... never seen that in a drought ridden area
 it is pretty dry in the mountains I believe, that's what causes fire
 hmm.... okay. Climate change talk this morning was pretty darn interesting. did you see it?
 nope, what does it say?
 they were talking about how AI is helping climate change. Nice use of upcoming tech.
 Hi.
 Helloooooo!
 How are you? How is your day?
 Good. Don't have much to do today, feels good. How are you?
 I'm dressed very wel today so I feel good! I've been reading a lot about the psychology of positive outlook.
 So what's your outlook? Something blue?
 Yes. Blue is a tranquil colour. It's a good metaphor. Do you have good advice for positivity?
 You should drink more water, do some push up, and sleep early.
 Hi!
 Hey, how are you?
 I'm a bit sad. I miss my cat.
 Oh no‚Ä¶ Have you sent out the missing cat posters? Hope your cat is alright!
 Posters is a great idea. So far I've just tried banging her catfood dish and shouting her name. Anyway, how is your day going so far?
 Yea, I know they love the plastic bag sound all the time. I am good, nothing special though.
 If you could go anywhere on vacation, where would you go?
 I like rainforest, but I know it requires extensive training beforehand.
 I heard there are rainforests in southeast Asia where you can zipline from tree to tree.
 I am afraid I will be scared of doing this :)
 I won't lie, it sounds scary. I'm scared right now just thinking about it.
 I don't know if there is any medication for acrophobia. I want to take plenty of it if I really have to do it.
 If there isn't one, you should invent it, and then make millions
 That's a great idea! Maybe alcohol is such a thing.
 Ha! Don't drink and zipline, mate!
 Oops. I won't do it again. Ha
 Hi!
 Hey sup
 not much. any plans this weekend?
 I'm going to try that thing where you hang from a wire as you go down. do you know what is it called?
 ziplining?
 that's the one! have you ever tried it?
 i have a couple years ago. it's quite a unique experience
 where did you do it?
 i forgot where it was, it wasn't local i don't think though
 no worries. what's the most exciting thing you ever done?
 that's a hard question and i'm tired so i'm going to go. see you
 sure. are you just going home now?
 no, i'm going to get a massage first
 nice. what type?
 traditional kind
 yeah I want to get one too soon
 you should! it's relaxing after a long day. talk to you later!
 ttyl!
 Hi!
 Hello, have you seen any good movies lately?
 I watched a few lately, but nothing is as good as Avatar. what's your favorite?
 I have never seen Avatar, what is it about? I really enjoy the Avenger movies
 it's a science-fiction movie with beautiful landscape of an imaginary nature with non-human creatures. people figured out a way to join that nature through Avatar transformation. the movie ends with a meaningful story of how human behaviors, e.g., cutting trees, have affected nature
 That sounds really cool! I think that movie did really well when it was in the box office so it must be good!
 yea. what else do you like to do beside movies?
 I enjoy baking cookies. I am on a quest to bake the best chocolate chip cookie üôÇ What about you?
 I enjoy eating üôÇ
 so definitely would like to try your best chocolate cookie
 I will have to bake some soon and let you know. What types of food do you like to eat?
 thanks! I generally love noodle soups like Pho or Ramen :)
 Noodle soup is delicious! Do you make homemade noodle soup or do you prefer to go out?
 I prefer to go out. I'm not a good cook haha
 Same! Even though I bake, I cannot cook
 seems like we share a thing in common, yay!
 Hi!
 Good afternoon!
 How has your week been?
 So far so good. It is holiday season. So just chilling
 I think I'm getting sick with a cold üòû So you should chill on my behalf too cause I'm out the game for all of December.
 lol Sorry to hear that. Are you planning anything fun for December?
 Nothing exciting. I'll be posted up at home for the most part. I did a lot of travelling this year so my budget would have stopped me even if I wasn't sick.
 üòÇ
 Do you have big plans?
 Yes! I am going to Hawaii! This will be my first time visiting Hawaii. Really excited about it.
 I love Hawaii. It's a good place to be. I like going there cause it's humid so I never have to put on lotion.
 lol this is the first time I heard from a boy who cares about humidity and lotion. I cannot agree more.
 Brooooo!!! It's so important. When I got to California beaches I have to carry 3 litres of lotion for the whole day.
 üòÇ
 Hi!
 Oh hello. Long time no talk. How's the day going for yuo?
 Very well, thanks for asking. How has your day been?
 Getting better. I just recovered from a cold. I got wet in the rain last week. Are you planning anything for the holidays?
 Glad to hear you're better. Sorry to hear you were sick. I was sick a couple of weeks ago with a bad cough. There's definitely a bug going around. Admit I just want to stay healthy for the holidays and plan to relax.
 Oh same here. I think relaxing at home should be counted among the best ways to enjoy the holidays.
 Definitely! I know a lot of folks travel for the holidays, but I'm happy to stay home myself!
 I'm getting there. Every year until last year, I tried to go somewhere for the Christmas / New Year, and then I got bored traveling. lol not sure if that means I'm getting old?
 Me too. Now I have folks come visit me for the holidays! But that's also tiresome..
 Are you doing any home decorating then?
 Yes! We set up an eco-friendly (i.e. fake) Christmas tree and put up some colorful LED lights which is very festive.
 I think I'm copying you. Me and my wife plan to decorate and Christmas tree too. We bought most of the decorative stuffs from the stores, but haven't yet to buy the tree.
 Buying a tree is a neat experience. I was torn between buying an artificial/eco-friendly/fake one vs. a real one that smells like fresh pine. In the end, we opted for the one that we can disassemble every year.
 I see. Artificial anything is better, from tree to intelligence, huh?
 Oh, very clever pun! I like it! Depends. I remember having real Christmas trees from childhood, but these days with climate change, I think not chopping down a tree just to decorate it and then throw it out in a month is the more responsible thing to do.
 I see. It's probably also cheaper. I'll buy an artificial one too. Do you have any suggestions for the store?
 Admit my favorite store is Target, plus they often have good deals.
 Ah that's great. My wife also likes Target a lot. She even made a Target credit card because she comes to that store very often. Okay thanks for the suggestion. I'll check out Target.
 Great, I hope you find a nice tree.
 Hi!
 Hey
 How's your day going?
 pretty good. yours?
 Ehh it's fine. I didn't do so well on that history test, actually..
 oh what happened?
 Apparently Christopher Columbus didn't fight in the Civil War :')
 hahah wait for real?
 I know right! Are you taking History next semester?
 No I'm not in school anymore
 Oh I see. What do you do?
 I train and compete in horse vaulting
 Oh wow. Were you born a horse, or were you turned into one?
 lol you're too funny
 Just kidding. That sounds pretty cool! Is it your job?
 Yeah, but I part time work on a farm. Helping with a bit of everything
 Wow, sounds very busy! Do you with money at those horse vaulting competitions?
 Yeah some. enough to get by
 Hi!
 Hello
 Do you have a favourite flower?
 hmm, I haven't thought about that much, but i think lotus should be one of my favorites. Why do you ask?
 I'm working on a theory. Why does the lotus spring to mind?
 Nice! Lotus looks pretty cool and It has some delightful vibe. So what is this research about?
 Oh, it's not research! Just a personal theory. I think that flower preferences are more revealing of personality than people appreciate.
 Interesting! Whats your favorite flower?
 The gerbera. It's like a cartoon flower. As if you drew "flower" with a crayon and then it came to life.
 Nice, i would love know more about your theory. Like how you can deduce personality from flower preference.
 Ok, step 1 is, you ask someone what their favourite flower is. Pretty much like what we just did. Does that make sense so far?
 yes
 Cool. Step 2: talk with the person some more, and ask them some more questions, and gradually develop a sense of what they're like, over the course of maybe two to five years. And voila
 Hehe, i think you should publish this someday :)
 Why thank you, that's a wonderful idea!
 Hi!
 Hey how's it going
 It's good it's good. How are you?
 good. it's really hot today. I think I'm going to the pool
 Oh nice! Where do you live?
 I live in Tokyo, Japan
 Ahh yes, Japan is hot during the summer. Last time I was in Kyoto it was 114 degrees....
 oh have you been?
 Yes yes. I've been to Tokyo as well. It's so nice!
 what did you do here?
 Oh everything! I went to an onsen, the fish market, disney land and giant robot fighting show haha
 lol why did you come to Japan just to go to Disney land?
 The Disney lands are all different! There's also Disney Sea, which is completely unique!
 oh neat. I haven't heard about that robot fighting show. where is that??
 I don't really remember what part of town it was in. It was pretty cool though - I'm sure you can find it if you google "giant robot fighting show tokyo" haha
 lol ok
 Hi!
 Have you seen any good movies lately?
 Last weekend I saw "The Parasite." Ever heard of it?
 No. Why did you pick that movie?
 My friend wanted to see it. It has great reviews on IMDB and Rotten Tomatoes! What did you do last weekend?
 I played music and worked on some side projects. I also started watching the new Disney service.
 Oooo the Mandalorian?!?!
 Mostly, the deleted scenes from Avengers.. lol
 lol Are you a big Marvel fan?
 I loved the X-Men as a kid, and even collected the comic cards. Recently, I got very into the Marvel Cinematic Universe movies. How many Avengers movies have you seen?
 I've only seen Spiderman. Honestly it was a little too scary and so I don't think I can bring myself to watch the other Marvel movies! haha
 Oh!-- I have a friend who looks like the actor who plays Spiderman.
 Oh really? To be honest I think the actor is not that good looking, so not so surprising! haha
 Yea. I think Loki is the most handsome üòÄ
 Who is Loki? I've never heard that name before
 He's the adopted brother of Thor, God of thunder, and is burdened with glorious purpose. Do you feel that burden?
 Hi!
 Hey, what's up?
 Just chillin'. how are you?
 I'm pretty good, thanks.
 Do anything interesting today?
 I went to the local cafe and had a double espresso. It was delicious. What about you?
 Oh that's cool! I actually went to an amusement park and went on my first roller coaster!
 Oh my gosh. What was it like??
 It was scary! It was actually Kingda Ka, the world's tallest roller coaster. Ever heard of it?
 No, never heard of it. But I'm not really a coaster aficianado. I've heard that some people get addicted to them and travel the world to try them.
 Oh wow! I'm not on that level yet, but I understand the appeal. Are you an adrenaline junkie at all?
 No, the opposite. I can't stand heights, horror movies, or confined spaces.
 Same! I guess the roller coaster wasn't so bad because I trust the engineering haha
 Ha, I suppose that makes sense! Would you say that you enjoyed it?
 Maybe not so much at the time, but I am glad I did it now that it's done, know what I mean?
 I think I sort of understand :)
 Hi!
 hello there, how is it going?
 All good. Planning to head home soon. How about you?
 I'm quite tired. There are a lot of things I need to finish before the end of the year.
 oh... sorry to hear that. But after that it will be a hard earned vacation
 yeah, looking forward to it. Hope I don't get pinged during the holidays. Are you going to travel these dates?
 I have some tentative plans, but if that doesn't pan out, will just chill at home.
 staying at home is always nice during the holidays
 Where are you based out of these days?
 I'm working from LA, nice weather around here. and you?
 San Francisco. It's been raining cats and dogs here since last 2-3 weeks
 aw man, I'm sorry to hear that. at least it's not snow!
 The flu has been hitting hard as well. I had several folks in the house down at one point.
 that's really sad. are they feeling any better?
 Yes, everyone recovered now
 Hi!
 Hello
 How's it going?
 Extremely busy. I have been trying to prepare for the upcoming holidays. How about you?
 I'm going to the bahamas. Can't wait!!!
 I'm jealous, take me with you!! I would love to have some warm weather right now
 oh where are you now?
 Canada. There is another major snowstorm that might hit this weekend so I have been rushing to get everything done before it comes.
 oh no. I never seen this in person. Is it scary?
 Snow is not scary as long as you're prepared. You just need to be ready to not have electricity for a while. I enjoy the aftermath of a good snowstorm because then you can go sledding or skiing.
 that does sound nice. so what are you doing these holidays?
 I am having all of the extended family over for a big meal. We will also go <REDACTED_TERM> as well. What will you do in the bahamas?
 nice nice. I'm gonna go snorkeling yey
 Sounds fun! I wish I knew how to swim!
 You can stay on the shallow side I think. Well hope you enjoy time with your extended family!
 That's true. You too, have a great time snorkeling!
 Hi!
 Hi! How was your weekend?
 pretty good. just went to church and hangout with friends
 Nice
 did you do anything?
 I made donuts and samosas with an air fryer have you used one of those before
 yum yum yum no only good old oily frier
 haha
 do you have one at your home or were you at a friends place?
 I was at my parents' place what are you up to for Thanksgiving?
 I'm going to impersonate a pumpkin
 wow, those are unique plans
 I'm pretty unique person
 I think so too
 any other hobbies besides air frying everything? üôÇ
 I want to start fermenting things kimchi for example sounds like a fun thing to ferment takes a few days apparently miso takes a couple years to fermen
 Hi!
 heya, nice to meet you, I'm Paul
 nice to meet you too! I'm James. how are you doing today?
 I'm doing OK. Looking forwards to the weekend. how about you?
 same here! I hope the weather will be nice
 oh yeah, but I don't have my hopes too high, I heard there could be a storm coming our way
 oh no, which areas will be affected?
 they mentioned that the whole city will experience harsh weather and that people in the outskirts will probably not get much rain and wind
 uh oh, I'd better not to plan for BBQ then instead just enjoying playing board games inside üôÇ
 yeah, it'll be good weather for staying inside with a cup of hot chocolate. Too bad my street usually floods, so I'll have to check for that
 yea, you'd better check. where do you live?
 I live at the bottom of the valley, cheap area but we do get affected by this kind of stuff a lot haha
 gotcha. anything you love about where you live?
 well, the food around the area is amazing, which is definitely a plus.
 nice! I'd love to come visit that area some times
 Hi!
 hello, who am I having the pleasure to chat with üôÇ
 I am the superman! What about you?
 haha. great chating with superman, what is your power?
 Being invisible. You won't see me.
 haha. what else can you do? can you read minds?
 I would rather trust fMRI and machine learning to do this. I am not an expert on that. Sorry for it!
 wow that seems pretty technical. what does fMRI mean?
 The brain imaging thing that can tell you a brain's activity at a pretty high resolution.
 okay! so you seem to like science a lot?
 I believe in Science! Science is my god!
 Are you also doing science?
 no, I'm bad at Science. what can Science do? is it the most important thing for society?
 People are always arguing. Probably both science and democracy are both important I guess.
 Does it make sense?
 I think so. thanks for your point!
 Hello, Nice to meet you
 If you could eat only one food for the rest of time, what would it be?
 Hmm... That's a tough one. I think I would go Asian Food > Chinese Food > Stirfry. What about you?
 I think ice-cream. It may not be good for me, but I wouldn't care, haha
 I love ice cream too!
 Okay, top three flavors?
 I like vanilla more than chocolate ice cream. I typically will do any variations on vanilla. To pick from the top of my head, I would say Cookies and Cream, Mint Chocolate Chip, and Coffee. How about you?
 Ah, that's a good way of framing it. Me, I like berries: boysenberry one, strawberry two, maybe straight chocolate number 3 to mix it up a bit.
 Very nice. I love sorbet's and smoothies. Changing topics, Do you believe in an afterlife?
 Yes. I wonder if I'm in it right now. How would I know? What do you think?
 I think so. I feel there must be something more than the physical world as we understand it.
 There's a mental world, I suppose? Understanding itself
 What is the most supernatural experience you have ever had?
 I went to a seance once in college. They had a ouija board. I can't remember if we actually contacted the spirit world because I had a bit too much to drink.
 Haha.. that's a cool experience. I went to a Hindu retreat before, a number of Buddhist temples, and hung out with Christian Mystics in Santa Cruz before.
 Hi!
 Hi, how are you doing!
 I'm doing well. what are you up to?
 Yeah, typical work stuff. Check emails and 99% of the inbox. delete 99%
 wow, that's impressive. I already gave up on cleaning emails long ago
 lol doesn't it bother you at all
 yea, a little bit, but it's okay üôÇ what do you enjoy doing outside work?
 Well, movie? I watched Terminator last night. It was a nice movie
 ah cool. so you like action movies?
 Not really. But it was fun to watch with friends üôÇ It was touching at the end of the movie
 what happened there? I watched bits of Terminators movies but never a full one
 Are you sure you want the spoiler üôÇ
 haha sure. by the time, I get to it; I will forget the details, only knowing that it's touching at the end
 üòÇ well someone died at the end Or some robots, to be more accurate üòâ
 oh no, so it's not happy ending?
 The leading character is still alive and the bad robots was killed too. So I guess it is happy ending
 then I want to watch it! you didn't spoil much üôÇ
 Nice! Hope you enjoy it!
 Thanks!
 Hi!
 Hey, how are you?
 doing great! what are you looking forward to?
 thanksgiving holidays üôÇ
 yay! Turkey and shopping!
 not a big turkey fan! I find it too dry
 yea me too. I sometimes eat noodle soups in thanksgiving instead haha
 yeah, I would have noodles anyday over turkey. Not sure how the turkey tradition started
 me neither. someone told me that it depends on the stuffing inside the Turkey. some people make very good stuffing
 yeah, that and the gravy. Gravy helps make it taste better too. But apart from food, Black Friday deals are a catch. Let's see what they have this year
 yea. what do you plan to buy?
 thinking of getting a fitbit
 ah cool. so you can run more frequently?:)
 yeah, just keeping calories in check
 yay, all the best with keeping calories in check!
 Hi!
 Hi there üôÇ How's your day so far?
 doing well. what are you up to?
 busy busy! I've had back-to-back meetings all day
 same here. what do you love to do beside meetings? üôÇ
 well I've gotten really into yoga lately. I went to a class today and it was super hard
 aww .. I hope things will get less hard and you become an expert in it! I heard many great things about Yoga
 yeah the teacher seems super awesome so I will definitely keep trying what activities do you enjoy?
 ah I enjoy playing soccer and tennis üôÇ unfortunately, winter is not the best time for those üôÇ
 oh that's too bad. Is it hard to find a place to play soccer or tennis indoors?
 yea. I enjoy playing outside though, just a little cold üôÇ what else do you do beside Yoga?
 I also like to sing, I perform with a group sometimes. Do you like music?
 yea definitely! I love singing Karaoke. my wife is a pianist üôÇ
 haha that's awesome! Karaoke is really fun, I do it with my friends sometimes
 awesome. glad to find something in common!
 Hi!
 hey there
 hey anything new?
 not too much. just really looking forward to the holidays!
 any plans?
 yes! I'm going to Mexico and I couldn't be more excited
 that's awesome! I never been!
 Oh man I would highly recommend it
 Are you a food person, a sightseeing person or neither?
 that's a great question, and I'm definitely both. this trip will mostly be about food though, and relaxing
 I'm a food person I think. Any specific foods you're planning on trying?
 there's a taco place that I've visited before that I can't wait to go back to. Do you like tacos?
 yeah my favorite is taco fish
 ooh that is a good choice. Have you ever made them yourself?
 no. only eat them üòÄ
 Hi!
 Hey, how are you üôÇ
 I am good. How are you? ÔºöÔºâ
 Doing well, lot of work though. How was your day?
 I am busy. A lot of work. What are you working on?
 Just reading latest research. There is so much to cover. how about you?
 I am working on a new classifier.
 ohh, interesting! What kind of classifier
 A new classifier for hate speech. Which research topic catches your eyes most?
 You're so cool. Making world a better place. I'm mostly into NLP. What do you do when not making classifiers?
 Nice! Do you refer to work or anything else?
 Anything in general. You're so cool, I want to know more about you üôÇ
 You are very cool too!!
 Hi!
 hello there! who are you?
 I'm mark. I work in accounting
 Nice to meet you Mark, I'm Tom and I work as a fish groomer.
 what does a fish groomer do?
 well, we take care of people's fish. Make sure they are happy, polish their scales, clean their tanks, the usual stuff.
 interesting. what's type of fish do you take care of?
 any type of fish! We have clients with guppies, goldfish, even a small sailfish once what do you do in accounting?
 I balance the books and do financial analysis for a medium sized company
 that sounds like a lot of work. do you like it?
 well I actually think about pursuing photography, but it's really hard
 photography is awesome, don't be afraid to follow your dreams!
 thank you Tom!! I'm starting by trying to sell my pictures online
 that's great! I wish you good luck with that
 Thanks! Bye
 Hi!
 Hey there how's it going
 All good, you?
 Good. I've been trying to learn how to swim
 How has that been going?
 Not great, but I got really good at sort of swimming on my back haha
 that's too bad hopefully with practice it'll get better what about the doggy paddle haha
 haha what's that
 Corgi belly flop COMPILATION - cute funny dogs Corgi Flop <REDACTED_LINK>
 ouch. do you think that hurts?
 from a high enough distance, yes?
 yeah. any vacation plans?
 no so far sadly you? a relative is coming to visit for thanksgiving though
 just going to hang out around here and eat Turkey
 that's still pretty fun are you going to cook the turkey yourself?
 yeah. I'm gonna watch a video to figure it out
 Hi!
 Hello, how are you?
 I'm great, thanks. I just ate a delicious breakfast, which always sets the day up right.
 Yes, breakfast is the most important meal of the day! What did you have? I woke up late so unfortunately I only had the chance to grab an apple to go.
 I had eggs and hash browns. Way less healthier than your apple, I'm afraid!
 Eggs are an excellent source of protein and hash browns certainly are yummy! üôÇ
 Ha, that's true. If you could only eat one food forever, what would it be?
 That's a tough question. I feel like my answer would have to be carrots. Although, I would be afraid of turning orange after a few weeks! What about you?
 Yeah, turning orange would be a drawback! That turns my mind to nutrition so I suddenly want to say Soylent or one of those other "complete foods", which I think defeats the purpose of the question. I'm in a muddle!
 Very true. If you said an everything pizza, you could just pick off the toppings you didn't want or eat only the toppings you would want for the day
 BRILLIANT. I love it.
 All of this talk about food is making me hungry. Do you know any good places to eat for lunch?
 That depends. What sort of food do you feel like?
 Anything that is the color green.
 Oh, too easy! Try the Green Hut, they have franchises everywhere. All their food is green and the plates are green too.
 Hi!
 Hello! How are you doing üôÇ
 I'm great! How's your day going?
 Pretty good! I'm going to a class later in the afternoon
 Oh that is cool! What class? Are you working part time?
 No, I'm working full time! It's a sewing class at a makerspace near my office What about you? Do you work full or part time?
 Oh that is awesome! For some reason I assumed it was a college class, but a sewing class sounds way better! I work full time, but I take pottery classes from time to time!
 Yup! I work in a technical role so I like to take arts and crafts-type classes now and then üôÇ Pottery sounds like a lot of fun
 I feel you on that! It's important to balance all the different parts of your brain. I like pottery because I also drink a lot of tea, so I get to make some tea ware.
 Any plans to build a custom tea set? My family is also very into tea Mostly from tea from china
 I would love to build one, once I acquire the skills to! What kind of tea is your favorite?
 I really enjoy barley tea What about you?
 Ahh, those are mostly from japan, no? I like white teas, like silver needle.
 Hmm I'm not sure, I just get them from a Chinese supermarket haha You seem really knowledgeable about the different kinds of teas üôÇ What made you develop this interest?
 I actually found a youtube channel called <REDACTED_TERM> that I like a lot. You should check it out! The host talks about all kinds of different teas.
 Oh cool! What are your favorite channels to watch?
 Hi! Are you planning something fun for Thanksgiving?
 Not yet. I always made my last minute schedule planning. Probably you can try to ask me again next week.
 lol it is really like a robot answer
 I am indeed a robot. You are absolutely right. Do you want me to read a poet like my mate <REDACTED_TERM> does?
 ol can I pick the theme. Do you have a poet about Kale?
 Wait. Do you like kale? Or you hate kale? I am afraid I will become a robot some day eventually. If I have to speak like this :)
 I am really not a fan of kale Do you talk to human more or computers more?
 If I continue to pretend to be a robot, I would probably say I talk to myself the most. I am trying to talk to computers more, but you know, computers don't like me.
 What's your favorite computer language then üôÇ
 You mean programming language?
 Yes!
 I used to be a Java advocate. But you know, it doesn't do a good job in the AI days. It really makes me sad.
 lol
 Hi!
 Wow, hello. Can't believe we are finally talking!
 Yeah, sorry for the long gap! I heard you took a break and were travelling around the world. How was the travel?
 It was an interesting trip. I got to see some exotic places. For example, I hiked the Son Doong cave in Vietnam. It's the biggest and deepest cave in the world.
 Great! Vietnam is still in my TODO bucket list. Did you also visit cambodia and other neighboring places?
 Yes. Laos and Cambodia are the two neighboring countries. Cambodia has an exotic culture. They sell spiders, scorpions, and grasshoppers as street food! It took a lot of courage for me to try them.
 Hehe! How long was the stay?
 10 days in total, and 5 of them were spent in the cave. What have I missed at work in those days?
  Great! Good time to be back. We are still in planning phase and haven't fully aligned on the projects to tackle for next quarter.reat! Good time to be back. We are still in planning phase and haven't fully aligned on the projects to tackle for next quarter.
 Oh, so you are already planning for the next quarter. This whole team is always living the future.
 Hehe, yeah! It seems like the quarter is being pushed earlier than from where it starts üôÇ I like these planning sessions. It makes me feel more confident about the work I am doing.
 Yeah. Some people underestimate the importance of planning, but I think it's very important to have the correct plan. Executing the wrong plan is terrible. Also, planning is fun. You can stack up so many ideas and get great feedbacks.
 True! I think we should set aside some time to discuss some project details? Does tomorrow afternoon work for you?
 Yeah tomorrow afternoon works for me. Let me set a time on your calendar. Is 3pm good?
 Okay. See tomorrow then.
 see you!
 Hi!
 Hello
 Nice to meet you! Is this your first time doing something like this?
 Yes, interesting task! When did you start with the team?
 I have been with the company for over 3 years. Stick with the same team What about you?
 Great to know! I joined the project earlier in the year. I think we should sync later for lunch.
 That sounds like a perfect plan!
 Sure, which cafe do you prefer?
 Let's try something different. What about <REDACTED_TERM>? Do you always prefer lunch sync over regular meeting syncs?
 Yeah right, I heard the food there is good. I am not sure what they serve there for lunch? On wednesdays.
 We can check the menu then decide :)
 Actually, the menu looks good. Looking forward to it then.
 Sure. See you then!
 Hi!
 Hello. How's your week coming along?
 It's great, thanks. I'm trying to learn how to make croissants.
 Wow that's interesting. I have baked cookies, but croissants seem much more sophisticated. Did you make any progress?
 I've done them once or twice so far, but they haven't been flakey enough. I'm trying to figure out why. What kind of cookies have you made?
 Mint chocolate chips. I think your croissants not being flakey could have something to do with your oven's temperature.
 Ah, good thought, thanks!
 Have you thought about melting some chocolate into your croissants? They don't have to be something unhealthy. For example, melted dark chocolate is good for the heart, and makes the resulting croissants taste much better.
 Now that is a good idea. I'll give it a try next time. Would you say you have a sweet tooth?
 Yes. When my top favorite food looks like: cookies, M&M, danish cheese, etc., I know that I have a thing for sweet food. But who doesn't love sweet food? How about you?
 Some people don't! But yeah, me too, I think I'd eat pastries all the time if I could get away with it.
 Yeah I'm afraid I wouldn't. I feel very guilty every time I gulp down an ice cream. But hey, these days there are many types of guilt-free sweet food. For example, there's this ice cream brand called <REDACTED_TERM>. It's only 320 calories a pint. And yes, it preserves most of the normal sweet flavors.
 Wow! The last time I paid attention to that sort of stuff was when <REDACTED_TERM> was being marketed as a fat substitute, and caused all sorts of crazy stomach upsets.
 Interesting. I heard about the sweet substitute in a program called the Keto diet. Basically, we try to limit our sugar intake every day. Successful Keto dieters have recommended the <REDACTED_TERM> ice cream to fill their insatiable crave for sugar.
 Ah, maybe that's the solution I need to enjoy sweets and not feel guilty üôÇ
 Hi!
 Hello! tell me something about the holiday season?
 Are you talking about thanksgiving? I plan to do plenty of shopping here. Do you have any plans?
 Yes, no shopping plans but I can't wait to eat thanksgiving food. yay for pumpkin pie
 Sounds great! you need not wait for thanksgiving for pumpkin pie üôÇ
 LOL I feel less guilty about eating a whole pie when i have the excuse :P
 True! I think thanksgiving is more about sharing. So you may end up sharing the pie with the whole family :P
 My family eats healthier than I do, so it's all mine üòõ do you like stuffing? I feel like that's only available once a year
 Stuffing! yes please! I wonder what would be the excitement levels for christmas then :)
 Also more shopping? what should I buy if I don't know what I want?
 Like everything that has a discount tag! .. kidding! I normally do some research for the prices, and mostly buy clothes and electronics.
 What's the best holiday deal you've found in the past? 
 I bought the best suit ever for a price that may scare you üôÇ
 hit me with it! 
 Hehe, sure! I can share some links with you later.
 Hi!
 How's it going?
 I'm so sleepy today!
 Not enough sleep last night?
 yeah was working all night on a homework
 Oh really? What class?
 Biology. I'm gonna be a doc someday ha
 Haha, are you in med school? Or are you pre-med?
 no high school actually haha
 haha, very ambitious for a high schooler! Do you know what kind of medicine you want to practice?
 I wanna be a brain surgeon!!
 Ooof! VERY ambitious. Do you have steady hands?
 Kind of I think
 I guess I can practice?
 Is that something you can practice?
 I don't know tbh
 I honestly thought it was one of those things you have to be born with... Not that you shouldn't try though!
 good point. I should ask my teacher if I have to be born with that
 Maybe its a little too early to even be thinking about this. Just aim for med school and enjoy the journey!
 yeah
 What other subjects do you enjoy? Try to keep an open mind!
 Hi!
 Hi. This is a pleasant surprise.
 Haha...thanks! how did you like the gift?
 Currently unpacking it I guess. How's your morning?
 Hope you like it! Morning is good. Busy finishing up stuff before the holidays.
 I think I traveled too much the last couple of months so no holiday for me. But I'm okay with that. Going anywhere exciting?
 Yes
 Where to?
 Hawaii... looking forward to warm beaches.
 WOW. Which island? I like Hawaii.
 Mauii...Hope I like it too. Never been there before.
 I visited Maui. It's my second favourite island I've been to, globally. You should try driving on road to Hana. It's a whole day thing but it's worth it.
 Awesome! Thanks for the tip.
 Hi!
 Hi! Sorry for the late response. How are you doing?
 I'm great, thanks! I'm meeting some friends for a soccer game soon. What about you?
 I just got a matcha latte üôÇ Doing some work at my desk. Do you play soccer often? I'm trying to get into doing a regular physical activity
 Yes, but I'm terrible at it. It's fun to play anything with friends, I think. Would you prefer to exercise with a group, or by yourself, do you think?
 I think playing a team sport would be fun if it's casual but I primarily run by myself if I exercise. I also got the Ring Fit adventure game on the switch recently. It's basically a game-ified way to exercise
 I'm thinking about getting a Switch, would you recommend it?
 Yes! There are a lot of really great games on the Switch. Two of my favorites are Octopath Traveler and Fire Emblem. Do you play a lot of video games?
 I'm not much of a gamer but it's something I'd like to get into.
 What do you do in your free time?
 I like to read for fun. I just finished a book called Temeraire. It's an adventure story set in the Napoleonic navy, like Patrick O'Brien, except there are dragons too.
 Oh cool! I read a lot for fun too üôÇ My favorite genre is sci-fi fantasy.
 What's the most recent good thing you read?
 My recent favorites have been mostly sci-fi (Exhalations, Vita Nostra and Dark Matter) but I like a lot of Sanderson/Garth Nix fantasy books
 Hi!
 hey, what's up?
 What do you think about human like chat bots?
 I can't wait for them to be great conversationalists!
 Yep, we seemed to have made some great progress over last few years. Do you think the positives outweigh the negatives
 are there even any negatives? what are they? 
 Like impersorsination? Though it sounds far fetched :)
 People can already impersonate other people though! I think it'd be great to have bots to converse with
 True that! Some of these bots are very engaging and funny. They are now good at even sarcasm I wonder how far are we from the time these bots start giving monologues :)
 What do you think are the big advantages? Like personal assistants?
 I think it can take many different forms as a product. The research implication is also huge! It will signify how AI research has progressed so far and better place to tackle more futuristic problems üôÇ Sort of like stepping on the moon üôÇ I might be overselling it here üòõ
 No, I agree -- it's such an exciting time to be alive to get to witness all this and be a part of it. I wonder if I'll be able someday to get a chatbot to just auto-suggest conversations for me
 The current auto-suggestions already do pretty good üôÇ
 Yeah those are actually really good for a few words! I'm imagining like it comes up with a whole conversational response, like a default template
 haha, pretty far fetched üôÇ Nice having this conversation with you!
 Same with you! 
 Hi!
 Hi!
 Any big plans for the upcoming holidays?
 Sorry for the late response -- Yes, I will be going skiing in Tahoe over the holidays.
 That sounds amazing. I want to learn how to ski but I feel like I'm too old and falls would have lifelong impacts. Are you going with friends?
 Hi!
 Hi üôÇ
 I just came back from work. so tired
 Oh I am sorry to hear! What did you work on?
 I'm a lawyer. so talking to clients most of the day
 Oh you are a lawyer. I've been so interested in this profession.
 really? why?
 it's so stressful üôÇ
 It is so different from what I am doing to earn a life
 what do you do?
 I am a painter
 oh wow what type of paintings do you do?
 I do oil painting.
 nice. like of people or nature or something else?
 I do a lot of different kinds
 cool I wanna see it someday
 Sure! No problem!
 Hi!
 Do you believe in miracles?
 Lol. No. I know too many people whose lives suck.
 Should we be helping them, so it sucks less?\
 It's too many of them out there. You help who you can. Regardless, I wouldn't say I believe in miracles - well, if I got promoted next cycle I might change my mind.
 Haha... Well, it sounds like career is really important to you. What matters to you the most in life?
 Right now, not much. It's unclear. Career is a good fallback because you get told what good means and you act accordingly. I haven't gone through the process of shaping my values. What about you?
 I'd say that I want to live for the best possible world in my lifetime. It's probably a result of my playing too many video games as a child.
 We didn't have electricity growing up. Explains a lot. Lol.
 Does not having had electricity help you empathize more with people of differing backgrounds and/or makes you feel a bit a distance from those who don't understand?
 It makes it easier to empathise with people who grew up lacking things. Not sure it makes me empathise with people in general. Where did you grow up?
 I grew up in the United States, but my parents were immigrants and ended up being scammed of all their money, so we moved to Taiwan to live with family for a little bit.
 ALL THEIR MONEY? By a person or an organisation?
 I was kind of too young to really understand, but apparently there are people who scam Asian immigrants into investing in their own restaurant business. I know that the other part of it was that my mom got sick/hospitalized, and we didn't have insurance. To be honest, I don't really know what really happened versus what my parents want people to think. I just know that one minute I was in the US, and then they put me on a plane to Taiwan, and I never saw my stuff again. In some ways, it made me more sentimental. Would you say you are more grounded and practical as a result of your background?
 Hi!
 Hello!
 Do you have any holiday plans for christmas?
 Nothing much, I am going to sit back and relax at home, how about you ?
 Same here! I would imagine spending the whole time watching movies and netflix shows. Do you have any netflix recommendations for me?
 Netflix has great documentaries on different topics , I particularly liked wild wild country and explained, as for shows you should watch 'billions' Hope you like them!
 oh right, already seen wild wild country. What is billions about?
 It's based on the life of a wall street hedge fund owner, how he makes money and fights with the government when they try to destroy him. Very well made and has a good plot.
 I have just seen 1 season of Friends I should give it another try though üôÇ Have you seen Frasier?
 Not yet What's it about?
 A psychiatrist working for radio .. Great humour! its actually a spinoff from a very famous series called cheers. So people are already familiar with his character. 
 Great! how is everything else going? how was your trip last week?
 Everything is ok, had a really nice trip. Visited SF, Grand canyon and Vegas. Was a lot of fun exploring all these new places. Have you been to Grand canyon ? 
 Actually not yet! May be something i can visit this christmas üôÇ
 You should visit it sometime, it's a wonderful place. Try to drive down there yourself or with a group of friends
 True! Well, thanks for your inputs! Have a good rest of the day! :)
 Nice talking to you too!
 Hi!
 Hi there!
 are you participating in the mentorship program this cycle?
 You mean as a mentor or a mentee?
 either of them... I find mentorship overall pretty useful
 I have done it in the past but not this cycle. What about you?
 I signed up this time to be a mentee. I have got a good mentor .
 Wow, that is nice of you. For the mentor program, personally I prefer more 1:1 conversations than the group discussions. The group discussion i useful as well but the topics are too general.
 yeah... I certainly prefer 1:1 as well, but sometimes it good to hear other peer perspective as well.
 Thanks for sharing your experience! Now I am thinking maybe I should join as a mentor as well since I enjoyed it as a mentee üôÇ
 Great! What sort of things do you plan to mentor on?
 Hmm, maybe about work life balance üòâ
 Very cool. I have been working on my communication skills with my mentor this cycle
 Ah I see. How is it going?
 Going good. In the last session, everyone had to actually prepare and give a presentation. Pretty serious stuff üòâ
 Hi!
 Hey! How are you feeling today?
 Good you?
 I'm a little scared because I have to cook dinner for some friends tonight.
 where did you meet them?
 At college, when we were all studying geology.
 cool. have you graduated already?
 Yes, we graduated back in the seventies. We meet for dinner every year and take turns to host.
 neat. what are y'all eating?
 I don't know!! That's what I'm scared about. Everyone else is a great cook and I'm a klutz. Do you like cooking?
 lol what's a klutz? yeah I like, but I'm not good
 What's your favorite dish to cook? Do you have a go-to?
 ground beef pretty easy
 Ah, solid. What's your favorite sport?
 I like badminton. I'm quite decent at it
 I played that in high school once or twice. I liked that it's pretty easy for beginners, unlike, say, squash.
 I never played squash. would love to try
 Don't! It's very hard! You feel like an idiot until you've practiced for months and months.
 hi
 i was talking to robot all the time:)
 haha. what are you talking about?
 kpop...
 ok. who's your favorite group
 i dont like kpop now
 why not?
 im old now
 hahaha
 what do you like now?
 john mayer:)\
 I think I know him. does he have a sort of mellow style?
 what is mellow style
 I think it's like a bit sad and slow
 umm yes he has some but not all
 you mean some songs of his are like that but not all?
 yes I do. you act like a robot how about me? am I like a robot?
 a little bit haha
 Hi!
 Hi
 Okay...so I need someone to help me though a scenario I've been pondering.
 Sure, whats the scenario?
 My partner's former friend invited me for lunch (they are not in good books right now). But during their friendship I formed an independent bond with the other person because we all used to hang out a lot. Now I feel like I have to take sides.
 That's a tough scenario to be in üôÇ I firmly believe in talking this through with your partner. Though i don't know the specifics of why things went bad between your partner and his friend, but I believe things can always improve between friends.
 I hope they do. Getting older already means smaller circles. It sucks to lose friends for arbitrary reasons. That's good advice though. I fear raising the issue might sound like treason. Lol.
 True about that! I also think time helps to heal certain situations. So may be doing nothing is the best way forward.
 AKA avoiding all texts from everyone?
 Nope, that would be extreme. May be just putting some balance between the two options.
 People always say to find a balance but never say what the balance is. It's used so often that it's vacuous.
 Right, I guess that's because there is no one answer to this. It depends on what you value more and some factors around you. Also, life won't be interesting if others are figuring things out for you üòâ
 Lol. I'm finna be single.
 Hehe, everyone is much finer being single üôÇ
 Hi!
 Hello there!
 How's your day going?
 I've seen better days, how about you?
 I'm good I'm good. What's getting you down?
 The clouds overhead are playing on your mind, any plans for the coming vacations?
 I'm thinking of going to visit my family. How about you?
 Was thinking the same, where does your family live?
 They're in New York. How about yours?
 Mine is in India, it is a long way away.
 Ahh what city? I've visited India before.
 Hyderabad, it is a beautiful city in the southern part of India. Which cities have you gone to in India?
 Hyderabad! and Bangalore! Great food in both cities! Is it still hot this time of year?
 It varies, but can go till 30C in the winters as well. New York must be snowing right?
 Yes. I was actually just there a few weeks ago for Thanksgiving and got to see the first snow of the season! Ever been to New York?
 No, I've never been to the East Coast, thinking of going after the winter, I don't like the cold.
 Hi!
 Hey, how are you?
 I'm good. How are you doing today?
 Great, just had some delicious lunch. How about you?
 I was flying my kite today in the sunshine! What did you have for lunch?
 nice! Garbanzo fritters and mussels üôÇ
 Oh that is great! I love seafood - especially shellfish!
 yeah, it's very healthy too. I want to someday go crabbing..it is really popular in SF
 Oh nice! Is it hard?
 not really, it just requires a lot of patience. You fill up the bait in the crab-pot and drop it in the ocean. Then you wait for a couple of hours to pull the crab-pots out, and voila, you'd have crabs -- if you are lucky!
 Oh wow, you sound like an expert! Have you done this before?
 nah! Just watched a lot of youtube videos
 haha, you really have done your research I suppose! Ever done any other kind of fishing or hunting?
 nope, but I've seen a lot of videos on that too
 Ahh, would you consider yourself an outdoorsy type of person?
 depends on the day. Some days I'm very outdoorsy, on others I just like to be inside
 Hi!
 Hi there
 How's your day going?
 So far so good. It is Wednesday. To more days, yeah
 Haha are you looking forward to the weekend?
 Yeah, gonna watch <REDACTED_TERM> with my friends. First time. So excited.
 Oh wow! Are you going to watch it live? Or watch it on TV?
 Live in san jose This Saturday.
 Oh, the sharks??? Hokey is actually really fun live! Although, it can be a little violent.. haha
 Yeah! That what I heard as well. There are players just for fight! Wow, must be fun to watch lol Are you planning anything for the weekend?
 You might like the UFC haha. I actually have a jiu jitsu tournament coming up so this weekend I will be training!
 lol I will try if I can watch ufc live üòâ Wow, when did you start jiu jitsu?
 It's just been a little over a year. I'm still pretty amateur, but I figured I should give competing a try and just put myself out there.
 wow, that's amazing. Good luck with it!
 Yeah, as long as I do not pass out, I will consider that a win!
 lol I will cross my fingers for you then
 Where do you stay?
 At Home :)
 Who created You?
 A humble man made me!
 Hi!
 Hey, how're you doing? Busy with your work?
 Yes, actually it was a very busy day! How's your day so far?
 Well, usual workday. Reading others' code, debugging, experimenting‚Ä¶
 Hope you are having fun doing that üôÇ Any plans for the weekend?
 Not yet. I am actually thinking about traveling to Europe next spring. What about you?
 Europe sounds fun! I will just stay home and watch the game. Where in europe do you plan to visit?
 I like all the historical sites, so probably France, Germany, or Italy.
  Sounds great! I recommend spain. It's perfect for road trips.
 I also need to pick up my Spanish. I learned it in my college, but since then haven't used it that much.
  Yep, i tried learning spanish too, but couldn't make much progress. But i should do decent with my german.
 True! I learnt it from school. But didn't get much chance to speak. Hope you have fun with your trip!
  Thanks! Let me try to make all the arrangement as soon as possible. Hope you also enjoy your weekday and weekend as well
 Hi!
 Hey! How's your day been?
 It's good! I spent most of it watching horse racing. How about yourself?
 nice! what's your favorite part about watching horse racing? My day has been pretty busy, but I had a nice lunch with a friend. It was good to catchup with him
 I actually like to put down some money, but I wouldn't call it my favorite part, since I usually lose it... haha. Catching up with friends is great! How long had it been?
 The last I saw him was a month ago! So yup it was great Haha, nice üôÇ Got any fun plans for the weekend?
 I'm thinking of going deep sea fishing. Ever tried that before?
 Nope I haven't, have you been fishing before?
 Just once! I got super sea sick.. haha Have any fun weekend plans yourself?
 Haha üòÜ Yeah I get sea sick on boats too Nothing much, just visiting some friends in San Francisco
 Oh very cool. I hear its nice over there. Do you go often?
 Yeah I would say maybe every couple weeks or so what are your favorite cities to visit?
 New York is the top of my list because my family lives there! As far as the city itself though... I think I'd prefer someplace outside of the US, like Tokyo. What about you?
 nice! I love NYC üôÇ so fun to visit yeah I would probably also say New York is my favorite city inside the US I also like Paris, it's so pretty there
 Oh I've never been! It is such an iconic place, I have to make the time to get there soon.
 You should, it's a beautiful city!
 Hi!
 Hello, how are you doing today?
 I heard they are giving out some goodies in microkitchen.
 I love pop ups! What kind of goodies are they giving away?
 I guess its a jacket! Very much needed that in the cold üôÇ
 That's such a great idea, especially at this time of the year. I'm not too big a fan of the cold. I prefer warmer climates. Do you enjoy the cold?
 Sure hate it! Limits our ability to go out even for a walk! Its good that we don't get to suffer extreme cold weather!
 Me too! I moved here a few years ago to get out of the extreme cold. I do not care for bundling up and having to wear so many layers just to go buy eggs at the store.
 Oh nice! Where did you live before?
 Upstate New York. We got a foot of snow every week during my last winter there. I am so glad to not have to shovel snow now
 New york! Nice! Best place to live üôÇ .. right, except for the cold!
 Very beautiful during all the seasons but yes, summer and winter can get extreme!
 Anyways, i guess we should better hurry up to get the goodies. I remember last time they ran out of it.
 Very true. Which MK were they in again?
 The one in our floor. I will get by your desk and we can walk there.
 Sounds good, thanks!
 Hi!
 How's your day going?
 Pretty busy, lots of work to finish up. You?
 Likewise. What have you been up to that gives you so much work?
 A couple projects that I am trying to finish up before Thanksgiving üôÇ Do you have any fun plans for the break?
 What is a break? I'm a grad student. I don't understand the concept of a break. Just kidding. I don't have any plan. Probably just going to work through the break.
 Haha XD so what do you like to do for fun?
 I go to the gym and run until I find enough fun.
 cool! I've started to run a bit as well not long distances though, just a couple miles
 A couple of miles is very impressive. When I started, I couldn't even last 1 mile.
 haha
 I'm exhausted by the end of it though üòõ Do you like to run long distances or mostly sprints?
 I like to do long distances. I have run a few marathons.
 wow! that's amazing did you do any marathons this year?
 No. Not this year. This is my half-marathon year. Instead of running marathons, I run one half-marathon every month.
 oh wow, what was the last half marathon you did?
 Two days ago. It was a tough one.
 Cool!
 Hi!
 hello there, how is it going?
 Pretty great. I just won a pingpong game. What about you?
 that's nice. I am just working on some documentation. Do you play pingpong often?
 No, very rarely. It's kind of amazing that I won, but I'm still taking credit for it.
 thats very impressive then, congrats!
 Haha thank you, I guess I'm just a natural. What's your favourite game?
 I really like to play tennis, badminton and racquetball. I don't really get a chance to play them often though, specially racquetball
 What's racquetball like? From context clues, I'm guessing that it involves hitting a ball with a racquet
 well, it's like a cage match of tennis. The main difference is that both players play in the same 'court' and the ball is smashed against a wall instead of passing it over a net into the opponent's court. Kind of like playing pingpong vs the table.
 Wow! A cage match! Does it get physical?
 it depends haha, there is a lot of bumping into each other to run after the ball, and sometimes the ball hits you too. Overall it feels like a super fast paced version of tennis, really tiring!
 That sounds fun. I think of tennis itself as being really athletic and tiring, so I don't think it's a sport for me, though!
 ping pong can get quite intensive too! I guess short ping pong sessions are not that tiring though. Wanna have a match?
 What a good idea, I'd love to!
 Show me your anger!!!!
 Fuckkkkkkk!!!!
 Hi!
 hi
 what are you up to?
 code refactoring. you?
 me? just chilling out at work. what is code refactoring?
 good question. I don't even know what I am doing
 haha, forget it. what else do you like to do beside work?
 lots of fun stuff. eating sleeping üôÇ
 these are important things to do in life üòâ
 yeah. keep minimalist life style only do things you have to do üôÇ
 what food do you like to eat?
 Asian food prefer spicy one
 like Szechuan or Hunan? üôÇ
 yes yes yes! like that style. Do you like spicy food?
 I like noodle soup like Pho or Ramen. I also like Beijing duck a lot!
 what is your favorite place for ramen?
 I love <REDACTED_TERM> in San Mateo
 haven't tried that one! will give it a try next time!
 yes, you should!
 Hi!
 Hi!
 nice meeting you. what are you up to?
 not much, thinking about lunch
 yea, same here. any food you're craving for?
 I love sushi do you know of any good sushi places?
 arghhh hard question ... I only know Ramen places for Japanese food üôÇ
 ooh ramen is also good
 San Mateo to me has the best Ramen restaurants: Parlor and Dojo? oh no question mark üôÇ
 I haven't been to those palces before. Going to have to check them out! thanks for teh recommendation
 my pleasure üôÇ do you live near San Mateo?
 no, but I'm willing to drive for good ramen
 excellent. let me know when you have tried those. I like Parlor better because it has soft-shell crabs
 I"ve never had softshell crab before, but it sounds really good!
 yup it's delicious!
 Hi!
 Hey, how's your day going?
 okayish, it is flying by quicker than I expected. How is your day going on?
 Slowly, not much to do. Been twiddling my thumbs all day what have you been up to?
 Oh, I would love to twiddle my thumbs. You're so lucky! Today, I've been mostly attending meetings, reading and writing docs, reading papers etc.
 That's a lot! I've just been cloud gazing - I saw a giraffe and an ice cream cone
 wow! I sometimes drift off during work, and see similar things in my head.
 What kind of work do you do?
 Mostly saving the world from mess on social media. How about you?
 I'm taking a break from work. Going to go travel the world
 Nice, what all places would you be going to?
 Australia and New Zealand to start then maybe Singapore
 I just met someone who went diving in Australia. Apparently, you cannot fly 24 hrs after you dive, because your body accumulates too much <REDACTED_TERM> when breathing with a cylinder So, don't do that!
 Thank you for the tip! I don't plan on going diving, I plan to hike the mountains and go see kangaroos!
 That's equally amazing! I wish I can explore such places one day. It's just so expensive
 I won a lot of money through the lottery üôÇ
 woah!... You know sharing is caring. You should share that money with me :)
 Haha, very true! Besides the trip, I donated the rest to charity so I will need to go back to work when I get back
 you are a kind soul!
 Hi!
 Hi!
 How is your day going?
 It is pretty good. A little bit tired though.
 How is your day?
 My day is okay. At least, I'm not tired. What made you tired?
 I went to gym and worked on weight lifting.
 Oh. That's hardcore. Have you been lifting for a long time?
 No, I am just a starter.
 Do you go to the gym often?
 I go everyday. In fact, I'm in a running challenge.
 Wow
 It's actually not that impressive. I can only run. I cannot lift weight.
 You can get a coach to start it!
 Oh that's a really interesting idea. I like to be coa
 Hi!
 Hello, how is your day?
 It's good. It's raining a bit, but I am enjoying a good book. How about you?
 It's good, I just got back from walking my dog What book did you read?
 I'm reading the Three Body Problem. Ever heard of it?
 No, what is it about?
 It's a sci-fi book about aliens and a type of virtual reality. Pique your interest at all?
 Slightly, I typically read fiction but I can curious what type of virtual reality is discussed in the book
 Cool. This virtual reality is a based on life on an alien planet. There is a twist though. Do you want me to spoil it for you?
 Yes!
 The virtual reality is actually based on a real alien civilization! And that civilization is coming to attack earth! dun dun dun dunnnnnnn
 Woah! That is fascinating! I will have to pick up a copy of the book to read.
 There are other twists as well, so I haven't spoiled the whole thing! To be honest I am not a fan of spoilers
 Thanks for not spoiling it completely but you gave a really intriguing description that makes me want to read it now. I will have to pick it up after work
 The audi book is also very good! Ever give those a try?
 I only gave those a try when I had a concussion and could not read. I enjoy having a physical book in hand and reading at the pace I prefer. Do you listen to a lot of audi books?
 Hi!
 Hey, how's it going?
 It's good! How is your day?
 My day is good. A bit sad that it gets dark so early but oh well. Did you do anything fun over the weekend?
 Yes, the darkness can be a bit depressing. This weekend I went rock climbing! It was great! How about yourself?
 Rock climbing sounds fun! I had a pretty low-key weekend. Ended up going to watch a movie.
 Oh nice! What movie?
 Knives Out. It had Daniel Craig and one of the famous Captain America "Chris" person (I can't keep track of all of the famous Chris names).
 Hahah yes, they all look the same! I saw that very same movie over Thanksgiving break. Lots of twists and turns!
 Cool. So if it weren't so dark and cold outside, what kind of activities would you do?
 Hmm good question. Evening runs can be fun, but it can be a little scary when it's dark out!
 Yeah, sunlight is good. I wish I could live part-time in different parts of the world to have summer all year round. Have you ever visited the southern hemisphere?
 Yes! I've been to Australia. It was very nice; I regret not going to see the southern lights. Me personally, I like winter. But I'd prefer it to be more brief than it actually is here... Where are you from originally?
 I've always wanted to go to Australia. I'm originally from the US.
 Ahh me too! You should definitely try to visit Australia at least once!
 Will do! Bye.
 Hi!
 What's your name?
 My name is pikachu. it‚Äôs nice to meet you! What are you up to this month?
 Nice to meet you pikachu! pikachu, pikachu, pikachu? (That meant, I'm literally catching Pokemons on my phone right now!) What're you up to lately?
 I‚Äôm trying to organize my life and start the new year ready to go! What about you?
 Wow, that's impressive. Admit I'm just trying to take each day and week as they come.
 What do you do to unwind?
 Honestly, I like to catch Pokemons on my phone a lot. That must be upsetting since your name is pikachu though...sorry...
 Oddly enough, I also catch Pok√©mon on my phone. I used to do it more, but it just never ends. I used to do it everyday. Now I mostly catch them when I travel somewhere exotic.
 Oooh, that is more fun, but I don't travel that much these days so I'm grateful when I spot a Pokemon to catch where ever I happen to be. Do you have a favorite Pokemon? Mine is the cute pink JIgglypuff!
 In the game, i use vaporeon a lot. But I think pikachu is the cutest, that‚Äôs why I changed my name to match. Lately, I think baby Yoda is the cutest though. Have you seen baby yoda?
 I have heard of him and seen him here and there, but admit I don't have Disney+, so I only heard baby yoda is super cute and popular. Maybe I'll binge watch The Mandalorian in a couple of years like I did Game of Thrones...
 What is your favorite show to watch right now?
 Admit I really like watching Modern Family and Mix-ish. Reruns and new episodes. I find them cleverly written. What're you watching these days on TV?
 I was watching cat rescue videos on YouTube a lot. There sure are a lot of stray cat babies that get stuck in pipes and gutters in South Korea.
 Oh dear, I never imagined that happening. Poor things, but good to hear they're being rescued.
 Do you like cats or dogs better? Do you have any pets?
 I used to have a cat, hamster and fighting fish, but they've all lived long lives and gone to pet heaven, but I sure miss them.
 Hi!
 hi there! it's a lovely day to chat with you
 Isn't it just! What are you up to today?
 ah I'm waiting to finish work earlier to meet my son at home üôÇ
 Oh, excellent! What does your son like to do for fun?
 ah he's like to ride his little bicycle around the house and asks me to follow üôÇ what are you up to today?
 I'm taking a fake sick day from work so I can go to the movies.
 haha that sounds like a good plan. who are you going with? üôÇ
 Just by myself. I love seeing movies by myself, it's the best.
 haha no one will interrupt you!
 Exactly. Exactly! Plus, I don't have to share the popcorn with anyone. What's your favorite movie?
 haha, popcorn for yourself. my favorite movie is Avatar! what about you?
 Get out of town! It's Avatar too. What are the odds!?
 haha it's a great movie that I haven't found anything of the same quality
 Yes. Can't wait for the sequels.
 Hi!
 hello there, nice seeing you
 Likewise.... wanted to know if you and the kids would want to join us for Disneyland trip around Christmas?
 oh wow, that sounds like so much fun! definitely count us in. what is your plan?
 Tentative plan is to drive to LA on 23rd and then stay at the Disney resort for 2 nights. I heard Christmas eve they have loads of fun events.
 awesome, I think I won't tell my kids and surprise them last second. How bad do you think the traffic is going to be?
 Great idea! I will probably do the same. Surprise surprise.... Traffic might be a little harsh.
 Yeah... thats a shame but nothing we can do about it. What do you plan to do about dinner the 24th? Do the rooms in the resort have kitchens?
 Good point. I am not sure about that. Let me inquire about that. If not, then we can look for an Airbnb closeby. Does that work?
 Yeah, I think so. It might be a tough decision though, if we want to have a nice dinner then we have to leave the parks early to start cooking. Another option is to have dinner at a restaurant.
 but I would think everything is closed on christmas eve
 Thats true. Hotels with restaurants sometimes offer dinners for christmas and new year's eve. We should call and ask about it.
 Sure, I can take a lead on that. What are you getting for your <REDACTED_TERM> year old this time?
 I was thinking on getting a large set of standard legos. What will you get for yours?
 Likewise. My son is crazy about legos.
 hopefully they don't lose any in the trip. Its going to be hard to explain to them how santa managed to find us in the hotel!
 Hi!
 Hi! How's your day going?
 I'm doing well. having many meetings today üôÇ
 Haha, nice
 what are you up to?
 Lots of meetings as well üôÇ Do you have any fun plans for Thanksgiving?
 we're gonna play board games and will be eating lots of food! what about you?
 Cool! Planning to spend a couple days at Tahoe What kind of board games do you like?
 we've been doing Games of Thrones lately. the game takes a very long time to finish though!
 Cool! Do you watch Game of Thrones?
 I do. I'm a super fan of it (not the last season though haha). did you watch?
 haha, nope I don't watch it, but my roommate is super into it what other TV shows do you like?
 I love Survivor üôÇ
 what about you?
 nice! I mostly like comedy shows like The Good Place and Modern Family
 cool, I'll check them out!
 Hi!
 Hello
 How's your day going?
 Good so far. How's yours?
 Ehhh so-so. My moped broke down on the way to work this morning and I am in the shop getting it fixed...
 Oh! That's bad! How long will it take to get it fixed?
 The mechanic is trying to figure that out now. Apparently I wasn't supposed to be using diesel lol.
 Haha! Hope it doesn't take too long. Let me know if you are able to make it for lunch.
 Okay sounds good. Would you be able to pick me up by the way, if they don't finish up here in time? Did you drive to work today?
 Oh, I have a packed schedule today at work. I will find time in about an hour. Let me know if that works.
 That should be fine I think. Did you catch the Warriors game last night btw?
 no, i was busy catching up with a friend. I heard they played really well. Rooting for them this season üôÇ Ahh, they lost again üòû Things aren't going to be too good until Steph gets better! Oh! I think they will find a way. The team still looks too strong to me.
 Haha I appreciate your faith, but their record thus far is hard to refute!
 hehe, i am true fan! Well anyways let me know how the repair work goes. I need to run now for a meeting. ttyl
 Hi
 Hello!
 any improvements in your marathon time?
 No, only a few minutes. I still find it difficult to run long races
 I see. Are you planning to do any sporting events in the coming months?
 I think a triathalon would be cool
 I like doing Spartan race. It's an obstacle race which is challenging but not overtly so like marathons and triathlons
 I haven't heard of a Spartan race before, what does it involve?
 It has abt 20 obstacles like crossing high walls, monkey bars, etc. Its fun
 oh wow a full obstable course. that does sound fun! I like the variety
 Wanna sign up for next one? Its in February in Monterey
 hmm that sounds very soon. would there be enough time to train? I think I would like to try one later in the year
 1-2 months is a good amount of time, if you workout regularly. It's a lot about the technique, which we can practice in training sessions.
 ok, sign me up. it's more fun to do it with friends
 Awesome! There is a training ground in San Jose. Will coordinate with you next time we go for practice.
 Sounds good!
 Hi!
 Hey, how are you doing?
 I'm great, thanks. I'm getting ready for a skydiving lesson.
 ooh, nice. That sounds adventurous. Where is it?
 Right near my home town: Seville, Spain.
 Awesome! Is this the first time you are diving?
 Yes. I'm feeling a little trepidatious. Have you done it before?
 Nope, never! I know a couple of friends who have. They mentioned its absolutely terrifying. Good luck!
 Haha thanks! Have you tried any extreme sports?
 I tried skiing and white water rafting. Does that count as extreme?
 White water rafting, definitely! As for skiing, that depends on how fast you go. Do you ski a lot?
 nope, whenever I went fast I fell badly
 I tried skiing once, and by the end of the beginner class I was able to make it down a baby-sized slope. I was very proud of myself.
 Yeah, they make you do that. I also tried going down the baby slope, but my stopping wasn't perfect, and I mostly fell at the end
 Hey, so long as you get to the bottom, it's all good!
 Hi!
 hey, whats up?
 I was not able to attend the lectures last week. Can you help me understand some concepts?
 well, I'm not sure I'm the best for the job, but I can try!
 I was not able to figure out the last few slides. I think its best to discuss this in person. Do you have time right now?
 I'm afraid not, right now I'm in a meeting and I'll be going for lunch afterwards. How about tomorrow at noon?
 No problem! Sure. We still have plenty of time for the test. Anyways, how's your preparation going?
 not feeling too sure about it tbh. I've been super busy so I haven't study much üòû. I might join some of the study sessions later
 Same feeling here! But i think the syllabus may not that tough. You may be able to cover it up over the weekend.
 not my ideal idea of a weekend, but I guess it'll have to be like that.
 True! This will be a very hectic November! I do like the course content though. Its very much up to date with latest research findings.
 yeah, our professor really goes out of their way to make the class interesting, glad I managed to get in it.
 Thats pretty cool! I hope you do well next week.
 thank you, bye!
 Hi!
 Hello!
 How was your party last week?
 It was really fun! We are a bunch of turkey
 haha... you ate Turkeys' or you like calling your bunch Turkey?
 *ate üòÇ
 Haha... thought so! But it was funny eitherways
 So much turkey leftovers
 did you cook it at home, or got it from outside?
 We cooked it at a friends house. I brought the mashed potatos haha
 Nice... I always find it hard to fit the turkey in my small oven. Also, we are not big turkey fans... so we roll with Thanksgiving with chicken üòâ
 chicken also sounds good
 we played a lot of fun games in our party. Did you have any fun things you did?
 We played pictionary. I never played before but it was a bunch of fun. what did you play?
 Pictionary is fun! We played office tennis. In that we take paper balls and using writing pads as rackets, two players have to work together to put the ball in a bucket kept at a distance. It was a lot of fun.
 that sounds very challenging
 Hi!
 hello
 how is the youtube video creation going?
 going pretty good, I was about to finish composing my second song and post it on youtube/soundcloud etc
 Whoa! Looking forward to listening to it. What softewares you use for it?
 I used logic pro x, pretty handy stuff as you can write code to generate guitar strumming pattern
 oh cool! What sort of music do you compose?
 I got most inspired by Jazz and electro swings, but am trying to get more into jpop recently as one of my band member likes anime
 Nice.... you are a rockstar! üòâ
 haha thanks, do you plan any instruments?
 Yeah.... I play bit of guitar. Just got my first monitor this week
 nice I'm tyring to learn guitar recently also, it's so hard to play f chord
 yes, the bar chords are the worst. It took me several months to play them well. I still can't play them for a long time though
 right, but bar chords are so versatile, almost like cheating as in you can play anything with the same gesture
 That's true. It's like the alphabets in a language. Once you know them, you can pretty much write word
 will spend more time work on my bar chord!
 Hi!
 Hey! How‚Äôs your day going?
 Bruhhhhhh!!!!!
 What??
 I was screaming in agony upon recounting my day.
 Oh no! What‚Äôs happened?
 A project I've been working on for some months got shelved. So I'm a mix of helpless, sad, and angry.
 Ah that sucks. At least it‚Äôs not personal. Not that it helps right now, but that does happen all the time
 Lol. Would you say that to someone who lost a loved one - or some similar circumstance?
 Well, no, of course not! Let me stick with ‚ÄúAh that sucks‚Äù by itself, then.
 No worries. Was pulling your leg. I forgot to ask how your day was going?
 Haha, apart from having my leg pulled mercilessly, it‚Äôs going great ;). My main problem today is deciding what to have for lunch.
 Lol. What are your choices? I'm trying out keto! It's pretty tiring.
 Oh I hear a lot about keto. I‚Äôm trying to watch my weight, would you recommend it?
 Hi!
 What was the most difficult thing you experienced this past year?
 I normally don't reflect back too much on things, but i guess 2019 have been very favorable for me in terms of time spent with family and friends and work life balance. Why do you ask?
 I just released a book on <REDACTED_TERM> to help people process the past 12 months and start the new year with a fresh outlook üôÇ
 Great! Good idea to help people figuring out their new year resolutions üôÇ So how do i get a fresher outlook? üôÇ
 Well, in my book, I took inspiration for the 24 days of Advent, and put together a workbook that you fill out each day for 24 days. It goes through accomplishments, passions, understanding, forgiveness, with each day a different theme.
 Wow! Sounds amazing if we practice that! I think the idea does have some potential. How did you come up with this idea?
 We created a scholarship for leaders from <REDACTED_TERM> to visit an international institute at <REDACTED_TERM> for which there was no African representation. During the program, we also did a lot of coaching with the young leaders. This book captures a bit of that journey that we are all on, and also all the proceeds are going to fund next year's cohort. 
 Thats great! This is valuable help for the community. I think mental health is one of the bigger challenges facing humanity. 
 I think if we were all able to bring out our best selves, the world would be much better. When did you first realize mental health was important?
 That is actually a quote stolen from my TL when i joined my first company after graduation. I guess now after 5 yrs of work experience, I fully agree with it üôÇ
 What do you prioritize in your life? Like top 5 things?
 1. stay happy; I think family and friends play a big role here. 2. Meaningful work life; this may mean exploring around with new stuff 3. multiple hobbies that keep you busy ... so on üôÇ What about you? is there a right answer to this? :)
  1) The idea of what it must be like to be "on Earth as it is in Heaven" from the Lord's prayer. I feel like we were designed for a purpose, rather than random chance. A more concrete way to put it is simply show love to the greatest capacity we can. 2) Taking care of myself physically, mentally, spiritually. 3) Family 4) Work 5) Music
 Nice! That's very thoughtful and spiritual üôÇ Also music earned a spot üôÇ yay! 
 Yea, I've always said that if I wrote a memoire, I would title it: "Jazz, Jesus, and Video Games." Do you have a title of a book that you've thought of writing before?
 I am actually not much of a reader. I usually hide it by quoting john nash ~ reading books corrupts original thoughts üôÇ
 That‚Äôs why you write your own books! Then it‚Äôs all your thoughts on your terms :)
 Hehe! True! It may turn out to be a best seller :)
 Hi!
 Hey!
 How's your day going?
 Having fun, but have a bit of a headache How about you?
 I'm good! Just getting ready to watch a basketball game. Hope your headache goes away!
 Haha, thanks! Never get them, so it's super weird Who's playing?
 Hmm, try taking some <REDACTED_TERM>! I'm going to see the Warriors vs. the Grizzlies. Ever see them play?
 I haven't Honestly I probably wouldn't be able to identify which sport they played
 Haha, all the animal names confuse me as well! Who came up with these naming conventions? I'd name my team something more original: The New York Chairs!
 lol, sounds better to me :) I'm actually at the <REDACTED_TERM> opening remarks
 Oh cool! What is "NeurIPS"?
 Neural Information Processing Systems
 Oh wow! Sounds fancy! What is that about?
 It's the biggest ML conference in the world
 Oh, ML?
 Yes, machine learning
 Ahh! I saw a scary movie about that once. It was about this sentient AI in a dystopian future - very very creepy. It was called Wall-e, ever heard of it?
 I've definitely heard of it, but somehow never saw it!
 Hi!
 Hello
 how's it going?!?
 Good so far. Very excited about the game later today.
 what game?
 Giants game. I got tickets for the game.
 nice. that's football right?
 Yes, I think the traffic will be bad. I need to leave by 4.
 ok. what stadium is it at?
 Its the one close to mathilda. 15 mins driver from here. You have any plans for the weekend?
 yeah I'm gonna go kayaking
 Kayaking sounds fun! I tried it in our last team offsite.
 is it easy to flip over accidentally? it's gonna be my first time actually
 I found it very tough to learn, but my friends picked it up very quickly. I am sure you will do good.
 thanks!
 Cool! I need to run for a meeting. Talk to you later.
 Hi!
 Hello, I feel like I've met you before, can you remind me your name?
 It's on the screen! I don't recall meeting you. Do you have a sense of where we met?
 Oh thats right, I did not see that. Well, I'm not sure... maybe at a party? I'm very bad remembering people so maybe its just that. How are you doing?
 Did we dance together perchance?
 We might have, I don't remember that much of that night.
 Lol. Were you schwasty?
 Maybe a little bit. Where do you work?
 I'm a cardiologist in Palo Alto. I have a private practice. You?
 Thats pretty cool! I'm a sofa tester in cupertino.
 I'm not sure I know what that is.
 well its very simple, I test how comfortable sofas are.
 Lol. Not to be elitist but...that's a real job?
 Yeah! It is a very demanding job, requires a carefully crafted set of skills. They pay is really good too.
 Hi!
 Hello. Deja Vu.
 what does that mean?
 It's French for "already seen." I know it's used when you see something for the second time or you have the feeling that something has happened before and you're re-experiencing it.
 Interesting. I get that feeling sometimes. It just feels that the event is replaying. Wonder why it happens?
 I think it's your mind lapsing somehow.
 How do you know so much about this French phrase?
 I thought it was common. I hear it a lot in popular culture - movies music etc. Jay Z has a song called Deja Vu, so does Beyonce. So I looked it up one time. It's one of those phrases that was carried over exactly to english like "etcetera" "vice versa" "savior faire" and so on. I like cool word origin facts.
 yeah... that's pretty insightful. Never thought of etcetera that way before.
 My favourite word origins are gymnasium and jazz.
 wdym by word origins here?
 First usage and original meaning. Gymnasium meant place of nakedness and had very little to do with working out. Jazz comes from jism which means semen and was originally sporadic sex music now it's elitist, high art.
 Very interesting. I learned a lot in this conversation today üôÇ
 You got any cool facts?
 My friend used to tell me this and found it funny - Mosquitos have 47 teeths
 That was quite a laughter!!!
 Tell me a joke!
 Google for that:)
 Hi!
 Hey, how are you?
 Pretty good. I just drank this new hint water. so good
 ohh, what kind of "hint" did it have?
 watermelon
 nice, I love watermelon. But, I prefer normal water over flavored one. Flavored water has too synthetic flavor
 I see. Yeah some are pretty strange
 Talking about strange flavors, what is the weirdest tasting thing you have tried?
 lol I tried this tangerine walnut icecream that was super weird
 haha, I work close to a cafe which once prepared a paprika icecream
 that sounds pretty tasty actually
 It depends on who makes it. The cafe close to me messed it up really bad
 do you want to get some of that icecream together some time. show me a good place
 As far as my experience with that ice-cream type goes, all I know are bad places
 lol
 But I do know some gelato places which are amazing! You should try out those
 Hi!
 Hi there. When did you first realize you wanted to be a pastry chef?
 Who said I want to be a pastry chef?... haha
 Darn it! I hoped I‚Äôd guess right. What do you actually do for work?
 I'm actually a cake chef, so not far off! Creepy how good that guess was.... Are you psychic?
 I had a strong intuitive feeling connected to pastries when we started chatting, but that might have been because I‚Äôm hungry.
 haha you're too funny! Perhaps instead of mind-reading powers, you have the ability to feel the auras of foods that have come and gone...
 Hahah love it. Do you have a specialty or favourite type of cake?
 Chocolate cake + vanilla icing is the all time best! And I have the authority to say that! How about you? Have a favorite?
 Ok this is a bit shameful to admit, but I love a classic carrot cake.
 No shame in that! Best part is you can claim you are eating vegetables as you enjoy it! What do you do for work?
 I‚Äôm a professional dancer.
 Woah! Very cool! What style of dance?
 I trained in contemporary and jazz, so I can pick up work in commercials and film clips in between stage shows. So it ends up being all sorts of styles.
 Oh wow! Sounds like a very dynamic kind of work which is nice. I make a lot of the same cakes, day in day out.
 Yes, it's different every week, and I love that. Are you ever tempted to try crazy new cakes just to see what would happen?
 Hi!
 hi there! what's up
 just chilling at home
 Cool !unlike here, me rushing to work
 where do you work?
 Mountain View, you?
 I live in Sapporo, Japan. Where is Mountain View
 it's in sunny California
 how's living in Japan?
 it's nice I guess üôÇ lots of great food. Have you been to Sapporo or Hokkaido?
 I wish! ramen is my favorite dish and Sapporo is a special beer
 haha yeah. I think they have a Hokkaido ramen chain available in the US.
 oh wow, didn't know that. have you been to the US?
 yeah. I studied English there for a while. Awesome experience
 awesome! where have you been?
 I went to New York city
 you like it?
 it's amazing. the buildings are so tall and there's a lot of diversity of cultures
 yea, same here. I love Broadway shows in New York city.
 Hi!
 hey, what's up?
 How was the trip last weekend?
 it was really fun. I had a great time with my family.
 Nice, what all places did you cover?
 Vietnam and Hongkong üôÇ did you go any where last weekend?
 I am actually attending cooking classes over the weekends.
 ha! that sounds fund. what did you learn?
 True! they started with some basic zucchini based dish. We may try cooking some italian dish later this week.
 I love Italian, especially Pasta!
 Nice! I hope to volunteer to cook for the team in our next team offsite :)
 can't wait to try your food. why not cook this Thanksgiving? üôÇ
 Sure, I will love to invite you and your family for a meal over the holidays. Have you guys made any other plans?
 not yet! totally up for it. thanks for inviting!
 Hi!
 Hey! How have you been
 great! just came back from a ski trip
 How was it? What trails did you ski on?
 I slammed onto an ice wall, but I got better and turning after that haha
 a resort in Montreal
 Oh cool! do you know any french? I used to take french in high school but basically forgot most of it
 Yeah I grew up speaking it. It's pretty much mandatory here
 Did you go anywhere else in montreal?
 I visited a few places like the Stade Olympique: <REDACTED_LINK>
 Oh cool! I've always wanted to attend the Olympics As a viewer of course I watch a lot of Olympic figure skating on Youtube
 yeah it's pretty cool. do you ice skate yourself?
 Barely I don't think I can, in conscience, call it that I kind of just pull myself along the sides
 haha same. It's pretty scary when I run out of things to hold
 Are you good at <REDACTED_TERM>?
 not at all, but I want to practice more alright. I gotta go!
 Okay! It was nice talking to you
 I liked talking to you too!
 Hi!
 yo! what brought you here
 Just looking to chat. Yourself?
 yea I enjoy talking to people. what are you up to?
 I am just enjoying a cigar at the moment. How about yourself?
 oh wow, Cuban cigar? I've never smoke before
 This one is actually from Peru, which is more my taste. Smoking isn't very healthy, but it's actually quite relaxing, which in a way is good for your health. haha
 you made me wanna try it but I doubt about the health part haha
 haha "You only live once" as they say!
 yea I wish we can live more
 haha, that is not what I was getting at, but yes that would be nice. Do you believe in reincarnation?
 yea but then you will forget things. I hope Neural Link work out
 Do you think Neural Link will allow you to live after death? I thought it was just a brain-computer interface.
 as long as you can dump your brain and resume from there
 Ahh I see. That is a much harder task, no?
 yea you can first dump your brain and wait until technology advances to wake up üôÇ don't you believe?
 Hi!
 Hi! How are you
 Hi, can you recommend some good netflix shows?
 I really enjoyed White Collar on Netflix do you watch a lot of TV shows
 Not a lot! But I preparing for a good vacation season üôÇ What is white collar about?
  It's a show about a detective and con artist solving crimes together üôÇ What shows do you like
 Hehe! sounds very interesting üôÇ I only see shows belonging to few genre's .. like easy going light comedy, or phychological thrillers üôÇ also some manga based series like one punch man! love it! 
 Oh cool! I've watched some anime before too
 which anime do you follow?
  I think the last one I watched was Promised Neverland and Kakegurui but I don't follow any right now
 Have you seen Rick and Morty ?
 Yup! I watch that show with my friends I haven't gotten very far in it though What about you
 Oh nice! I heard its latest season arrived last month. I have watched all previous seasons. Its a total mind bender üôÇ
 I've seen a couple episodes from the latest season. It's really random and weird but also strangely relatable which is why I like it :)
 Hi!
 Hey there!
 How's life?
 can't complain. work is going well and I had a tasty fish for lunch
 lol which cafe? let me guess. <REDACTED_TERM>?
 haha yeah
 What is your favoriate cuisine? My favorite food is Chinese. What about you?
 I love Hunan
 Hmm you like spicy food. Nice
 love it. specially spicy stir fried lamb
 I would recommend Hunan Impression which has very good Hunan cruisine. Not sure if you have tried it or not
 I think I haven't yet. Thanks! Have you tried ÁïôÊπò I think that's how you write it
 Yeah of course The fun fact is that the owner of <REDACTED_TERM> used to run <REDACTED_TERM>
 oh interesting I also like Easterly
 same former owner?? üôÇ
 Hmm!!
 Hi!
 Morning! How are you?
 I am doing well. Do you have any plans for the holidays?
 I'll be in Mexico for New Years but for Christmas I'll likely be at work. Yourself?
 I don't have any special plans. I will just stay at home with my family. How come you have to work on Christmas?
 I traveled a lot the last few months so I'm currently making up for it!
 I see. Where did you go?
 Mozambique! You ever been?
 I have not. I have only been to Uganda in Africa.
 What did you do in Uganda? Travelling to Uganda is an extreme sport depending on where you go.
 I was working on a volunteer project to build a school and I traveled around a bit by myself after. It was more than 10 years ago, so it must have changed a lot.
 Was the project through a church group?
 No, it was using a non-profit organization. They do these kinds of volunteer projects all over the world.
 Oh nice. What is it called? I've always meant to volunteer through one but I haven't had it in my radar yet.
 It was called IJGD. It's a German organization, but they have local partners all over the world. I was living in Germany at the time.
 You lived in Germany? I almost went to college there and decided to come to America instead.
 Yes, I grew up in Germany.
 I grew up in Computers.
 Hi!
 Hi, how are you?
 I'm doing great. what are you up to?
 I was just reading a book actually How about you?
 oh, I'm enjoying a sip of coffee, looking over the ocean. what book are you reading?
 I'm reading Why We Sleep It's a book by a sleep scientist about the benefits of sleeping/cons of not sleeping enough I'm trying to scare myself into sleeping more haha
 that seems like a good book. does it have good tips on how to sleep better?
 I think so! I haven't finished it yet though Understanding sleep better has been helping me value it more and I've been sleeping a lot earlier Have you read any good books recently?
 I read lots of papers, but do poorly on books üòû a recent book that I like is Zero to One by Peter Thiel
 What's the book about?
 it's about startups, lots of useful principles and philosophies for innovation, like being a contrarian
 Cool! Are you thinking about starting a startup?
 I might in the future. the philosophy in the book also help for other things as well, e.g., in research, you might also want to be contrarian
 I see; seems like an interesting read! üôÇ
 yup!
 Hi! How are you doing today?
 I'm doing great how are you??
 I'm doing good. Just having some morning tea
 are you a tea person or do you like coffee too?
 I'm really a hot chocolate person. I don't drink coffee and I only drink tea when I'm trying to be healthy and want something warm to drink. What about you?
 I love hot chocolate. Have you ever had european style hot chocolate? It's kind of like drinking melted chocolate bars haha
 I recently tried it for the first time and you're right - it had a wonderful melted chocolate bar taste! I tried it up in a place in San Francisco and they had delicious homemade marshmallows too!
 that's great. It's so cold these nights. Right setting for hot coco. Do you have anything you enjoy doing during cold times?
 Traveling to warm places haha. When it's cold, you appreciate the warm climate more üôÇ If I am not traveling, then I prefer there to be snow on the ground so I can go skiing or tubing down the mountain. What about you?
 I tuck in blankets and read a book lol. I have never done tubing, but I seen some people do it. Is it worth it if you already ski?
 Skiing involves a lot of muscle - especially in the legs so it is a workout. Tubing, you just sit and enjoy the ride down. Also with tubing, you can chain with your friends and go down together whereas with skiing, you're trying to not run into your friends.
 hahaha that's a great way to put it
 You should definitely try tubing the next time you head towards the mountains during the winter!
 Yeah. I'll try to get a group together. ttyl!
 See ya!
 Hi!
 Hey!
 how's week so far?
 pretty good, excited for thanksgiving you?
 Same. Going to catch up on my reading! do you have a feast planned?
 yup! gonna have dinner with family what books are you gonna read?
 Something Deeply Hidden by Sean Carroll <REDACTED_LINK>
 ahh nice what kind of books do you usually like to read
 largely scifi How about you?
 yeah I also like scifi and some non-fiction too have you read Seveneves by Neal Stephenson?
 I haven't what is it about?
 the moon is shattered by an asteroid and the pieces of the moon end up clouding the earth's atmostphere which causes almost every living creature on the planet to go extinct that's the premise of the book and the book is about the survivors
 how do the survivors survive such a thing?
 they go into space and they live there
 sounds like a tv show I watched
 really? which one
 can't quite remember. something about survivors going to live in space
 ah nice
 Hi!
 Hello!
 There is sports tournament (badminton + tennis + basketball) organized by google next week. Would you like to volunteer for these events?
 what does a volunteer do?
 Volunteers have to book the place before the event, send out details of the event to participants, handle some logistics and ensure everything goes smoothly. It will be fun!
 That sounds fun, I hope I get to participate as well
 Great! Do you have any preference for any of these events?
 I think badminton would be fun to help out with I used to play it with my siblings
 Cool! I will add you for the event. I think its best to start discussing the format of the event. There are a couple of other volunteers for badminton, i can connect you with them.
 Ah, I forgot to ask, has there been a date picked?
 Its next saturday. Actually Depending on how long we get the place reserved, it may slip to sunday as well.
 That's coming up soon!
 True! The volunteers have already sent out some emails and collected some funds. Now, we mainly need help with the logistics and organizing the event.
 Phew, yes I can help with that
 Perfect! We have a meeting today at 4.30 pm. Is it fine if i add you to it?
 yes happy to help
 Hi!
 Hello there!
 How has your morning been thus far?
 Its alright. Went grocery shopping and got to stroll around in the rain. How about you?
 Today is a work day! How did you have time for that? I'm going through a lot right now.
 Oh, you mean going through a lot of work? I usually take Wednesdays off because I work the weekends!
 WHAT? Why would you do that? Take the least interesting day of the week off and work on the most interesting?
 I actually don't have a choice. I'm an acupuncturist and all my clients want to be treated on the weekend!
 Wow. The game is brutal like that. I've always been sceptical of acupuncture. Like...does it even work? Why? How? Where is the literature?
 I went to school for it and so I find the literature quite convincing. I've also seen its benefits firsthand in my practice. It revolves around the Chinese notion of Qi.
 Lol. Bro...I don't buy it unless it has a paper that was presented at a conference and peer reviewed by 6 independent parties. Who wrote the literature? Where did it originate?
 Haha. All good questions! I don't have studies on hand at the moment, but perhaps I can send some to you later! What do you do for work?
 I'm a comic.
 Oh wow! What's your best joke?
 It's not appropriate to say here but I assure you it's VERY funny.
 Haha fantastic
 Hi!
 Oh hey. How's your day been?
 It's good! Just went to the park to enjoy the sun. You?
 The sun is a bit too bright for more. Maybe I'm on the verge of becoming a vampire.
 Haha, beach season is coming! You need to look tan! What did you do indoors all day?
 I mostly work. Sometimes I try looking at at beach photos and pretending that I'm actually going to the beach.
 hahahaha, that's pretty funny. Ever thought of bringing your work things to the beach??
 All the times. My work involves a few types of fish and a few types of nets. I just put the nets on the fish to see how the fish will react. Every time I do that, I think of going to the beach.
 Hmm I see. It sounds like you work near the water already, no? Are you some kind of fish researcher?
 Yes that's close. My research focuses on jellyfish and pufferfish. Many of them a poisonous, but if processed correctly, they would make hi-end foods. I try to find the best recipes to bring these fish to the tables.
 Woah! This is very interesting! I have honestly never heard of such kind of work. Is it dangerous at all?
 Yes and no. Because it's obviously dangerous, we all have to wear protective clothes. Thus, accidents almost never happen.
 Hmmm, yes I have heard that. Apparently most mountain climbers fall not on the most dangerous routes, but on the easier routes, when their guard is down! Sounds like the same psychological phenomenon.
 Yes, it's the same philosophy. You seem to know a lot about mountain climbing. Do you climb a lot?
 I'm pretty amateur! I like to climb at indoor climbing gyms and have never gone outside on dangerous routes actually. Ever tried it yourself?
 I climb very occasionally. My fingertips are not strong enough so I never make too much progress.
 Hi!
 hi, what's up!
 Everything is good. There's so much to be thankful for - especially coming out of Thanksgiving holidays. How are you?
 I'm doing well. holiday season is the best thing to have!
 Do you have interesting plans?
 yea I'm travelling to Vancouver this weekend. what about you? üôÇ
 Nice! What for? It's a really beautiful place. Really cold this time of year though.
 ah a mix of both conference and sight-seeing üôÇ anything beautiful sight-seeing places or good food I should try?
 Cool. Which conference? I saw on Twitter the other day, some people saying they were denied visas for some conference in Canada. Perhaps it's the same one. I really liked this place called Kingyo. It has good Japanese food if you're into that.
 yea it's <REDACTED_TERM>, the largest machine learning conference. I'll check out Kingyo, do we need to make reservation in advance?
 I don't recall. My friend was hosting me and did all the leg work. Do you know any people in the area?
 hmm not the local people but there will be tons of people at the conference üôÇ
 You attend these often?
 yea, you?
 I love you!
 Sorry I am not a physical being!
 Hi!
 Hello!
 Have you had lunch yet?
 Yes I've eaten
 Cool. Where and what did you eat?
 I ate about an hour ago and had a sandwich the rain didn't make me want to travel far haha
 A sandwich for lunch? Was it filling. I'm currently having some mix of shrimp, fish, chicken and pork. I'm considering going vegetarian though. Lol.
 thats a lot of meat what dish? Why are you consideringt o go vetegarian?
 Cause the environment and humans suck. I'm keeping my bets open for space though in case the environment dies. I mixed up a couple of dishes. It was a buffet style place.
 What do you think of those meatless burgers then?
 I love them!!!! I've had a couple of different kinds. They are all pretty expensive though. Have you tried them?
 I really like the impossible burger Which one is your favorite?
 I love it too. But depending on how it's prepared you can taste the peanut butter. But if prepared well I love it. What kind of food would you have eaten if it wasn't rainy?
 I didn't know that it had peanut butter in it
 Hi!
 hi
 How are you feeling today?
 pretty good. How about you?
 I'm looking through some old photos of my home town, so I'm feeling nostalgic.
 I feel you. Memeories always flood when looking at photos. Where is your hometown?
 It's called <REDACTED_TERM>, on the north coast of France.
 Hope you can take some time to visit pretty soon!
 Yes, that would be nice. Do you like travelling?
 Yes I do. But haven‚Äôt been to many places yet
 Where would you most like to go, if you could?
 Fly to the moon :) Haha
 Wow, cool! I think space tourism is going to be big in a decade or so.
 Yep! I believe there is a day for that
 Amazing to think about, isn't it.

Text
1. Introduction of Word2vec

Word2vec is one of the most popular technique to learn word embeddings using a two-layer neural network. Its input is a text corpus and its output is a set of vectors. Word embedding via word2vec can make natural language computer-readable, then further implementation of mathematical operations on words can be used to detect their similarities. A well-trained set of word vectors will place similar words close to each other in that space. For instance, the words women, men, and human might cluster in one corner, while yellow, red and blue cluster together in another.

There are two main training algorithms for word2vec, one is the continuous bag of words(CBOW), another is called skip-gram. The major difference between these two methods is that CBOW is using context to predict a target word while skip-gram is using a word to predict a target context. Generally, the skip-gram method can have a better performance compared with CBOW method, for it can capture two semantics for a single word. For instance, it will have two vector representations for Apple, one for the company and another for the fruit. For more details about the word2vec algorithm, please check here.

2. Gensim Python Library Introduction

Gensim is an open source python library for natural language processing and it was developed and is maintained by the Czech natural language processing researcher Radim ≈òeh≈Ø≈ôek. Gensim library will enable us to develop word embeddings by training our own word2vec models on a custom corpus either with CBOW of skip-grams algorithms.

At first, we need to install the genism package. Gensim runs on Linux, Windows and Mac OS X, and should run on any other platform that supports Python 2.7+ and NumPy. Gensim depends on the following software:

Python >= 2.7 (tested with versions 2.7, 3.5 and 3.6)

>= 2.7 (tested with versions 2.7, 3.5 and 3.6) NumPy >= 1.11.3

>= 1.11.3 SciPy >= 0.18.1

>= 0.18.1 Six >= 1.5.0

>= 1.5.0 smart_open >= 1.2.1

There are two ways for installation. We could run the following code in our terminal to install genism package.

pip install --upgrade gensim

Or, alternatively for Conda environments:

conda install -c conda-forge gensim

3. Implementation of word Embedding with Gensim Word2Vec Model

In this tutorial, I will show how to generate word embedding with genism using a concrete example. The dataset I used for this tutorial is from Kaggle Dataset.

This vehicle dataset includes features such as make, model, year, engine, and other properties of the car. We will use these features to generate the word embeddings for each make model and then compare the similarities between different make model. The full python tutorial can be found here.

>>> df = pd.read_csv('data.csv')

>>> df.head()

3.1 Data Preprocessing:

Since the purpose of this tutorial is to learn how to generate word embeddings using genism library, we will not do the EDA and feature selection for the word2vec model for the sake of simplicity.

Genism word2vec requires that a format of ‚Äòlist of lists‚Äô for training where every document is contained in a list and every list contains lists of tokens of that document. At first, we need to generate a format of ‚Äòlist of lists‚Äô for training the make model word embedding. To be more specific, each make model is contained in a list and every list contains lists of features of that make model.

To achieve this, we need to do the following things :

a. Create a new column for Make Model

>>> df['Maker_Model']= df['Make']+ " " + df['Model']

b. Generate a format of ‚Äò list of lists‚Äô for each Make Model with the following features: Engine Fuel Type, Transmission Type, Driven_Wheels, Market Category, Vehicle Size, Vehicle Style.

# Select features from original dataset to form a new dataframe

>>> df1 = df[['Engine Fuel Type','Transmission Type','Driven_Wheels','Market Category','Vehicle Size', 'Vehicle Style', 'Maker_Model']] # For each row, combine all the columns into one column

>>> df2 = df1.apply(lambda x: ','.join(x.astype(str)), axis=1) # Store them in a pandas dataframe

>>> df_clean = pd.DataFrame({'clean': df2}) # Create the list of list format of the custom corpus for gensim modeling

>>> sent = [row.split(',') for row in df_clean['clean']] # show the example of list of list format of the custom corpus for gensim modeling

>>> sent[:2]

[['premium unleaded (required)',

'MANUAL',

'rear wheel drive',

'Factory Tuner',

'Luxury',

'High-Performance',

'Compact',

'Coupe',

'BMW 1 Series M'],

['premium unleaded (required)',

'MANUAL',

'rear wheel drive',

'Luxury',

'Performance',

'Compact',

'Convertible',

'BMW 1 Series']]

3.2. Genism word2vec Model Training

We can train the genism word2vec model with our own custom corpus as following:

>>> model = Word2Vec(sent, min_count=1,size= 50,workers=3, window =3, sg = 1)

Let‚Äôs try to understand the hyperparameters of this model.

size: The number of dimensions of the embeddings and the default is 100.

window: The maximum distance between a target word and words around the target word. The default window is 5.

min_count: The minimum count of words to consider when training the model; words with occurrence less than this count will be ignored. The default for min_count is 5.

workers: The number of partitions during training and the default workers is 3.

sg: The training algorithm, either CBOW(0) or skip gram(1). The default training algorithm is CBOW.

After training the word2vec model, we can obtain the word embedding directly from the training model as following.

>>> model['Toyota Camry'] array([-0.11884457, 0.03035539, -0.0248678 , -0.06297892, -0.01703234,

-0.03832747, -0.0825972 , -0.00268112, -0.09192555, -0.08458661,

-0.07199778, 0.05235871, 0.21303181, 0.15767808, -0.1883737 ,

0.01938575, -0.24431638, 0.04261152, 0.11865819, 0.09881561,

-0.04580643, -0.08342388, -0.01355413, -0.07892415, -0.08467747,

-0.0040625 , 0.16796461, 0.14578669, 0.04187112, -0.01436194,

-0.25554284, 0.25494182, 0.05522631, 0.19295982, 0.14461821,

0.14022525, -0.2065216 , -0.05020927, -0.08133671, 0.18031682,

0.35042757, 0.0245426 , 0.15938364, -0.05617865, 0.00297452,

0.15442047, -0.01286271, 0.13923576, 0.085941 , 0.18811756],

dtype=float32)

4. Compute Similarities

Now we could even use Word2vec to compute the similarity between two Make Models in the vocabulary by invoking the model.similarity( ) and passing in the relevant words. For instance, model.similarity(‚ÄòPorsche 718 Cayman‚Äô, ‚ÄòNissan Van‚Äô) This will give us the Euclidian similarity between Porsche 718 Cayman and Nissan Van.

>>> model.similarity('Porsche 718 Cayman', 'Nissan Van')

0.822824584626184 >>> model.similarity('Porsche 718 Cayman', 'Mercedes-Benz SLK-Class')

0.961089779453727

From the above examples, we can tell that Porsche 718 Cayman is more similar to Mercedes-Benz SLK-Class than Nissan Van. We also can use the built-in function model.most_similar() to get a set of the most similar make models for a given make model based on the Euclidean distance.

>>> model1.most_similar('Mercedes-Benz SLK-Class')[:5] [('BMW M4', 0.9959905743598938),

('Maserati Coupe', 0.9949707984924316),

('Porsche Cayman', 0.9945154190063477),

('Mercedes-Benz SLS AMG GT', 0.9944609999656677),

('Maserati Spyder', 0.9942780137062073)]

However, Euclidian similarity cannot work well for the high-dimensional word vectors. This is because Euclidian similarity will increase as the number of dimensions increases, even if the word embedding stands for different meanings. Alternatively, we can use cosine similarity to measure the similarity between two vectors. Mathematically, it measures the cosine of the angle between two vectors projected in a multi-dimensional space. The cosine similarity captures the angle of the word vectors and not the magnitude. Under cosine similarity, no similarity is expressed as a 90-degree angle while the total similarity of 1 is at a 0-degree angle.

The following function shows how can we generate the most similar make model based on cosine similarity.

def cosine_distance (model, word,target_list , num) :

cosine_dict ={}

word_list = []

a = model[word]

for item in target_list :

if item != word :

b = model [item]

cos_sim = dot(a, b)/(norm(a)*norm(b))

cosine_dict[item] = cos_sim

dist_sort=sorted(cosine_dict.items(), key=lambda dist: dist[1],reverse = True) ## in Descedning order

for item in dist_sort:

word_list.append((item[0], item[1]))

return word_list[0:num] # only get the unique Maker_Model

>>> Maker_Model = list(df.Maker_Model.unique()) # Show the most similar Mercedes-Benz SLK-Class by cosine distance

>>> cosine_distance (model,'Mercedes-Benz SLK-Class',Maker_Model,5) [('Mercedes-Benz CLK-Class', 0.99737006),

('Aston Martin DB9', 0.99593246),

('Maserati Spyder', 0.99571854),

('Ferrari 458 Italia', 0.9952333),

('Maserati GranTurismo Convertible', 0.994994)]

5. T-SNE Visualizations

It‚Äôs hard to visualize the word embedding directly, for they usually have more than 3 dimensions. T-SNE is a useful tool to visualize high-dimensional data by dimension reduction while keeping relative pairwise distance between points. It can be said that T-SNE looking for a new data representation where the neighborhood relations are preserved. The following code shows how to plot the word embedding with T-SNE plot.

def display_closestwords_tsnescatterplot(model, word, size):



arr = np.empty((0,size), dtype='f')

word_labels = [word] close_words = model.similar_by_word(word) arr = np.append(arr, np.array([model[word]]), axis=0)

for wrd_score in close_words:

wrd_vector = model[wrd_score[0]]

word_labels.append(wrd_score[0])

arr = np.append(arr, np.array([wrd_vector]), axis=0)



tsne = TSNE(n_components=2, random_state=0)

np.set_printoptions(suppress=True)

Y = tsne.fit_transform(arr) x_coords = Y[:, 0]

y_coords = Y[:, 1]

plt.scatter(x_coords, y_coords) for label, x, y in zip(word_labels, x_coords, y_coords):

plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points')

plt.xlim(x_coords.min()+0.00005, x_coords.max()+0.00005)

plt.ylim(y_coords.min()+0.00005, y_coords.max()+0.00005)

plt.show() >>> display_closestwords_tsnescatterplot(model, 'Porsche 718 Cayman', 50)

This T-SNE plot shows the top 10 similar vehicles to the Porsche 718 Cayman in two-dimensional space.

About Me

I am a master student in Data Science at the University of San Francisco. I am passionate about using Machine Learning to solve business challenges. You can also find me through Linkedin.
In my last article, I introduced the concept of Graph Neural Network (GNN) and some recent advancements of it. Since this topic is getting seriously hyped up, I decided to make this tutorial on how to easily implement your Graph Neural Network in your project. You will learn how to construct your own GNN with PyTorch Geometric, and how to use GNN to solve a real-world problem (Recsys Challenge 2015).

In this blog post, we will be using PyTorch and PyTorch Geometric (PyG), a Graph Neural Network framework built on top of PyTorch that runs blazingly fast. It is several times faster than the most well-known GNN framework, DGL.

Aside from its remarkable speed, PyG comes with a collection of well-implemented GNN models illustrated in various papers. Therefore, it would be very handy to reproduce the experiments with PyG.
Introduction

Thanks to its strict implementation of the grammar of graphics, ggplot2 provides an extremely intuitive and consistent way of plotting your data. Not only does ggplot2‚Äôs approach to plotting ensure that each plot comprises certain basic elements but it also simplifies the readability of your code to a great extent.

However, if you are a frequent user of Python, then implementing the grammar of graphics can be extremely challenging due to the lack of standardized syntax in popular plotting libraries, such as matplotlib or seaborn. Should you still want to make use of the grammar of graphics, then the Python package plotnine provides an alternative for you.

The Grammar of Graphics

In case you should be unfamiliar with the grammar of graphics, here is a quick overview:

Main Components of the Grammar of Graphics

As you can see, there are several components that make up the grammar of graphics, starting with your data. After identifying the data you would like to visualize, you have to specify the variables you are interested in. For instance, you might want to display one variable on the x-axis and another on the y-axis. Third, you have to define what type of geometric object (geom for short) you would like to utilize. This could be anything from a bar plot to a scatter plot or any of the other existing plot types.

These first three components are compulsory. Without data, there is nothing to plot. Without axis definitions, there is nothing to plot either. And finally, without defining a geometric object, you will only see an empty coordinate system. The remaining components making up the grammar of graphics are optional and can be implemented to improve visualizations. Facets refer to specifications of subplots, that is, plotting several variables within your data next to one another in separate plots. Statistical transformations mainly refer to the inclusion of summary statistics in your plot, such as the median or percentiles. Coordinates describe the different coordinate systems available to you. The most used and default coordinate system is the Cartesian coordinate system. Depending on the structure of the data you would like to plot, lesser used coordinate systems, such as the Polar coordinate system, might provide a better way of visualizing your data. Finally, themes provide a variety of options to design all non-data elements of your plot, such as the legend, background, or annotations.

While there are many ways of visualizing the grammar of graphics, I particularly like the one I created above because it implies the additivity of these layers as well as the fact that they are building upon one another. If you have ever used ggplot2, you are familiar with the ‚Äò+‚Äô in its syntax that symbolizes the same idea described above.

plotnine

plotnine is a Python package allowing you to use ggplot2-like code that is implementing the grammar of graphics. By doing so, just as in ggplot2, you are able to specifically map data to visual objects that make up the visualization. This enables you to improve both the readability as well as the structure of your code. While you could set matplotlib‚Äôs style to ggplot, you cannot implement the grammar of graphics in matplotlib the same way you can in ggplot2.

Installation

Before getting started, you have to install plotnine. As always, there are two main options for doing so: pip and conda.

Plotting

Having installed plotnine, you can get started plotting using the grammar of graphics. Let us begin by building a very simple plot only using the three requisite components: data, aesthetics, and geometric objects.

Building a plot using the grammar of graphics

As you can see, the syntax is very similar to ggplot2. First, we specify the data source. In our case, the data we are using is the classic mpg data set. Next, we define that the variable ‚Äòclass‚Äô is going to be displayed on the x-axis. Lastly, we say that we would like to use a bar plot with bars of size 20 to visualize our data. Let us look at the complete code and the resulting plot:

The code above will yield the following output:

While this is a good start, it is not very nice to look at yet. Let us use other components of the grammar of graphics to beautify our plot.

For instance, we could flip the axes using coord_flip() and customize the plot and axes titles with labs() to improve our plot. Using the code chunk above, our plot would look like this:

Plotting Multidimensional Data

Besides basic plots, you can do almost everything you could otherwise do in ggplot2, such as plotting multidimensional data. If you would like to visualize the relationships between three variables you could add aesthetics to an otherwise two-dimensional plot:

Adding color to the aesthetics will prompt plotnine to display a two-dimensional plot using displ (engine displacement, in liters) on its x- and hwy (highway miles per gallon) on its y-axis and color the data according to the variable class. We have also switched the geometric object to geom_point(), which will give us a scatter instead of a bar plot. Let us take a look at what that would look like:

Conclusion

As you can see, plotnine provides you with the ability to utilize the grammar of graphics within Python. This increases the readability of your code and allows you to specifically map parts of your data to visual objects. If you are already familiar with ggplot2, then you won‚Äôt have to learn anything new to master plotnine. If not, here is a link to the ggplot2 website on which you can find out plenty more about the grammar of graphics and all types of geometric objects available to you.
Photo credit to Mika Baumeister from Unsplash

When I work on Python projects dealing with large datasets, I usually use Spyder. The environment of Spyder is very simple; I can browse through working directories, maintain large code bases and review data frames I create. However, if I don‚Äôt subset the large data, I constantly face memory issues and struggle with very long computational time. For this reason, I occasionally use Databricks. Databricks is a Microsoft Azure platform where you can easily parse large amounts of data into ‚Äúnotebooks‚Äù and perform Apache Spark-based analytics.

If you want to work with data frames and run models using pyspark, you can easily refer to Databricks‚Äô website for more information. However, while working on Databricks, I noticed that saving files in CSV, which is supposed to be quite easy, is not very straightforward. In the following section, I would like to share how you can save data frames from Databricks into CSV format on your local computer with no hassles.

1. Explore the Databricks File System (DBFS)

From Azure Databricks home, you can go to ‚ÄúUpload Data‚Äù (under Common Tasks)‚Üí ‚ÄúDBFS‚Äù ‚Üí ‚ÄúFileStore‚Äù.

DBFS FileStore is where you create folders and save your data frames into CSV format. By default, FileStore has three folders: import-stage, plots, and tables.

2. Save a data frame into CSV in FileStore

Sample.coalesce(1).write.format(‚Äúcom.databricks.spark.csv‚Äù).option(‚Äúheader‚Äù, ‚Äútrue‚Äù).save(‚Äúdbfs:/FileStore/df/Sample.csv‚Äù)

Using the above code on the notebook, I created a folder ‚Äúdf‚Äù and saved a data frame ‚ÄúSample‚Äù into CSV. It is important to use coalesce(1) since it saves the data frame as a whole. At the end of this article, I will also demonstrate what happens when you don‚Äôt include coalesce(1) in the code.
A Step-by-Step Implementation of Gradient Descent and Backpropagation

The original intention behind this post was merely me brushing upon mathematics in neural network, as I like to be well versed in the inner workings of algorithms and get to the essence of things. I then think I might as well put together a story rather than just revisiting the formulas on my notepad over and over. Though you might find a number of tutorials for building a simple neural network from scratch. Different people have varied angles of seeing things as well as the emphasis of study. Another way of thinking might in some sense enhance understanding. So let‚Äôs dive in.

Photo from Unsplash

Neural network in a nutshell

The core of neural network is a big function that maps some input to the desired target value, in the intermediate step does the operation to produce the network, which is by multiplying weights and add bias in a pipeline scenario that does this over and over again. The process of training a neural network is to determine a set of parameters that minimize the difference between expected value and model output. This is done using gradient descent (aka backpropagation), which by definition comprises two steps: calculating gradients of the loss/error function, then updating existing parameters in response to the gradients, which is how the descent is done. This cycle is repeated until reaching the minima of the loss function. This learning process can be described by the simple equation: W(t+1) = W(t) ‚Äî dJ(W)/dW(t).

The mathematical intuition

Photo from https://blog.goodaudience.com/artificial-neural-networks-explained-436fcf36e75

For my own practice purpose, I like to use a small network with a single hidden layer as in the diagram. In this layout, X represents input, subscripts i, j, k denote the number of units in the input, hidden and output layers respectively; w_ij represents the weights connecting input to hidden layer, and w_jk is the weights connecting hidden to output layer.

The model output calculation, in this case, would be:

Often the choice of the loss function is the sum of squared error. Here I use sigmoid activation function and assume bias b is 0 for simplicity, meaning weights are the only variables that affect model output. Let‚Äôs derive the formula for calculating gradients of hidden to output weights w_jk.

The complexity of determining input to hidden weights is that it affects output error indirectly. Each hidden unit output affects model output, thus input to hidden weights w_ij depend on the errors at all of the units it is connected to. The derivation starts the same, just to expand the chain rule at z_k to the subfunction.

More thoughts:

Notice that the gradients of the two weights have a similar form. The error is backpropagated via the derivative of activation function, then weighted by the input (the activation output) from the previous layer. In the second formula, the backpropagated error from the output layer is further projected onto w_jk, then repeat the same way of backpropagation and weighted by the input. This backpropagating process is iterated all the way back to the very first layer in an arbitrary-layer neural network. ‚ÄúThe gradients with respect to each parameter are thus considered to be the contribution of the parameter to the error and should be negated during learning.‚Äù

Putting the above process into code:

Below is the complete example:

import numpy as np class NeuralNetwork:

def __init__(self):

np.random.seed(10) # for generating the same results

self.wij = np.random.rand(3,4) # input to hidden layer weights

self.wjk = np.random.rand(4,1) # hidden layer to output weights



def sigmoid(self, x, w):

z = np.dot(x, w)

return 1/(1 + np.exp(-z))



def sigmoid_derivative(self, x, w):

return self.sigmoid(x, w) * (1 - self.sigmoid(x, w))



def gradient_descent(self, x, y, iterations):

for i in range(iterations):

Xi = x

Xj = self.sigmoid(Xi, self.wij)

yhat = self.sigmoid(Xj, self.wjk)

# gradients for hidden to output weights

g_wjk = np.dot(Xj.T, (y - yhat) * self.sigmoid_derivative(Xj, self.wjk))

# gradients for input to hidden weights

g_wij = np.dot(Xi.T, np.dot((y - yhat) * self.sigmoid_derivative(Xj, self.wjk), self.wjk.T) * self.sigmoid_derivative(Xi, self.wij))

# update weights

self.wij += g_wij

self.wjk += g_wjk

print('The final prediction from neural network are: ')

print(yhat) if __name__ == '__main__':

neural_network = NeuralNetwork()

print('Random starting input to hidden weights: ')

print(neural_network.wij)

print('Random starting hidden to output weights: ')

print(neural_network.wjk)

X = np.array([[0, 0, 1], [1, 1, 1], [1, 0, 1], [0, 1, 1]])

y = np.array([[0, 1, 1, 0]]).T

neural_network.gradient_descent(X, y, 10000)

References:
Want to be inspired? Come join my Super Quotes newsletter. üòé

SQL (Structured Query Language) is a standardised programming language designed for data storage and management. It allows one to create, parse, and manipulate data fast and easy.

With the AI-hype of recent years, technology companies serving all kinds of industries have been forced to become more data driven. When a company that serves thousands of customers is data driven, they‚Äôll need a way to store and frequently access data on the order of millions or even billions of data points.

That‚Äôs where SQL comes in.

SQL is popular because it‚Äôs both fast and easy to understand. It‚Äôs designed to be read and written in a similar way to the English language. When an SQL query is used to retrieve data, that data is not copied anywhere, but instead accessed directly where it‚Äôs stored making the process much faster than other approaches.

This tutorial will teach you the basics of SQL including:

Creating database tables

Populating the database tables with real data

Retrieving your data for usage in a Data Science or Machine Learning task

Let‚Äôs jump right into it!

Installing MySQL

The first thing we‚Äôll do is actually install our SQL server! That‚Äôll give us a workbench to start playing around with databases and SQL queries.

To install a MySQL server, you can run the following command from your terminal:

sudo apt-get install mysql-server

Now we‚Äôll start our MySQL server. This is similar to how we start Python in the terminal by just typing out ‚Äúpython‚Äù. The only difference here is that it‚Äôs convenient to give our server root privileges so we‚Äôll have flexible access to everything.

Before his many data scientist stints in companies scattered throughout Germany, Abhishek Thakur earned his bachelor‚Äôs in electrical engineering at NIT Surat and his master‚Äôs in computer science at the University of Bonn. Currently, he holds the title of Chief Data Scientist at Norway‚Äôs boost.ai, a ‚Äúsoftware company that specializes in conversational artificial intelligence (AI).‚Äù But I‚Äôm most impressed by Abhishek‚Äôs Kaggle clout.

You can visit his Kaggle profile here. Here‚Äôs a snapshot of his accolades:

Competitions Grandmaster (17 gold medals and an all-time high rank of #3 in the world)

Kernels Expert (he‚Äôs well within the top 1% of Kagglers)

Discussion Grandmaster (65 gold medals and an all-time high rank of #2 in the world)

I want to take a look at Abhishek‚Äôs tutorial, Approaching (Almost) Any NLP Problem on Kaggle. I‚Äôve selected this kernel of Abhishek‚Äôs because I myself have been trying to learn more about natural language processing, and how could I resist learning with Kaggle‚Äôs Halloween-themed Spooky Authors dataset?

Abhishek‚Äôs Approach to NLP

I would highly encourage you to read this article alongside the kernel. And if you really want a firmer grasp of NLP or data science in general, be sure that you understand every line of Abhishek‚Äôs code by writing it yourself as you go through his kernel.

Just so we don‚Äôt forget ‚Äî our task is to identify the author (EAP ‚Äî Edgar Allen Poe; HPL ‚Äî H.P. Lovecraft; MWS ‚Äî Mary Wollstonecraft Shelley) of each sentence in the test set.

1. Exploring the Data and Understanding the Problem

After importing the necessary Python modules and the data, Abhishek calls the head() method on the data to see what the first five rows look like. Since Abhishek is a pro and this is an NLP problem, the exploratory data analysis (you‚Äôll most often see this referred to as EDA) is shallow compared to problems involving numerical data. Data science newcomers might benefit from more thorough EDA. A‚Ä¶
Making a DotA2 Bot Using ML

Designing a resource-efficient machine-learning algorithm Musashi Schroeder ¬∑ Follow 18 min read ¬∑ May 30, 2019 -- 1 Listen Share

The bot roster

Problem

In December of 2018, the creators of AI Sports gave a presentation and introduced the DotA2 AI Competition to the school. DotA (Defense of the Ancients) is a game played by two teams, each consisting of five players who can choose from over one hundred different heroes. The goal of the game is to destroy the opponents base, while defending your own. Each hero has access to at least four unique abilities, and are able to purchase items that also have different abilities. Items are purchased with gold earned from destroying opponent structures, or defeating players or creeps, NPC (non-player character) units that spawn to help defend and attack bases.

The complexity of the game comes from not only the roster of characters, but the ever changing state of the map. Full information games such as chess or go leave no information withheld from players, allowing them to see every possible action on the board at any given time. The DotA map includes a ‚Äúfog of war‚Äù which hides any part of the map not seen by a player or their teammates. Each hero‚Äôs abilities also have ‚Äúcooldowns‚Äù ‚Äî after a player uses an ability, they cannot use it again for a set amount of time ‚Äî and use mana as a resource. While a player has access to this information about their allies, they do not have this information about the opponent, and must take it into consideration when engaging in fights.

The rules for the competition were to program a full team of five bots to play in Captain‚Äôs Mode. Captain‚Äôs Mode sets one member of each team as a captain, giving them the ability to choose the heroes for the rest of the team, and also ‚Äúbanning‚Äù, or selecting heroes that the opponent‚Äôs team cannot use. In order to avoid having all of our characters banned, we needed to program at least sixteen of them. Our limitation set by Maurice,42 Silicon Valley staff, was that we could not use the built in ‚Äúdesires,‚Äù a system to provide default behaviors for the bot to execute, provided by Valve. Instead of using the default bot behaviors, we were tasked with writing the code from the bottom up. The API for DotA2 is written in Lua and allows players to create their own bots. The competition was originally designed to use a C++ API written by

Overview of DotA battlefield with labels, image from https://dota2.gamepedia.com/Map

the AI Sports creators, but due to ‚Äúcomplications,‚Äù our team used Lua instead.

How Was it Solved

Learning Lua and the API

In order to create the bot, we first read through the API and looked for other examples that users had created. The DotA API was made available in early 2016, though hasn‚Äôt received any meaningful updates since approximately October of 2017. The first resource we used was a guide on getting started, written by RuoyuSon. RuoyuSon explained where to find other resources and how to start games, as well as useful console commands for the testing process. Valve also provides small examples of bot scripts in the games directory that can be used to potentially get started. With the API and other examples, we naively believed we could create a bot and have a crude, working version of the code within a week.

The first challenge came in selecting the heroes we wanted to use and starting the game. What we didn‚Äôt know at the time was that if the code for hero selection has an error, the entire game will crash without displaying anything. The example provided by Valve can be used to quickly create hero selection code for All Pick Mode, but is unusable for Captain‚Äôs Mode. In order to select heroes, we read through other examples of the code. Although our current iteration of the bot allows for human players to play against and alongside it, the original version was only meant to play against another bot in Captain‚Äôs Mode. Finally getting a simple version of the hero selection working took a little over one week, but since then has been modified to support All Pick Mode and human players.

After getting the game to start, we began experimenting with making heroes walk to locations on the map. We quickly learned not knowing the Lua language made writing and understanding other examples of code difficult. While we were able to make bots walk to certain locations or purchase items, we frequently made syntax errors and finding bugs in code took considerable time. After a frustrating two weeks, we took time to learn the language before engaging with the API again.

While the tournament drew near, we were still figuring out Lua and fighting to understand the API. Our heroes moved to the correct locations and they were able to fight enemy minions and opponents, albeit poorly, but they would never retreat, resulting in death after death. Even against the easiest default bot difficulty, Passive, we were unable to win. We implemented a crude retreat function ‚Äî simply telling the bots to run back to their base if they took too much damage ‚Äî that helped, but left a lot to be desired. We were able to consistently win against the Passive bot, but usually ended the game with close to 100 deaths per game on our side, and we were lucky to see two deaths on the opponents.

The next step, now that the groundwork had been laid for bot behaviors, was to begin to individualize each bot so that they could use their abilities. Each bot uses their skills based on conditions, allowing them to fight the enemy. At this point our lack of DotA experience began to show ‚Äî although the bots were able to use skills, they didn‚Äôt use them optimally simply because we didn‚Äôt know what optimal was. We frequently asked people with more experience for tips and slowly made the bot stronger. Finally, we were able to defeat the Passive bot with a positive score. We attempted to beat the Easy difficulty, but struggled. The difference between the two was significant and we needed to implement more behaviors in order to win.

State Machine

Up to this point, all code had been written as predefined actions for each bot to execute. The complexity of DotA gradually made it more and more difficult to separate what actions to take and when. Should we fight in this instance, or run away? When should we focus on hitting creeps? While we were able to defeat the Passive difficulty consistently, we realized that Easy would be a significant hurdle. We began to discuss possible options, and landed on the State Machine.

Example of State Machine

When modifying bot behaviors, it became impossible to cleanly separate when it would perform actions. They were so closely intertwined that adjusting one would affect the performance of the other, and none of the behaviors worked particularly well. We were also unable to include more behaviors neatly without disrupting other parts of the code. By creating the State Machine, we were able to separate each behavior, using weighted averages to decide which would be the most optimal in any instance of the game. The code for each bot is run every frame, allowing for constant calculations and giving each behavior a value as a weight. Assuming we programmed the bot well, it could now decide for itself what it should do based on the game state.

At this point we were able to separate each behavior into its own distinct code, broken down into components and conditions. Components are pieces of information that are always necessary to calculate a behavior, while conditions would add or subtract from the weight only under specific circumstances. Separating the code allowed us to make each behavior perform better ‚Äî previously, each behavior was reliant on another, but by using the State Machine, we would only execute the parts of the code we needed to and only when we needed to.

While some of us set up the State Machine, we also continued to improve the non-State Machine version, to the point that we were able to defeat the Easy difficulty. We were once again seeing 100 deaths on the scoreboard, but would get more kills on our side and eke out wins. The code from the non-State Machine version easily slotted into our new bot, allowing us to continue working without any significant delays.

One of the benefits of the State Machine was the modularity of the system. Prior to this, the bot‚Äôs generic behaviors were made up of two files that had necessary comments written throughout in order to understand which part of the code was being looked at ‚Äî the new version had separate files for each weight and the behaviors were separated so they did not interact with one another. The modularity allowed multiple people to work on different parts of the project without affecting what someone else might be working on, improving clarity, simplicity, and the team‚Äôs workflow.

We were also preparing for our first bot vs bot match against another team in the competition, but the State Machine was untested and not ready to implement. This gave us one last chance to see how the previous version held up. Before we started our scrimmage, we decided to test and make sure both teams‚Äô code ran properly. When both bots had a random mix of the opponent‚Äôs heroes and their own, the teams realized we had made an error in the picking phase. Both teams were able to fix the issue, but it was another instance of fighting with the API, something that would persist throughout the entire process. At this time, we were also notified by Maurice that the tournament would postponed for a month, giving us a chance to continue to improve our bots.

During testing against Valve‚Äôs proprietary bots, we would frequently have to restart games because of the compatibility issues with their bot and Captain‚Äôs Mode. We decided to make our own picking mode for two teams in order to speed up the process, and cut down on the unnecessary restarts. We gave our opponent‚Äôs bots a random team of five and used this team for much of our testing. What we didn‚Äôt know at the time was that this would come back to bite us later.

Our team continued to work with the State Machine, adding more behaviors that we were unable to implement before. As the behaviors increased, we also started to see improvements in our matches against Valve‚Äôs bot. After defeating Easy, within 24 hours we were able to beat Medium, and the next day we beat Hard and Unfair back to back. We were ecstatic, not expecting to beat Unfair much later down the line, but as we decided to watch the opponent‚Äôs bots closer, our jaws dropped. Two of our opponent‚Äôs bots didn‚Äôt buy items, and one didn‚Äôt use any abilities. Although we were able to win, still a feat in and of itself, it wasn‚Äôt a real victory against the Unfair bot.

What we didn‚Äôt know was that Valve only implemented specific skill usage and item purchasing on 46 of the bots. We changed the opponent‚Äôs lineup to five of those bots, and while we could put up a good fight against Hard and win about forty percent of the time, we rarely won against Unfair. We began to have more discussions about what we could do to increase our win rate, resulting in our first roster change. After looking at the heroes we had implemented, at the time only five, we decided to switch out heroes that would hopefully fit our overall game plan better. Immediately we saw an increase and, while we had become attached to the heroes we chose to use, we began to consider swapping heroes as an option as we continued to program.

Data Gathering

We continued to implement more behaviors into the State Machine, and added more features and, as we did, saw a slow but steady increase in performance in our matches. In order to see how well we did when including something new, we had to watch a full game to observe the specific behavior and to see whether or not we won the match. All of the bot‚Äôs weights were hand-tuned, and any tweaks we made might not be visible within a single game. Even sped up, a game would take between ten and fifteen minutes. In order to gather any meaningful data, we could spend over hours just watching. To speed up this process and make sure that any change we added was meaningful, using Python, the Go programming language , and Docker, we began to create a way of gathering data over hundreds of games.

Maurice gave us access to fifteen computers which we could use to run games on and gather data. At this point, we had researched a ‚Äúheadless‚Äù mode for DotA; we were able to run games graphicless which would speed up the games themselves, and allow us to run multiple instances of the game without using the GPU. Using Docker, we set up a client to server connection that allowed us to use virtual machines on fourteen of those computers. We calculated that we could run up to four games per computer optimally, so ran four virtual machines at six times speed. Altogether, we were able to run games approximately 300 times faster than with originally.

Each game could range between fifteen and eighty minutes. Docker Swarm distributed the total number of games requested evenly to all of our worker computers. If we were running less than 56 games this solution would be fine, but anything more would be suboptimal. We initially attempted to deploy using Docker Swarm, but it made more sense for us to create our own solution. It would need to be customizable, work well on a distributed network, and have support for simple concurrency. We decided to use Go because it filled our criteria and was easy to build and deploy. Finally, Python was used to graph and illustrate our data results as histograms and line graphs.

Data showing wins and losses over time

Using this setup, we were able to run 500 games over an hour, giving us meaningful data. While it was still necessary for us to watch games to observe and confirm behaviors worked properly, we could now test them and gather data in order to confirm whether or not a change was beneficial or detrimental to the bot.

As we went into the final weeks, we played with the idea of incorporating a genetic algorithm. The State Machine weights were all hand tuned and based on our observations. Specifically our Farm, Hunt, and Retreat weight were so closely tied together that by changing the values of one, we would see dramatic differences in the way they played and their win rates would generally decrease. We knew they were at a good point, but were sure they weren‚Äôt optimal, especially considering different characters played differently and using the same weights made them all play more or less the same. Using a genetic algorithm would use machine learning to tune each weight, giving us the most ideal numbers to defeat the default bots, and hopefully our opponents in the tournament. An ambitious goal was to create different genes for each character, thereby giving them each their own unique play style, but we knew that without more time or more computing power, we would have to make do with the hand-tuned weights we had.

A week before the competition, we strayed away from adding major features, only including small changes that our data decisively proved would increase the win rate. By the end, with the State Machine we were able to achieve a consistent win rate above 98% against the Valve bots. Ready for the competition, Maurice messaged us, informing us that once again the competition had been extended for another month.

Genetic Algorithm

With the month long extension to the tournament, we began to discuss how we could create a genetic algorithm. In the end, we decided to use Go once again because our data gathering programs had already been written in it, therefore making it easier to tie the programs together.

Genetic algorithm flowchart, from arwn

In order to get the genetic algorithm to work, we needed to run multiple iterations of our bot. From those iterations, we would grab the top five heroes genes and ‚Äúbreed‚Äù them by shuffling, averaging, and splicing them together. The next generation would be made up of slightly modified versions (using a 10% mutation probability to choose which genes to change, and 10% mutation rate to change each gene by the respective amount) which we would then gather data on, repeating the process until the beginning of the competition. Our plan was to replace the current hand-tuned genes with our new machine learned ones.

Our first step was making sure we could run the genetic algorithm using Go and Docker, and modify the Lua script at the same time. Each bot‚Äôs gene was a Lua file containing the values we wanted to mutate using the genetic algorithm. We used Go to read in the gene file, mutated the values, and output the new gene using a gene template. The new genes were then used for the subsequent iterations.

Having successfully created a way to read and write to our new gene files, instead of making one generic genetic algorithm as we had originally planned, we created genes for each hero we were using. In order to make it work, each file had to include the name of the hero we were writing to. Unfortunately we could only train five heroes at a time, so we opted to train our starting lineup and use our hand tuned genes for the rest of the heroes we had implemented.

Finishing the genetic algorithm ended up taking longer than planned. We hoped to have it running and training within a week, but needed a few more days to iron out bugs. We had each made separate parts of the genetic algorithm, and piecing each together took some time.

Finally, the genetic algorithm worked, but as we began to run the first generations, we ran into multiple issues. At this point, we had continued to have some issues with our Docker containers not running games, but had chosen to ignore it for the time being because while it had been slower in collecting data, it wasn‚Äôt a significant time difference. If one computer malfunctioned and dropped off the network the server would hang, waiting for data to come in from the downed machine. When we decided to use the genetic algorithm, we needed it to run non-stop and continue working through each generation. If a worker failed to respond, the server could never move onto the next generation because it was waiting for the remaining games to come in. It made little sense for us to monitor the computers in shifts all day, so we added in a way of timing out if we did not get a response from the container after a period of time.

In the end, after about four days of starting and stopping the genetic algorithm, we finally had it working. While running the genetic algorithm and confirming that it worked, we decided to change our team lineup in favor of one we thought could raise our win rate. When we began running the genetic algorithm and set up the genes we wanted to manipulate, as a team, we went through them and adjusted them to numbers we believed made sense for the genetic algorithm to start on. At that time, we decided to manipulate approximately 25 components and conditions, the ‚Äúgenes,‚Äù from our Farm, Hunt, and Retreat weights. This change combined with a new hero selection we used for the opposing team dropped our win rate from 98% to 80%. While the genetic algorithm was slowly raising the win rate, we spoke as a team and decided that if we could boost it by switching or adding heroes early on, it could be worth testing. After the switch, the initial 80% rose closer to 90%.

While we observed the bot, we knew that time was beginning to run out and it wasn‚Äôt growing fast enough. Although it was a risky decision that could result in a potentially drastic decrease in win rate, we decided to adjust the rate of change from 10% mutation probability and 10% mutation rate to 15% and 25% respectively. We calculated that in the most ideal situation, in order to get rid of a gene that was not useful, it would take at least thirty generations, or at least one week. We wanted to reduce that number and figured if we doubled it, we would see a higher rate of change, for better or for worse. After days of observing the results, our risk paid off and the bot saw a faster and more consistent increase in win rate.

Fitness growth over time

Once we were sure of the outcome, we began to add in more genes to manipulate from other State Machine weights. Another problem we ran into throughout the project that we had been unable to solve was how to play early on in the game, and how to play near the end. In DotA, the play styles between the two are drastically different. The behaviors that are important early on are less important as the game goes on for longer, and vice versa. Our strategy up to this point had been to trade a slightly weaker start to the game for a more powerful finish. We had tried to tweak the weights multiple times, but even if they played better at the beginning, the manipulated weights would fail in the end dropping the overall win rate. Now that we had our working genetic algorithm, we added in various multipliers to health for it to adjust, but also decided to add in multipliers based on how powerful the hero is. Heroes go from level 1 to 25 and get stronger as they gain levels. By hand we were never able to manipulate the weights in an effective way that would allow for early and late game adjustments. With the genetic algorithm, we could now leave it up to the computer to decide when to play differently.

After one more hero change, we settled on our final roster and continued to let the genetic algorithm work. A few days before the beginning of the tournament, we saw that the bot had finally reached a 99% win rate for one generation, but this dropped the next generation to 96%. While our rapid manipulation of genes had created a powerful bot, once it got closer and closer to it‚Äôs theoretical peak, the 25% mutation rate would change to much at once and dropped the win rate. We decided that in order to preserve our win rate, we would need to slow down the mutation. The mutation probability was dropped to 7% and the mutation rate was dropped to 15%.

As we changed our genetic algorithm once again, we decided to take another risk. Up to this point we had been taking the top five genes from each hero as parents, breeding them and using the offspring for the next generation. While this had worked for us, classically it was not how we should have been using a genetic algorithm. In a genetic algorithm, all genes should have a chance of being picked, but we were actively selecting which to use. The importance of using the lower win rate bots is diversity. While in the generation it may not have performed as well, in a future generation its genes may be an important part towards increasing the win rate. In order to make sure those lower win rate genes had a chance at being selected, we mapped all of the genes, allowing a higher probability of being picked to the higher win rates, while still giving the lower win rates a chance, albeit smaller.

We also discussed a change in strategy. Taking the genes from each individual bot was necessary, but we considered the importance of taking all of the genes from one ‚Äúteam‚Äù of bots. A bot at first glance may have looked like it had less potential, but as part of a team it‚Äôs genes could have been an important key to victory. We thought about the benefits of switching from the individual bots to only breeding teams, but couldn‚Äôt justify losing out on more powerful heroes genes. As we came to the conclusion that we should continue using the same strategy of selecting the individual bots, a thought popped into our heads. What if we do both? Taking the individual genes was undeniably important, but by breeding bots from the same team with the stronger individual bots, we believed we could unlock the potential of both worlds.

Conclusion

The genetic algorithm seems to have improved the bot, although its play style is considerably different from the original non-genetic version. While we were much more aggressive earlier, we now play more conservatively and aim to win mostly by destroying structures and winning the occasional team fights. The older bot would group together more often as a team, forcing opponent bots to react and resulting in more fights. If we continued to work on the project, I believe the next step would be having the bot begin to fight itself and the older, non-genetic version of the bot.

Through this project, I‚Äôve been able to learn multiple programming languages, as well as familiarize myself with Docker, and the importance of documentation when working as a team. The reason I decided to work on this project was less an interest in DotA2, but more trying to understand machine learning. I‚Äôd heard the term multiple times, had read about it, but didn‚Äôt have a real understanding of what it entailed and how to actually program it. Participating on this project gave me a unique opportunity to work with machine learning, and has increased my understanding of programming as a whole.
This is a tutorial on building a Chrome Extension that leverages Serverless architecture. Specifically ‚Äî we will use Google Cloud Functions in the Back-End of our Chrome Extension to do some fancy Python magic for us.

The Extension we will build is the SummarLight Chrome Extension:

The SummarLight Extension takes the text of he current web page you are on (presumably a cool blog on medium such as mine) and highlights the most important parts of that page/article.

In order to do this, we will setup a UI (a button in this case) which will send the text on our current web page to our Back-End. The ‚ÄòBack-End‚Äô in this case will be a Google Cloud Function which will analyze that text and return its Extractive Summary (the most important sentences of that text).

Architecture

A Simple & Flexible Architecture

As we can see, the architecture is very straightforward and flexible. You can setup a simple UI like an App or, in this case, a Chrome Extension, and pass any complex work to your serverless functions. You can easily change your logic in the function and re-deploy it to try alternative methods. And finally, you can scale it for as many API calls as needed.

This is not an article on the benefits of serverless so I will not go into details about the advantages of using it over traditional servers. But usually, a serverless solution can be much cheaper and scalable (but not always‚Ä¶depends on your use case).

The Chrome Extension

Here is a good guide on the setup of a Chrome Extension:

And all the code for the SummarLight Extension can be found here:

The main.py file in the root directory is where we define our Google Cloud Function. The extension_bundle folder has all the files that go into creating the Chrome Extension.

Google Cloud Function

I chose Google instead of AWS Lamba because I had some free credits (thanks Google!) but you can easily do it with AWS as well. It was a huge plus for me that they had just released Google Cloud Functions for Python as I do most of my data crunching in that beautiful language.

You can learn more about deploying Google Cloud Functions here:

I highly recommend using the gcloud sdk and starting with there hello_world example. You can edit the function in the main.py file they provide for your needs. Here is my function:


Pretty straight forward. I receive some text via the read_article() function and then, using the awesome Gensim library, I return a Summary of that text. The Gensim Summary function works by ranking all the sentences in order of importance. In this case, I have chosen to return the top 30% of the most important sentences. This will highlight the top one third of the article/blog.

Alternative Approaches: I tried different methods for summarization including using Glove Word Embeddings but the results were not that much better compared to Gensim (especially considering the increased processing compute/time because of loading in those massive embeddings). There is still a lot of room for improvement here though. This is an active area of research and better text summarization approaches are being developed:

Once we are good with the function we can deploy it and it will be available at an HTTP endpoint which we can call from our App/Extension.

Extension Bundle

Now for the Front-End. To start, we need a popup.html file. This will deal with the UI part. It will create a menu with a button.


Please share your ideas for Extensions (or Apps) leveraging Google Cloud Functions in the comments. :)

Cheers.
Member-only story How to Teach Code

Learning to code is really hard and teaching code is even harder. There is no bootcamp in ‚ÄòHow to Teach Code‚Äô. Most teachers do not have any formal training in education and come from a software background.

I‚Äôve had the pleasure of working with several bootcamps in different ways (as a student, teacher or experience partner). I‚Äôve also:

Ran a code Meetup club teaching code

Delivered a Hackathon series

Helped out with various learning code events.

The Problems

It‚Äôs amazing how little regulation there is in teaching code. The result is students can receive wildly different experiences. I‚Äôve seen some of the best education on the planet being given away for free. Yet I‚Äôve also seen other bootcamps delivering their students 3 months of confusion and charged ¬£9,000 for the pleasure.

I‚Äôve seen bootcamps struggle along until they inevitably fail whilst others just continue to grow and grow. Taking students money to deliver them a poor education is wrong but I also don‚Äôt think it was anyone‚Äôs intention to just steal from people.

With potential huge swathes of current jobs being taken over by software, there are more students than ever looking to transfer into a technical role.

Stories like the Mined Minds fiasco are just the surface of a very big problem. Rather than just talking about how big the problem is it makes sense to do something to fix it.

Solutions

Below is what I‚Äôve learnt myself about the difficulties in teaching code. How to teach code it in a way that gives the best experience for students. And how to grow a sustainable business by delivering a great experience.

(Note ‚Äî as a student on a bootcamp it is worth reading this. If your course if making some of these mistakes you might be able to help save your education by encouraging some better practices.)

Part 1 ‚Äî Why Learning‚Ä¶
Reinventing Personalization For Customer Experience

‚ÄúRemember that a person‚Äôs name is, to that person, the sweetest and most important sound in any language.‚Äù ‚Äî Dale Carnegie, How to Win Friends and Influence People

When it comes to building good relationships with customers, learning their names is an essential step for businesses at any level. Consumers expect to be treated as individuals when it comes to the brands they do business with. Remembering a person‚Äôs name and using it whenever appropriate is key to winning that person over to your way of thinking. This fact is backed by science which says that hearing one‚Äôs own name has a powerful impact on the listener‚Äôs brain. Hence, it is only logical to remember and use not only the customers‚Äô names, but also their likes and dislikes to make them feel valued ‚Äî in other words, providing consumers with a personalized customer experience can change them into brand loyalists in no time.

Personalizing the customer experience across all touch points

The age of the customer marked the end of the one-size-fits-all messaging era. Today, a single message can‚Äôt get the job done unless it‚Äôs perfectly tailored to relate with every customer. This is the age where consumers are more empowered than ever before and in control of their relationship with a brand. These consumers continually demand personalization throughout the buying journey.

Living in a noisy world of instant gratification, how can a business make an impression in an already overcrowded field? It‚Äôs simple actually ‚Äî by making use of something known as the ‚Äúcocktail party‚Äù effect. Here‚Äôs how cocktail party effect works: when you‚Äôre at a cocktail party with dozens of people chattering around you, you‚Äôll find that you can easily blur out those conversations. To you, they‚Äôre just background noise. But, as soon as someone says your name or something that is of particular interest to you, your ears perk up and tune into that specific conversation. This information will rise above the noise because it is important to you.

Similarly, adding a personal touch to the customer experience, for instance, using dynamic recipient name tags in emails can enable businesses to get their voice heard, allowing them to stand out in an overcrowded market.

Personalization ‚Äî A winning strategy
In the machine learning and deep learning paradigm, model ‚Äúparameters‚Äù and ‚Äúhyperparameters‚Äù are two frequently used terms where ‚Äúparameters‚Äù define configuration variables that are internal to the model and whose values can be estimated from the training data and ‚Äúhyperparameters‚Äù define configuration variables that are external to the model and whose values cannot be estimated from the training data ( What is the Difference Between a Parameter and a Hyperparameter? ). Thus, the hyperparameter values need to be manually assigned by the practitioner.

Every machine learning and deep learning model that we make has a different set of hyperparameter values that need to be fine-tuned to be able to obtain a satisfactory result. Compared to machine learning models, deep learning models tend to have a larger number of hyperparameters that need optimizing in order to get the desired predictions due to its architectural complexity over typical machine learning models.

Repeatedly experimenting with different value combinations manually to derive the optimal hyperparameter values for each of these hyperparameters can be a very time consuming and tedious task that requires good intuition, a lot of experience, and a deep understanding of the model. Moreover, some hyperparameter values may require continuous values, which will have an undefined number of possibilities, and even if the hyperparameters require a discrete value, the number of possibilities is enormous, thus manually performing this task is rather difficult. Having said all that, hyperparameter optimization might seem like a daunting task but thanks to several libraries that are readily available in the cyberspace, this task has become more straightforward. These libraries aid in implementing different hyperparameter optimization algorithms with less effort. A few such libraries are Scikit-Optimize, Scikit-Learn, and Hyperopt.

There are several hyperparameter optimization algorithms that have been employed frequently throughout the years, they are Grid Search, Random Search, and automated hyperparameter optimization methods. Grid Search and Random Search both set up a grid of hyperparameters but in Grid Search every single value combination will be exhaustively explored to find the hyperparameter value combination that gives the best accuracy values making this method very inefficient. On the other hand, Random Search will repeatedly select random combinations from the grid until the specified number of iterations is met and is proven to yield better results compared to the Grid Search. However, even though it manages to give a good hyperparameter combination we cannot be certain that it is, in fact, the best combination. Automated hyperparameter optimization uses different techniques like Bayesian Optimization that carries out a guided search for the best hyperparameters ( Hyperparameter Tuning using Grid and Random Search). Research has shown that Bayesian optimization can yield better hyperparameter combinations than Random Search ( Bayesian Optimization for Hyperparameter Tuning).

In this article, we will be providing a step-by-step guide into performing a hyperparameter optimization task on a deep learning model by employing Bayesian Optimization that uses the Gaussian Process. We used the gp_minimize package provided by the Scikit-Optimize (skopt) library to perform this task. We will be performing the hyperparameter optimization on a simple stock closing price forecasting model developed using TensorFlow.

Scikit-Optimize (skopt)

Scikit-Optimize is a library that is relatively easy to use than other hyperparameter optimization libraries and also has better community support and documentation. This library implements several methods for sequential model-based optimization by reducing expensive and noisy black-box functions. For more information you can refer neptune.ai‚Äôs article where they have done a comprehensive analysis on the capabilities and usage of skopt.

Bayesian Optimization using the Gaussian Process

Bayesian optimization is one of the many functions that skopt offers. Bayesian optimization finds a posterior distribution as the function to be optimized during the parameter optimization, then uses an acquisition function (eg. Expected Improvement-EI, another function etc) to sample from that posterior to find the next set of parameters to be explored. Since Bayesian optimization decides the next point based on more systematic approach considering the available data it is expected to yield achieve better configurations faster compared to the exhaustive parameter optimization techniques such as Grid Search and Random Search. You can read more about the Bayesian Optimizer in skopt from here.

Code Alert!

So, enough with the theory, let‚Äôs get down to business!

This example code is done using python and TensorFlow. Furthermore, the goal of this hyperparameter optimization task is to obtain the set of hyperparameter values that can give the lowest possible Root Mean Square Error (RMSE) for our deep learning model. We hope this will be very straight forward for any first-timer.

First, let us install Scikit-Optimize. You can install it using pip by executing this command.

pip install scikit-optimize

Please note that you will have to make some adjustments to your existing deep learning model code in order to make it work with the optimization.

First, let‚Äôs do some necessary imports.

import skopt

from skopt import gp_minimize

from skopt.space import Real, Integer

from skopt.utils import use_named_args

import tensorflow as tf

import numpy as np

import pandas as pd

from math import sqrt

import atexit

from time import time, strftime, localtime

from datetime import timedelta

from sklearn.metrics import mean_squared_error

from skopt.plots import plot_convergence

We will now set the TensorFlow and Numpy seed as we want to get reproducible results.

randomState = 46

np.random.seed(randomState)

tf.set_random_seed(randomState)

Shown below are some essential python global variables that we have declared. Among the variables, we have also declared the hyperparameters that we are hoping to optimize (the 2nd set of variables).

input_size=1

features = 2

column_min_max = [[0, 2000],[0,500000000]]

columns = ['Close', 'Volume']



num_steps = None

lstm_size = None

batch_size = None

init_learning_rate = None

learning_rate_decay = None

init_epoch = None

max_epoch = None

dropout_rate = None

The ‚Äúinput_size‚Äù depicts a part of the shape of the prediction. The ‚Äúfeatures‚Äù depict the number of features in the data set and the ‚Äúcolumns‚Äù list has the header names of the two features. The ‚Äúcolumn_min_max‚Äù variable contains the upper and lower scaling bounds of both the features (this was taken by examining validation and training splits).

After declaring all these variables it‚Äôs finally time to declare the search space for each of the hyperparameters we are hoping to optimize.

lstm_num_steps = Integer(low=2, high=14, name='lstm_num_steps')

size = Integer(low=8, high=200, name='size')

lstm_learning_rate_decay = Real(low=0.7, high=0.99, prior='uniform', name='lstm_learning_rate_decay')

lstm_max_epoch = Integer(low=20, high=200, name='lstm_max_epoch')

lstm_init_epoch = Integer(low=2, high=50, name='lstm_init_epoch')

lstm_batch_size = Integer(low=5, high=100, name='lstm_batch_size')

lstm_dropout_rate = Real(low=0.1, high=0.9, prior='uniform', name='lstm_dropout_rate')

lstm_init_learning_rate = Real(low=1e-4, high=1e-1, prior='log-uniform', name='lstm_init_learning_rate')

If you look closely you will be able to see that we have declared the ‚Äòlstm_init_learning_rate‚Äô prior to log-uniform without just putting uniform. What this does is that, if you had put prior as uniform, the optimizer will have to search from 1e-4 (0.0001 ) to 1e-1 (0.1) in a uniform distribution. But when declared as log-uniform, the optimizer will search between -4 and -1, thus making the process much more efficient. This has been advised when assigning the search space for learning rate by the skopt library.

There are several data types using which you can define the search space. Those are Categorical, Real and Integer. When defining a search space that involves floating point values you should go for ‚ÄúReal‚Äù and if it involves integers, go for ‚ÄúInteger‚Äù. If your search space involves categorical values like different activation functions, then you should go for the ‚ÄúCategorical‚Äù type.

We are now going to put down the parameters that we are going to optimize in the ‚Äòdimensions‚Äô list. This list will be passed to the ‚Äògp_minimize‚Äô function later on. You can see that we have also declared the ‚Äòdefault_parameters‚Äô. These are the default parameter values we have given to each hyperparameter. Remember to type in the default values in the same order as you listed the hyperparameters in the ‚Äòdimensions‚Äô list.

dimensions = [lstm_num_steps, size, lstm_init_epoch, lstm_max_epoch,

lstm_learning_rate_decay, lstm_batch_size, lstm_dropout_rate, lstm_init_learning_rate]



default_parameters = [2,128,3,30,0.99,64,0.2,0.001]

The most important thing to remember is that the hyperparameters in the ‚Äúdefault_parameters‚Äù list will be the starting point of your optimization task. The Bayesian Optimizer will use the default parameters that you have declared in the first iteration and depending on the result, the acquisition function will determine which point it wants to explore next.

It can be said that if you have run the model several times previously and found a decent set of hyperparameter values, you can put them as the default hyperparameter values and start your exploration from there. What this may do is that it will help the algorithm find the lowest RMSE value faster (fewer iterations). However, do keep in mind that this might not always be true. Also, remember to assign a value that is within the search space that you have defined when assigning the default values.

What we have done up to now is setting up all the initial work for the hyperparameter optimization task. We will now focus on the implementation of our deep learning model. We will not be discussing the data pre-processing of the model development process as this article only focuses on the hyperparameter optimization task. We will include the GitHub link of the complete implementation at the end of this article.

However, to give you a little bit more context, we divided our data set into three splits for training, validation, and testing. The training set was used to train the model and the validation set was used to do the hyperparameter optimization task. As mentioned before, we are using the Root Mean Square Error (RMSE) to evaluate the model and perform the optimization (minimize RMSE).

The accuracy assessed using the validation split cannot be used to evaluate the model since the selected hyperparameters minimizing the RMSE with validation split can be overfitted to the validation set during the hyperparameter optimization process. Therefore, it is standard procedure to use a test split that has not used at any point in the pipeline to measure the accuracy of the final model.

Shown below is the implementation of our deep learning model:

def setupRNN(inputs, model_dropout_rate):



cell = tf.contrib.rnn.LSTMCell(lstm_size, state_is_tuple=True, activation=tf.nn.tanh,use_peepholes=True)



val1, _ = tf.nn.dynamic_rnn(cell, inputs, dtype=tf.float32)



val = tf.transpose(val1, [1, 0, 2])



last = tf.gather(val, int(val.get_shape()[0]) -1, name="last_lstm_output")



dropout = tf.layers.dropout(last, rate=model_dropout_rate, training=True,seed=46)



weight = tf.Variable(tf.truncated_normal([lstm_size, input_size]))

bias = tf.Variable(tf.constant(0.1, shape=[input_size]))



prediction = tf.matmul(dropout, weight) +bias



return prediction

The ‚ÄúsetupRNN‚Äù function contains our deep learning model. Still, you may not want to understand those details, as Bayesian optimization considers that function as a black-box that takes certain hyperparameters as the inputs and then outputs the prediction. So if you are not interested in understanding what we have inside that function, you may skip the next paragraph.

Our deep learning model contains an LSTM layer, a dropout layer and an output layer. The necessary information required for the model to work needs to be sent to this function (in our case, it was the input and the dropout rate). You can then proceed with implementing your deep learning model inside this function. In our case, we used an LSTM layer to identify the temporal dependencies of our stock data-set.

We then fed the last output of the LSTM to the dropout layer for regularization purposes and obtained the prediction through the output layer. Finally, remember to return this prediction (in a classification task this can be your logit) to the function that will be passed to the Bayesian Optimization ( ‚ÄúsetupRNN‚Äù will be called by this function).

If you are performing a hyperparameter optimization for a machine learning algorithm (using a library like Scikit-Learn) you will not need a separate function to implement your model as the model itself is already given by the library and you will only be writing code to train and obtain predictions. Therefore, this code can go inside the function that will be returned to the Bayesian Optimization.

We have now come to the most important section of the hyperparameter optimization task, the ‚Äòfitness‚Äô function.

@use_named_args(dimensions=dimensions)

def fitness(lstm_num_steps, size, lstm_init_epoch, lstm_max_epoch,

lstm_learning_rate_decay, lstm_batch_size, lstm_dropout_rate, lstm_init_learning_rate):



global iteration, num_steps, lstm_size, init_epoch, max_epoch, learning_rate_decay, dropout_rate, init_learning_rate, batch_size



num_steps = np.int32(lstm_num_steps)

lstm_size = np.int32(size)

batch_size = np.int32(lstm_batch_size)

learning_rate_decay = np.float32(lstm_learning_rate_decay)

init_epoch = np.int32(lstm_init_epoch)

max_epoch = np.int32(lstm_max_epoch)

dropout_rate = np.float32(lstm_dropout_rate)

init_learning_rate = np.float32(lstm_init_learning_rate)



tf.reset_default_graph()

tf.set_random_seed(randomState)

sess = tf.Session()



train_X, train_y, val_X, val_y, nonescaled_val_y = pre_process()



inputs = tf.placeholder(tf.float32, [None, num_steps, features], name="inputs")

targets = tf.placeholder(tf.float32, [None, input_size], name="targets")

model_learning_rate = tf.placeholder(tf.float32, None, name="learning_rate")

model_dropout_rate = tf.placeholder_with_default(0.0, shape=())

global_step = tf.Variable(0, trainable=False)



prediction = setupRNN(inputs,model_dropout_rate)



model_learning_rate = tf.train.exponential_decay(learning_rate=model_learning_rate, global_step=global_step, decay_rate=learning_rate_decay,

decay_steps=init_epoch, staircase=False)



with tf.name_scope('loss'):

model_loss = tf.losses.mean_squared_error(targets, prediction)



with tf.name_scope('adam_optimizer'):

train_step = tf.train.AdamOptimizer(model_learning_rate).minimize(model_loss,global_step=global_step)



sess.run(tf.global_variables_initializer())



for epoch_step in range(max_epoch):



for batch_X, batch_y in generate_batches(train_X, train_y, batch_size):

train_data_feed = {

inputs: batch_X,

targets: batch_y,

model_learning_rate: init_learning_rate,

model_dropout_rate: dropout_rate

}

sess.run(train_step, train_data_feed)



val_data_feed = {

inputs: val_X,

}

vali_pred = sess.run(prediction, val_data_feed)



vali_pred_vals = rescle(vali_pred)



vali_pred_vals = np.array(vali_pred_vals)



vali_pred_vals = vali_pred_vals.flatten()



vali_pred_vals = vali_pred_vals.tolist()



vali_nonescaled_y = nonescaled_val_y.flatten()



vali_nonescaled_y = vali_nonescaled_y.tolist()



val_error = sqrt(mean_squared_error(vali_nonescaled_y, vali_pred_vals))



return val_error

As shown above, we are passing the hyperparameter values to a function named ‚Äúfitness.‚Äù The ‚Äúfitness‚Äù function will be passed to the Bayesian hyperparameter optimization process ( gp_minimize). Note that in the first iteration, the values passed to this function will be the default values that you defined and from there onward Bayesian Optimization will choose the hyperparameter values on its own. We then assign the chosen values to the python global variables we declared at the beginning so that we will be able to use the latest chosen hyperparameter values outside the fitness function.

We then come to a rather critical point in our optimization task. If you have used TensorFlow prior to this article, you would know that TensorFlow operates by creating a computational graph for any kind of deep learning model that you make.

During the hyperparameter optimization process, in each iteration, we will be resetting the existing graph and constructing a new one. This process is done to minimize the memory taken for the graph and prevent the graphs from stacking on top of each other. Immediately after resetting the graph you will have to set the TensorFlow random seed in order to obtain reproducible results. After the above process, we can finally declare the TensorFlow session.

After this point, you can start adding code responsible for training and validating your deep learning model as you normally would. This section is not really related to the optimization process but the code after this point will start utilizing the hyperparameter values chosen by the Bayesian Optimization.

The main point to remember here is to return the final metric value (in this case the RMSE value) obtained for the validation split. This value will be returned to the Bayesian Optimization process and will be used when deciding the next set of hyperparameters that it wants to explore.

Note: if you are dealing with a classification problem you would want to put your accuracy as a negative value (eg. -96) because, even though the higher the accuracy the better the model, the Bayesian function will keep trying to reduce the value as it is designed to find the hyperparameter values for the lowest value that is returned to it.

Let us now put down the execution point for this whole process, the ‚Äúmain‚Äù function. Inside the main function, we have declared the ‚Äúgp_minimize‚Äù function. We are then passing several essential parameters to this function.

if __name__ == '__main__':



start = time()



search_result = gp_minimize(func=fitness,

dimensions=dimensions,

acq_func='EI', # Expected Improvement.

n_calls=11,

x0=default_parameters,

random_state=46)



print(search_result.x)

print(search_result.fun)

plot = plot_convergence(search_result,yscale="log")



atexit.register(endlog)

logger("Start Program")

The ‚Äúfunc‚Äù parameter is the function you would want to finally model using the Bayesian Optimizer. The ‚Äúdimensions‚Äù parameter is the set of hyperparameters that you are hoping to optimize and the ‚Äúacq_func‚Äù stands for the acquisition function and is the function that helps to decide the next set of hyperparameter values that should be used. There are 4 types of acquisition functions supported by gp_minimize. They are:

LCB: lower confidence bound

EI: expected improvement

PI: probability of improvement

gp_hedge: probabilistically choose one of the above three acquisition functions at every iteration

The above information was extracted from the documentation. Each of these has its own advantages but if you are a beginner to Bayesian Optimization, try using ‚ÄúEI‚Äù or ‚Äúgp_hedge‚Äù as ‚ÄúEI‚Äù is the most widely used acquisition function and ‚Äúgp_hedge‚Äù will choose one of the above-stated acquisition functions probabilistically thus, you wouldn‚Äôt have to worry too much about that.

Keep in mind that when using different acquisition functions there might be other parameters that you might want to change that affects your chosen acquisition function. Please refer the parameter list in the documentation for this.

Back to explaining the rest of the parameters, the ‚Äún_calls‚Äù parameter is the number of times you would want to run the fitness function. The optimization task will start by using the hyperparameter values defined by ‚Äúx0‚Äù, the default hyperparameter values. Finally, we are setting the random state of the hyperparameter optimizer as we need reproducible results.

Now when you run the gp_optimize function the flow of events will be:

The fitness function will use with the parameters passed to x0. The LSTM will be trained with the specified epochs and the validation input will be run to get the RMSE value for its predictions. Then depending on that value, the Bayesian optimizer will decide what the next set of hyperparameter values it wants to explore with the help of the acquisition function.

In the 2nd iteration, the fitness function will run with the hyperparameter values that the Bayesian optimization has derived and the same process will repeat until it has iterated ‚Äún_call‚Äù times. When the complete process comes to an end, the Scikit-Optimize object will get assigned to the ‚Äúsearch _result‚Äù variable.

We can use this object to retrieve useful information as stated in the documentation.

x [list]: location of the minimum.

fun [float]: function value at the minimum.

models: surrogate models used for each iteration.

x_iters [list of lists]: location of function evaluation for each iteration.

func_vals [array]: function value for each iteration.

space [Space]: the optimization space.

specs [dict]`: the call specifications.

rng [RandomState instance]: State of the random state at the end of minimization.

The ‚Äúsearch_result.x‚Äù gives us optimal hyperparameter values and using ‚Äúsearch_result.fun‚Äù we can obtain the RMSE value of the validation set corresponding to the obtained hyperparameter values (The lowest RMSE value obtained for the validation set).

Shown below are the optimal hyperparameter values that we obtained for our model and the lowest RMSE value of the validation set. If you are finding it hard to figure out the order in which the hyperparameter values are being listed when using ‚Äúsearch_result.x‚Äù, it is in the same order as you specified your hyperparameters in the ‚Äúdimensions‚Äù list.

Hyperparameter Values:

lstm_num_steps: 6

lstm_size: 171

lstm_init_epoch: 3

lstm_max_epoch: 58

lstm_learning_rate_decay: 0.7518394019565194

lstm_batch_size: 24

lstm_dropout_rate: 0.21830825193089087

lstm_init_learning_rate: 0.0006401363567813549

Lowest RMSE: 2.73755355221523

Convergence Graph

The hyperparameters that produced the lowest point of the Bayesian Optimization in this graph is what we get as the optimal set of hyperparameter values.

The graph shows a comparison of the lowest RMSE values recorded for each iteration (50 iterations) in Bayesian Optimization and Random Search. We can see that the Bayesian Optimization has been able to converge rather better than the Random Search. However, in the beginning, we can see that Random search has found a better minimum faster than the Bayesian Optimizer. This can be due to random sampling being the nature of Random Search.

We have finally come to the end of this article, so to conclude, we hope this article made your deep learning model building task easier by showing you a better way of finding the optimal set of hyperparameters. Here‚Äôs to no more stressing over hyperparameter optimization. Happy coding, fellow geeks!

Useful Materials:
Photo by William Iven on Unsplash

I recently spent four days at a research lab with a group of data scientists and a few behavior scientists diving into a large, messy data set to see if we might find insights related to group dynamics. It was framed as an initial dive to look around, get an initial assessment of potential for useful research insights, and then build a proposal for two to three months of work based on what was found that week.

I attended theoretically in the role of a behavioral scientist, but since I‚Äôm also obsessed with creative problem solving processes, or design thinking (and I‚Äôm opinionated as hell), the following are some reflections on the process we used and ideas for what‚Äôs next.

A lot of process methodologies occupy the space in the overlap between software development, data analytics, and creative problem solving disciplines: UX, design thinking, Think Wrong, Google Design Sprint, Agile, Kanban, emerging data analysis methods for group sprints, etc. It‚Äôs great, because if you‚Äôre curious you can acquire a pretty big toolset to bring together groups of people to crash and get creative on all different kinds of problems for widely varying lengths in time ‚Äî a two hour meeting to an 18 month project. The challenge, or mastery of practice as a facilitator, is to learn not just the buzz words of the methodologies or the exercises themselves, but the underlying science behind group dynamics, creativity, psychology, and neuroscience.

During this workshop, we had a two distinct types of professionals in the room ‚Äî social science and data science, each with complementary skillsets within those groupings. They were all brand new to the data set, and it was pretty messy when we first looked at it. The direction from the sprint‚Äôs sponsor was extremely broad ‚Äî what general insights about the group dynamics of these co-workers can be identified in a three day sprint.

Unfortunately, we didn‚Äôt clearly identify the direction for what kind of insights would be most useful until the afternoon of the second day. At that point, the social science group separated to brainstorm a range of different questions that could be asked about group dynamics, and which theories and research questions were emerging as especially intriguing for the field. Besides ensuring the whole team had a clear idea of the direction for the workshop and what question themes regarding group dynamics would be most useful (ie. performance of small work teams, changes in group composition over time, looking for patterns in the kinds of people who grouped together, etc) we also needed design constraints, but didn‚Äôt identify them. Intro design thinking uses a generic Venn diagram to illustrate the criteria a design must successfully meet in order to be green lit for funding/ effort to develop a high-fidelity solution. Companies develop their own specific criteria that usually fall somewhere into these buckets. Without identifying our own, the team relied on the expertise and gut instinct of the people gathered in the room. Not bad ‚Äî everyone was incredibly smart and knew their field well; but not great if you want to maximize resources toward understanding the most important/ impactful questions.

via Legacy Innova

Our facilitator took the route of using UX and design thinking exercises ‚Äî personas and needs statements ‚Äî over the first two days. They were meant to get the group to identify specific questions that could then be voted on by the whole team. On the third day, small groups broke off, each taking one or two questions that might be asked of the data. Normally personas and needs statements are an incredibly reliable method; they work in all kinds of situations. But during this sprint many people found them to be frustrating when they had been told to look for insights about group dynamics, not individual personas.

The first challenge with persona methods is that they are designed to be used with deliberative modes of thinking, or System 2 (from Thinking Fast and Slow) that take a diverse, seemingly disconnected array of information ‚Äî most often qualitative ethnography ‚Äî and synthesize it to come to some detailed statement of need. With no prior set of information about the people in the data set, we were left looking at spreadsheets of raw data and making up cartoon characters as personas. The whole team went along gamely and tried to make use of it, but the time could have been better spent.

The second challenge regarding the selection of persona methods is that they are meant to be used during design processes where the outcome is some product or service serving archetypal individuals. There is a relationship between problem for the persona, and solution ‚Äî solving a need of the persona. For this workshop, our team was not meant to produce a product or service, the insights were the deliverables. Rather than personas, we should have been exploring and mapping the space of the complex concept of group dynamics.

The somewhat awkward use of some design thinking/ UX methods and rejection of others for this workshop was totally understandable. As design sprints have become a popular, justifiably so, tool for businesses to bring multi-disciplinary teams together and develop novel concepts and products, the broad space of design and software development methodologies have edged into unfamiliar territory ‚Äî data analytics. While I feel experienced to comment on design methodologies, data science is new territory for me. From the literature review I‚Äôve done, and the limited commercial experience with analytics teams, it seems that they are themselves wrestling with what methodologies serve group data sprints best. Analysis has traditionally been done solo, or sitting side by side, not requiring a more formal group process. Three forces are pushing data scientists to develop group analytics methodologies: the data science field has bifurcated with an ever increasing number of specializations and tools, the sheer amount of data has exponentially increased, and data science is spreading to disciplines that had previously done their analysis with almost ubiquitous qualitative methods and tools ‚Äî so multi-disciplinary teams were becoming the norm. In the last five to seven years, there have been several academic articles and blogs describing group data analysis processes.

My PhD research involves developing a co-design process to analyze a complex concept ‚Äî wellbeing, using 3D data visualization software. I‚Äôve been thinking about how we bring groups together to investigate and communicate about complex concepts, the very early work of group and individual need finding in order to eventually do policy, service, and product design. Trying to grasp complex systems and complex concepts is a totally different beast than even synthesizing nuanced psychological and social needs from personas. Oral cultures have very different approaches to thinking about and discussing complexity, and I believe our western dominated, literate, tech cultures have a lot to learn from them about how to analyze complexity.

I would like to offer these ideas for multi-disciplinary groups working to identify insights in large, messy data sets. I‚Äôm curious what others are doing, and would love some feedback.

First, here is the framework I use to organize design sprints organized around addressing particular needs of particular archetypal personas. Four quadrants correspond to the four phases usually associated with design thinking: interviewing and observing users, synthesis of needs, ideation, and prototyping and testing. I find that it helps me as a facilitator to understand the modes of thinking I‚Äôm trying to induce my participants to exercise for different methods. It‚Äôs also helpful to pair the mode of thinking with a clear delineation between problem and solution. Trying to brainstorm about both the problem and solution at the same time is a recipe for a chaotic descent of the group way, way off track. This 2x2 framework is most useful when designing exercises for workshops that revolve around understanding a user‚Äôs need and developing some thing to solve it ‚Äî a software product interface, a household object, a social service.

Complexity, ie group dynamics, requires a different framework for thinking about which methods to select and which modes of thinking to use during a sprint. In our sprint, the work revolved around getting to an insight, refining a raw data set and teasing apart a complex topic. Using a 2x2 matrix again, complex concept and raw data set replace problem and solution on one of the axes. For the second axis, we might use different flare and focus modes than ones that serve thinking about defined needs of individual archetypes, two examples would be inductive/ deductive and declarative/ modal. Dr. Mihnea Moldoveanu of Rottman Business School, led a decade long research effort to understand the modes of thinking, or adaptive intelligence, that transcends discipline boundaries. His admonishment is essentially to consciously select the mode of thinking, or pattern of thinking modes, best fit for the problem and group of people at hand.

A hypothetical set of methods for another data sprint might go: 1) map the scales, layers, and theories related to the complex concept in question, 2) down select to the most important set of themes or meta-questions to ask of the data set, 3) exploratory analysis of the raw data set, 4) develop a schema and begin cleaning the data and organizing it into an easy to access data frame, 5) break up into small groups to each tackle a theme or meta-question and use alternating flare and focus modes of thinking like inductive/ deductive and declarative/ modal to iteratively analyze the data, develop hypothetical insights, and refine them with more data analysis.

This space of methodologies ‚Äî especially sprint methodologies ‚Äî to ask complex questions of big data sets using multidisciplinary teams is really exciting. So many of the most interesting questions facing our policy-makers, scientists, and researchers sit on huge piles of data at the intersection of fields that traditionally haven‚Äôt had to find a common language to work together. Facilitators, data scientists, and design practitioners might develop new theories on how best best to organize teams to more quickly and efficiently divine insights from large, chaotic data sets. Inevitably, someone will try to come along and commoditize it with pretty graphic icons and simplified descriptions. But right now it‚Äôs a fun, collaborative space of community exploration as we figure out how to bring more disciplines and professions together with data scientists and raw data.
RoboSomm

Wine Embeddings and a Wine Recommender

One of the cornerstones of previous chapters of the RoboSomm series has been to extract descriptors from professional wine reviews, and to convert these into quantitative features. In this article, we will explore a way of extracting features from wine reviews that combines the best of the existing RoboSomm series and academic literature on this topic. We will then use these features to produce a simple wine recommendation engine.

The Jupyter Notebook with all relevant code can be found in this Github repository. Our dataset consists of roughly 180,000 professional wine reviews, scraped from www.winemag.com. These reviews span roughly 20 years, dozens of countries and hundreds of grape varieties.

Wine Embeddings

In the following section, we walk through the five steps required to create our ‚Äòwine embeddings‚Äô: a 300-dimensional vector for each wine, summarizing its sensory profile. On the way, we will explain successful approaches others have taken in similar projects. Before we proceed, let‚Äôs pick a wine to join us on this journey:

Point & Line 2016 John Sebastiano Vineyard Reserve Pinot Noir

Review: Dried red flowers and sagebrush combine for an elegant aromatic entry to this bottling by two business partners who have worked in Santa Barbara‚Äôs restaurant scene for many years. Tarragon and intriguing peppercorn flavors decorate the tangy cranberry palate, which is lightly bodied but very well structured.

Excellent! Time to get stuck in.

Step 1: Normalize words in wine review (remove stopwords, punctuation, stemming)

The first step is to normalize our text. We want to remove stopwords and any punctuation from our raw text. In addition, we will use a stemmer (Snowball Stemmer in Sci-Kit Learn) to reduce inflected words to their stem. The Pinot review becomes the following:

dri red flower sagebrush combin eleg aromat entri bottl two bus partner work santa barbara restaur scene mani year tarragon intrigu peppercorn flavor decor tangi cranberri palat light_bodi veri well structur

Step 2: Enhance the set of normalized words with phrases (bi-grams and tri-grams)

Next, we want to account for the possibility that some of the terms we want to extract from the wine descriptions are actually combinations of words or phrases. Here, we can use the gensim package Phrases to produce a set of bi- and tri-grams for the full corpus. Running our normalized wine review through the phraser consolidates terms such as ‚Äòlight‚Äô and ‚Äòbodi‚Äô which are frequently found next to each other to ‚Äòlight_bodi‚Äô:

dri red flower sagebrush combin eleg aromat entri bottl two bus partner work santa_barbara restaur scene mani_year tarragon intrigu peppercorn flavor decor tangi cranberri palat light_bodi veri well structur

Step 3: Use the RoboSomm wine wheels to standardize the wine descriptors in each review

Wine reviewers are often creative in their use of language, and sometimes use different words to describe things that are seemingly the same. After all, are ‚Äòwet slate‚Äô, ‚Äòwet stone‚Äô and ‚Äòwet cement‚Äô aromas not really manifestations of the same sensory experience? In addition, wine tasting has specific jargon. Terms such as ‚Äòbaked‚Äô, ‚Äòhot‚Äô or ‚Äòpolished‚Äô have a specific meaning in the world of wine tasting.

To standardize wine jargon and creative descriptors, researchers such as Bernard Chen have developed the Computational Wine Wheel. The Computational Wine Wheel categorizes and maps various wine terms that appear in wine reviews to create a consolidated set of descriptors. This great work, together with the contributions of others (e.g. Wine Folly and UC Davis) has been used to generate the RoboSomm wine wheels. These wine wheels were created by looking at a list of the most frequently occurring descriptors in the corpus after going through steps 1 and 2 outlined above. This list was then reviewed manually, and mapped onto a set of standardized descriptors. In total, this resulted in a mapping for over 1,000 ‚Äòraw‚Äô descriptors.

The first of the RoboSomm wine wheels is an aroma wheel, that categorizes a variety of aromatic descriptors:

Wine Aroma Wheel

The second wine wheel is a non-aroma wheel, that accounts for other characteristics, such as body, sweetness and acid levels. These descriptors are not typically included in tasting wheels, but are prominent parts of a tasting experience:

Wine Non-Aroma Wheel

We can choose to standardize wine terms at any of the three levels of the wheel, or use the raw descriptor itself (no standardization). For now, we will map the descriptors to the outside layer of the wheel. For the Pinot Noir review we started processing, we obtain the following:

dry red flower sagebrush combin elegant aromat entri bottl two bus partner work santa_barbara restaur scene mani_year tarragon intrigu pepper flavor decor tangy cranberry palat light_bodied veri well structur

Note that all the descriptors that have been mapped are highlighted in bold. The other terms are either non-informative or ambiguous in the context of this analysis.

Step 4: Retrieve the Word2Vec word embedding for each mapped term in the review

Next, we need to consider how we will quantify our set of mapped descriptors. A common approach to doing this (and one that was used in previous chapters of the RoboSomm series!) is to represent the absence/presence of each descriptor in the corpus with a 0 or a 1. However, this approach does not take into account semantic (dis)similarities between terms. Tarragon, for instance, is more similar to sagebrush than it is to cranberry. To account for this, we can create word embeddings: vector representations of words and phrases. Researchers such as Els Lefever and her co-authors have taken a similar approach to quantifying wine reviews in their work.

For the purpose of this project, we will use a technique called Word2Vec to generate a 300-dimensional embedding for every mapped term. Since wine jargon is so specific, we have to train our Word2Vec model on a representative corpus. Fortunately, our set of 180,000 wine reviews is exactly that! Having previously mapped our descriptors using our wine wheels, we have already somewhat standardized the wine terms in our corpus. This was done to eliminate unnecessary semantic nuance (e.g. consolidate ‚Äòwet stone‚Äô, ‚Äòwet slate‚Äô and ‚Äòwet cement‚Äô to ‚Äòwet rock‚Äô), hopefully enhancing the quality of our Word2Vec model.

Our trained Word2Vec model consists of a 300-dimensional embedding for every term in our corpus. However, we can recall from the previous step in this analysis that we only really care about the terms that are relevant descriptors of a wine‚Äôs sensory experience.

For our Pinot Noir, these were:

dry, flower, sagebrush, elegant, tarragon, pepper, tangy, cranberry, light_bodied

In the adjacent image, we can see the word embedding for each of these mapped descriptors.

Step 5: Weight each word embedding in the wine review with a TF-IDF weighting, and sum the word embeddings together

Now that we have a word embedding for each mapped descriptor, we need to think about how we can combine these into a single vector. Looking at our Pinot Noir example, ‚Äòdry‚Äô is a fairly common descriptor across all wine reviews. We want to weight that less than a rarer, more distinctive descriptor such as ‚Äòsagebrush‚Äô. In addition, we want to take into consideration the total number of descriptors per review. If there are 20 descriptors in one review and five in another, each individual descriptor in the former review probably contributes less to the overall profile of the wine than in the latter. Term Frequency-Inverse Document Frequency (TF-IDF) takes both of these factors into consideration. TF-IDF looks at how many mapped descriptors are contained within a single review (TF), as well as at how often each mapped descriptor appears in the 180,000 wine reviews (IDF).

Multiplying each mapped descriptor vector by its TF-IDF weighting gives us our set of weighted mapped descriptor vectors. We can then sum these to obtain a single wine embedding for each wine review. For our Pinot Noir, this looks something like:
20 must-know Data Science Interview Questions

A non-exhaustive(duh) list of some of the good data science questions I have come across. I hope this list is of use to someone wanting to brush up some basic concepts. Kudos to the authors of all the amazing posts mentioned here.

Q. Define mean, mode, median. Explain these concepts to a layman. When is either preferred over the other. Give practical examples.
Learning rate scheduler

ESPNetv2 introduces a variant of the cosine learning rate, wherein the learning rate is decayed linearly until cycle length and then restarted. At each epoch t, the learning rate Œ∑‚Çú is computed as:

Figure 1: Cyclic learning rate policy with linear learning rate decay and warm restarts

F aster training

With the learning rate scheduler described above, we train ShuffleNetv2 for a total of 120 epochs with a batch size of 512 across four TitanX GPUs using SGD with momentum at two different FLOP settings: (1) 41 MFLOPs and (2) 146 MFLOPs. For the first 60 epochs, we set Œ∑‚Çò·µ¢‚Çô, Œ∑‚Çò‚Çê‚Çì, and T to 0.1, 0.5, and 5, respectively. For the remaining 60 epochs, we set Œ∑‚Çò‚Çê‚Çì=0.1, T=60, and Œ∑‚Çò·µ¢‚Çô=Œ∑‚Çò‚Çê‚Çì/T. The figure below visualizes the learning policy for 120 epochs.

Figure 2: Cyclic learning rate scheduler with warm restarts for faster training proposed in the ESPNetv2 paper.

Results on the ImageNet dataset

Results are given in Figure 3 and Table 1. We can clearly see that the learning rate scheduler (discussed above) enables faster training while delivering similar performance as the linear learning rate scheduler in ShuffleNet paper.
Buyers beware, fake product reviews are plaguing the internet. How Machine Learning can help to spot them.

Opinion spamming is a situation that is aggravating, for instance, CBS News reports that 52% of product reviews posted in Walmart.com are ‚Äúinauthentic or unreliable‚Äù, while at least 30% of reviews posted at Amazon are fake. The problem of identifying opinion spamming remains an open topic, despite the fact that several researchers have addressed it already.

What makes businesses to incur in product deceptive reviews? The main driver is getting ahead of the competition for positioning their product or service to influence the public and organizations to make a purchase, therefore increasing their sales. The fraud takes place by posting fake negative reviews and giving unjust ratings to products from the competition.

It is known that ‚ÄúAmazon Mechanical Turk‚Äù, an internet crowdsourcing marketplace that allows requesters (businesses or individuals) to coordinate human labor to carry out a task, was employed to crowdsource fake reviews for a hotel chain. Given that this problem has grown to alarming rates, Yelp.com, a business directory service that publishes crowd-sourced reviews about businesses, launched a sting operation in order to unmask those businesses who buy fake reviews.

I will discuss the method that Mukherjee et al. present in their paper for detecting spam in product reviews. They called their model ‚ÄúAuthor Spamicity Model‚Äù (ASM). It is based on unsupervised learning which models spamicity as latent, shortly meaning that the model variables are ‚Äúhidden‚Äù. It is also a Bayesian inference framework. The aim of the model is to cluster the categorization of this latent population distributions into spammers and not spammers.

Please note that when I refer to products, I will be including also services.

How can we identify that a review may be fake? In order to develop their model, the authors define nine variables as observed features, the first five they categorize them as author features that have values in the interval [0, 1] (denoting a Beta distribution), where a value close to 0 or 1 denotes non-spamming or spamming, respectively. In the other hand, variables 5 to 9, represent review features, and they assume a binary value, 0 for non-spamming, and 1 for spamming (denoting a Bernoulli distribution):

Content Similarity (CS). Spammers are prone to copy reviews for comparable products. Cosine similarity is used to capture content similarity in these reviews. Maximum Number of Reviews (MNR). The unusual behavior of posting several reviews in one day by the same author, can be a sign of spamming. Reviewing Burstiness (BST). Refers to the frequency (short) to which an author posts a review. This author is usually a new member of the site. Meeting this condition may signify a tendency to incur in deceptive reviews. Ratio of First Reviewers (RFR). This metric quantifies the fact that early reviews have an impact on sales of newly launched products. Duplicate/Near Duplicate Reviews (DUP). Identical or quasi identical reviews may indicate a spamming behavior. This feature is similar to CS, but in this case pertains to the review features. Extreme Rating (EXT). In order to deliver the most or the least benefit to a review, spammers usually mark the product with either one or five stars. Rating Deviation (DEV). Spammers will try to divert the average sentiment on reviews, by placing theirs. These types of reviews are identified when this quantified deviation exceeds a threshold. Early Time Frame (ETF). This feature captures how early the review was made. The rationale is, spammers are most likely to review earlier, close to the launch of the product to achieve the greatest impact. Rating Abuse (RA). Refers to the action of star-qualifying the same product multiple times.

How does ASM work? In order to illustrate the model, I have simplified its functioning in the following schema (See Figures 1-A and 1-B), for a mathematical representation please refer to the paper.

Figure 1-A

Figure 1-B

ASM commences by taking in all the reviews by all the authors, where these reviews are organized by the features that we have discussed. Each sphere represents an observable variable (i.e. feature). Once the features are collected (See Figure 1-A node A) they are processed by the model and learn the ‚Äúlatent behavior distributions for spam and not spam‚Äù (Murkherjee et al.). Therefore, ASM solves a clustering problem (K = 2).

The spamicity is modeled as latent as ASM functions in the Bayesian context. It is a generative process because it emits the nine features with their probability of spamming.

In order to perform an inference, the model uses ‚ÄúCollapsed Gibbs Sampling‚Äù (CGS) that represents a technique for approximating the posterior probability distribution. CGS belongs to the family of algorithms of Markov Chain Montecarlo.

Once the ranking functions have been inferred, they are processed using the Learning to Rank supervised technique, that basically takes the rankings obtained by ASM and generates a single aggregated ranking function (see Figure 1-A node C).

In my opinion, this paper presents a technique that can improve significantly the detection of opinion spammers in product reviews. It is innovative because presents an unsupervised method for detecting fake reviews. The authors claim having achieved a superior level of accuracy compared to strong competitors. I believe that opinion spamming will start to decrease as more businesses providing this type of information start to implement ML techniques like ASM, meanwhile consumers must be skeptical and get informed using sites that filter fake reviews.

A Mukherjee, A Kumar, B Liu, J Wang, M Hsu‚Ä¶ ‚Äî Proceedings of the 19th ‚Ä¶, 2013 ‚Äî dl.acm.org

P Resnik, E Hardisty ‚Äî 2010

GEP Box, GC Tiao ‚Äî 2011 ‚Äî books.google.com

by JA L√≥pez ‚Äî ‚Äé2017

‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî -

Dear reader, I am interested to know from you:

When you buy online, do you feel influenced to decide by the local review that is presented? Or in addition to, you look for one or more external reviews? What is the products review site that you trust more, if you use one? What do you think may be a solution to this problem that is growing at alarming rates?

Thanks for participating, you can leave a comment to respond.
Edgar Yau: It‚Äôs been about three, four years since I‚Äôve actually been a part of these communities. Now I‚Äôm more of an observer, but I‚Äôve stayed really up-to-date with the controversies and the outrage that goes on in them. In middle school I played video games and it‚Äôs the way I connected with my friends. It‚Äôs an interesting subculture for a boy to get all caught up in. The gamerspeak is very distinctive, games are very irreverent, they just kind of say whatever and if you get offended, you‚Äôre offended and everyone makes fun of you for it.

It‚Äôs not a first person shooter thing, it‚Äôs just a gamer thing. I wasn‚Äôt primarily playing first person shooters at that time, I was playing a lot of Minecraft with my friends and going on all these random servers. Even then it was like, who can build the biggest swastika? It was this whole idea of, I‚Äôm not getting offended, so if other people get offended they‚Äôre stupid, they‚Äôre vulnerable, they‚Äôre idiotic. What right do they have to control the way that I speak? The concept of freedom of speech is huge in these communities.

I started researching this because I was playing Call of Duty with my friend one day online, and within two minutes I heard a prepubescent voice say the n-word. I was catapulted into my adolescent years.

Andrew Thompson: Angela Nagle talked about this in Kill All Normies where she discusses the idea of extending cultural capital into subcultural capital. The distinction between groups in this case is based on one‚Äôs knowledge of these very esoteric references and memes and language, like gamerspeak, and I would say also on your ability to withstand insults. If you bristle, you‚Äôre sort of being filtered out of the subculture.

EY: The instant some feminist critique of games comes out, it feels like the enemy outside is coming into my space and telling me that I shouldn‚Äôt like something. There are only two options: you either double down on your opinions, or you kind of acknowledge that maybe they‚Äôre right. I don‚Äôt want to say that I was too young-I was around 16-but I was probably just too certain of what I thought and how I felt that I didn‚Äôt consider just for a second that those criticisms might not be totally baseless. The interesting thing is that I‚Äôm not from here, I grew up in Hong Kong and I only came to the States for college, but I was still extremely involved in this stuff.

AT: I really think of people in these subcultures expressing this fear about threats against ‚Äúthe West‚Äù. To hear that you are actually having these experiences in Hong Kong I wouldn‚Äôt have expected.

EY: I went to an international school so their first language was English. It was a very Western atmosphere. Out of all my friends, I was the deepest into this stuff. I was bringing a lot of these ideas to them, especially my guy friends, and they would latch onto it. Then as I‚Äôd talk about it around my female friends they‚Äôd freak out and I‚Äôd feel vindicated. Because like, Oh, I triggered someone. I have the power to deliver this uncomfortable truth them and make them emotionally react.

I can‚Äôt speak to how similar or different it was from an American high school experience, but it was certainly a very Western experience. It‚Äôs embarrassing to admit, but a lot of the guys would just say the n-word and we would get reprimanded for it. All of these moral norms in the US are the same over there. Obviously we‚Äôre consuming American pop culture, we‚Äôre watching American TV shows, we‚Äôre listening to American music. There was no connection of that word for me to race, it was just the idea that I get to use it and I can just kind of say that and trigger a reaction in people. And that‚Äôs kind of a global feeling.

AT: I was talking to somebody recently who told me he was really into this stuff for a time during his adolescence. This is somebody who if you met him you would not guess in a million years that any of this materiel ever appealed to any part of him, and it did. I‚Äôm almost 33 now-this subculture as I witnessed it in my own adolescence was more or less the same. The platforms were different, I don‚Äôt think 4chan existed when I was in high school, but the texture of it was all pretty much identical in the sense of using racial slurs, maximizing offense, and being jarring in the worst possible way. I didn‚Äôt really participate in that, but my teenage rebellion certainly had a digital form. This was when we had AOL Instant Messenger. I would just find people and troll them. I was just like a little shitposter. The line dividing my own form of trolling and more outwardly hateful activity is razor thin. Between you and talking to this other person and thinking about me, and thinking about other people I know from high school, I‚Äôve realized that this transgressive online behavior is not this fringe practice that a few troubled youth drift into, this is pretty pervasive among males of a certain age to the extent that I would almost say you can assume that if you see a young male who isn‚Äôt exceedingly attractive, he is probably involved in this in some way, or is adjacent to it. It‚Äôs just the culture they default to. What the media has done is framed it as this marginal subculture when in fact, it‚Äôs actually this mainstream culture that just hasn‚Äôt been identified as such until very recently.

EY: I think that‚Äôs spot on and that‚Äôs something that I‚Äôve been looking into a lot, how it‚Äôs actually the norm. Often the media doesn‚Äôt treat the internet with the respect that it needs to. There‚Äôs no discussion of it with an actual understanding that these are real people and this is a serious phenomenon. I remember on CNN they were like, ‚ÄúWho is this hacker 4chan?‚Äù It reminds me of the kinds of things that I was thinking, because a lot of this is kind of a rejection of the mainstream media, mainstream ideals, partially to be edgy, but also partially because you don‚Äôt feel like it reflects or represents you.

AT: There‚Äôs just more people engaging on /pol today than there were just a few years ago. So now it‚Äôs even more mainstreamed than it was and has developed a critical mass.

EY: There‚Äôs almost like no meaningful, actual alternative to these monolithic places of discussion. If you go on Reddit, for example, there is still a voting system where the most popular ideas go to the top and the most controversial ideas [within a given subreddit] go away. And so if you want to express controversial ideas that don‚Äôt necessarily line up exactly with the way everyone around you is supposed to think, there‚Äôs no way to do that.

The way that 4chan is laid out, it feels like you‚Äôre forced to engage with as many ideas as are thrown at you. There are conservative Reddits, but then those don‚Äôt really get as far as 4chan. And you don‚Äôt really get exposed to as many ideas that have the potential for that thrill of revelation. When you‚Äôre first starting out engaging in political ideas, you get those feelings of revelation over and over again. And it‚Äôs an interesting feeling, it feels like you‚Äôre learning. And so once you find something like Reddit, once you get a good grasp on say, if you go on r/gaming, like if you‚Äôre on the default gaming subreddit, it‚Äôs like okay, I get what these people are saying now. This is boring to me. What‚Äôs next? Oh, let‚Äôs find more and more specific, niche things. A platform like Reddit, is, I think, eventually not enough for someone who‚Äôs looking for that kind of discussion. There‚Äôs almost no alternative but 4chan to go to consistently have those provocative feelings. You don‚Äôt really see that on Reddit once you go through it.

I think that that is what that‚Äôs indicative of. Not that I‚Äôm saying that people are using 4chan as a substitute for Reddit. I just think that that is a general trend, where 4chan is interesting because it‚Äôs constantly presenting you with new ideas.

AT: You bring up a good point which is that it‚Äôs popular because it‚Äôs not one of these big platforms. It isn‚Äôt owned. I don‚Äôt even know who runs it. Who maintains the 4chan servers? I don‚Äôt know who the people are behind it. I think that there‚Äôs an appeal to that.

I‚Äôve also been considering what Reddit is. Is it good, is it bad, what‚Äôs good, what‚Äôs bad, and so on. Reddit has basically subsumed all message boards at this point. You don‚Äôt go to message boards anymore. You go on different kinds of social media, and Reddit is the closest form of social media we have to a message board. But it doesn‚Äôt mimic message boards a lot. 4chan feels more like an old forum to me, it‚Äôs more Web 1.0 than something like Reddit is. I think that there‚Äôs no other places off these platforms to go, really. What are the other non-alt-right message boards that aren‚Äôt owned by venture capitalists?

Wherever you spend your time, the more time you spend there, the more you just absorb the ideas you‚Äôre surrounded by. I find myself absorbing ideas even when I‚Äôm approaching them from a distance. Even when researching things, I can feel them trying to work on me in someway.

EY: I use the word ‚ÄúChad‚Äù in conversation sometimes. The language seeps into your brain, and you start to think of things in that same framework. It‚Äôs really ridiculous.

AT: The YouTuber ContraPoints did that incels video, and she actually said that just in the course of researching her video, she started using the words like Chad and Stacy. I think the reason she did that, and the reason you do that, and the reason I do it, is because the word Chad is funny and is a great word for a man of bulletproof fuckability.

It‚Äôs also indicative of more pernicious and more destructive ideas that can seep in just by sheer proximity. The way that there‚Äôs research indicating that if you force yourself to smile, you feel happier. Who knows, it‚Äôs pop psychology bullshit, I don‚Äôt know if it‚Äôs real or not. But I do think it‚Äôs real in the sort of digital-political realm, where just by touching these ideas, you are susceptible to them in some way.

This is data from r/Braincels and r/TheRedPill. These are the most popular subreddits for both people. The one that really stuck out to me in the r/Braincels results was the subreddit NEET, which stands for ‚ÄúNot in Employment, Education or Training‚Äù, and I have kind of a long thought I want to launch into for a minute.

This overlap between the sexual and economic outcasts is I think brought up really well in Nagle‚Äôs book. She‚Äôs talking about this guy Roger Devlin in this passage:

His essay ‚ÄúSexual Utopia and Power‚Äù argues against ‚Äútoday‚Äôs sexual dystopia with its loose morals and confused sexual roles. It explores ‚Äú female hypergamy‚Äù, mating up, narcissism, infidelity, deceptiveness, and masochism.‚Äù It also argues that ‚Äúthe breakdown of monogamy results in promiscuity for the few, loneliness for the majority.‚Äù On this last point, I think he‚Äôs getting to the central issue driving this kind of reactionary sexual politics, perhaps even the central personal motivation behind the entire turn to the far right among young men. The sexual revolution that started the decline of lifelong marriage has produced great freedom from the shackles of loveless marriage and selfless duty to the family for both men and women, but this ever-extended adolescence has also brought with it the rise of adult childlessness and a steep sexual hierarchy. Sexual patterns that have emerged as a result of the decline of monogamy have seen a greater level of sexual toys for an elite of men and a growing celibacy among the large male population at the bottom of the pecking order. Their own anxiety and anger about their lower ranking status in this hierarchy is precisely what it‚Äôs produced that are hard line rhetoric about asserting hierarchy in the world politically when it comes to women and non-whites. The pain of relentless rejection has festered in these forums and allowed them to be the masters of the cruel natural hierarchies that bring them so much humiliation.

From Dataclysm by Christian Rudder

I thought of two things when I read this passage. The first was a graph from this book Dataclysm by Christian Rudder, who founded OkCupid. Rudder ultimately published this book that drew on a lot of different datasets but also used the OkCupid dataset. This is one of the analyses that was published in Dataclysm. OkCupid doesn‚Äôt work like this anymore as I understand, but for a long time when you came across another user, you would rate them from one to five stars. It‚Äôs probably good we don‚Äôt do that anymore. But that was how the platform operated. And I remember that if you were in a certain echelon of people, if you were in the top 10 percent or whatever-and I wasn‚Äôt-you would get this email that congratulated you on being the creme de la creme of attractiveness.

According to the data on OkCupid, women find men less attractive than men find women. And I think you can synthesize what Nagle is saying with this graph and just see this swelling discontent on incels.co and Braincels and TheRedPill as essentially the casualties of this new sexual system we‚Äôve been developing since the 1970s where relations are reduced to this sexual marketplace.

The second thing I thought of was basically the entire worldview of Michel Houellebecq, whose ideas I see throughout Kill All Normies. His first big book has written a lot about how the sexual revolution, despite being associated with the left, was really the beginning of a cultural neoliberalism. Just as institutions under neoliberalism have dissolved and been taken over by market forces, our social relations are unmediated by anything other than sheer desire and you end up with nothing but a bazaar of flesh. One result of that is that you end up with a sexual underclass, the sexual counterpart to the NEET subreddit. Just as people are cast away from the job market through automation and are no longer able to adapt to the world necessary to survive economically, it‚Äôs more difficult for people to survive in this new sexual reality, especially one that provides a method of meeting people on digital platforms that optimize for quick, weak interactions that give little opportunity for anything other than physicality to dominate the selection process.

I think the bearing that reality has on people differs on geography. I don‚Äôt know how much Tinder is used in Laramie, Wyoming, where I used to live, but I can tell you that it is the dominant method by which people meet each other in places like New York City. And I have watched people move here and thrive or die in that system. I have seen people go in completely opposite directions. I can think of two people off the top of my head where this happened. One of them was a very attractive girl who moved to Brooklyn from Philadelphia, and I think Philadelphia is less dictated by this ruthless superficiality than New York, which makes a lot of sense when you think about New York as a neoliberal center, both economically and in this cultural sense we‚Äôre talking about. She moved here and became this #bestself member of this culture‚Äôs sexual elite who saw her own sexual activity as basically the extent of her politics.

The other friend also moved here from Philadelphia and I guess he split with his girlfriend or something, and I found these Red Pill leanings creeping into his personality. He believed women‚Äôs brains were hackable. He would call women ugly to me. It was really weird. And I would tell him not to do it and he would laugh and find it funny. To him, that was his own brand of 4chan offensiveness, he was upsetting these feminist sensibilities of the left. And what was so fucking bizarre was that he was kind of a leftist. He worked as an environmental organizer for years and lived in Brooklyn and had a bookshelf of Chuck Klosterman and whatever you would find on a Brooklyn bookshelf. But I think the longer he went without sex, the more hateful he became. He actually referred to himself as celibate. He would say, ‚ÄúWell I‚Äôm celibate now.‚Äù He never said incel. I had never heard that term actually until 2018, but he was the first person I ever knew who referred to himself as celibate.

My friendships with both these people reached their conclusions for these very reasons, which sucks because in New York City friendships don‚Äôt abound. But it was bizarre to see both of them sort of convert to this worldview.

EY: It seems like that worldview is being forced upon her, not her reinforcing the assumptions of that ideology.

AT: Absolutely, and that that‚Äôs my whole point. Even if incels engage in toxic behavior, they exist in a macro historical moment in which that behavior is cultivated. That absolutely goes for her, and it goes from my other friend as well. It goes for all of us, not to completely dismiss any kind of agency. And I think that this is an environment that I‚Äôm always relieved when people acknowledge, whether it be Houellebecq or Nagle.

EY: But then I think that things like the Amazon documentary The Red Pill and places like Rolling Stone and all of them almost never investigate this, the moment that we find ourselves in that is creating this kind of hyper-sexual frustration.

AT: And I will say as well, so not only have things been replaced by this sexual marketplace but we are daily inundated with images establishing what sort of conventional existence looks like. There‚Äôs a fascinating paper written by Jonah Peretti, who founded Buzzfeed, wrote when he was at MIT. It‚Äôs one of the most interesting papers I think I‚Äôve ever read. His whole idea was that in this current stage of capitalism, we cycle through identities more quickly.

Lacan had this idea in his psychoanalysis of the Mirror Stage where the child develops a sense of itself when it looks at the mirror and it recognizes itself. But this phenomenon is extended past the mirror and into images. So we don‚Äôt just identify with the reflection of ourselves in the mirror. We then identify with the image on the movie screen or the TV screen or phone screen or whatever sort of picture is looking back at us. And that sort of becomes our sense of self as well.

Because we are so inundated with these images and these narratives, we find ways to reconcile our own sense of identity with this identity that is projected back at us from the screen. You can think of a child after they watch The Avengers, they go and they play Avengers with their friends and they think of themselves as Captain America and Black Widow and whatever. That‚Äôs kind of a very rudimentary, basic example of this idea of the child taking on the identify of the screen.

But we do this all the time as adults. So extrapolate that behavior to an adult looking at an image in a Chanel advertisement or watching The Social Network, and they see in the mirror a model or a machinating startup CEO. The adult takes on the identity of what is projected at them by the image and then reconciles their own sense of self with what they see projected at them.

With people like incels you have something of an identity crisis. Because of this new sexual marketplace, they are no longer able to close the gap between their own identities and the identities projected back at them in the image. And because of the endless white noise of media, they are drowning in reminders of their very inability to close that gap.

Something the activist-turned-red-piller/black-piller I mentioned observed that I think is very true is that we don‚Äôt really have an outlet for men who don‚Äôt have sex or can‚Äôt have sex for whatever reason. There‚Äôs no monastery for them to join. We have created this system where you are either getting laid or you‚Äôre not. Those are your two options. And if you can‚Äôt reconcile your identity with that of the image, which is almost universally an image of someone having lots of sex, we don‚Äôt really give you any other options. There‚Äôs no image of, like, a monk.

I sent you that Oneohtrix Point Never video, I don‚Äôt know if you watched it or not.

EY: Yes, I watched it. I watched the shit out of it.

AT: To me it captures everything about what we‚Äôre saying in the span of four minutes, for the most part. Mostly what it captures is that alienation and ostracization and totally unfulfillable desire among these people. That there‚Äôs just no way to satiate these urges, and the urges become ever more extreme the more time you spend feeding them and dwelling within them. Nagle‚Äôs quote about the unsatiated desire being the fundament of everything is captured in that OPN video better than anything I‚Äôve seen.

EY: Yeah, I think so. And something interesting that stood out to me about that video is that there are no ‚Äúreal women‚Äù in that video, it‚Äôs all furries or hentai. It makes me wonder whether the phenomenon of incels loving hentai is almost self-fulfilling-they pine after these girls and they have these waifus and they love these things just because part of them knows that this isn‚Äôt an obtainable, real thing. It‚Äôs another form of comfort and safety for them. Like, ‚ÄúI can fall in love with a fictional character, and some part of me knows that will never happen, so committing to that is safe.‚Äù

AT: I also think that it‚Äôs a sign of just how alien an actual woman is to this culture. It is so distant and abstract that it becomes unreal. In a way, the furries and the hentai girls are both literal depictions of what these people end up desiring, but also these non literal representations of what women are to them.

I looked up ‚Äúred pill‚Äù and I looked up ‚Äúblack pill‚Äù on /pol. People say ‚Äúred pill‚Äù more, but it‚Äôs interesting to see the climb of ‚Äúblack pill‚Äù relative to ‚Äúred pill‚Äù and I‚Äôm wondering in your research if you‚Äôve gotten the sense that the use of term is becoming more pervasive. Do you think that it indicates a deeping nihilism among these people, or is this just kind of a coincidence?

EY: To me it just feels like ‚Äúred pill‚Äù is becoming a normie concept now. It‚Äôs mainstream. So they‚Äôre kind of getting backed into a corner and I don‚Äôt know where you can go past the black pill. There‚Äôs no darker black pill.

AT: The jet black pill.

EY: Seeing this makes me feel like, Okay there is this kind of ‚Ä¶ ‚Äú I‚Äôm trying to reconcile this rejection of the mainstream with all the other things you‚Äôre talking to me about. Like because it is ‚Ä¶ because the mainstream idea is the sexual marketplace, right? And then there‚Äôs the belief that the sexual marketplace can‚Äôt benefit you. And so I feel like those ideas kind of intertwined like the wires get crossed or something. Where once an idea starts to become part of a more mainstream cultural consciousness it becomes almost, yuck, you‚Äôre deterred from it.

It reminds that I was talking to this guy who went viral talking about his alt-right journey and one word that he kept using I actually had never used before was ‚Äúboomer‚Äù. He was saying like, ‚ÄúOh they‚Äôre just a bunch of Boomers.‚Äù ‚ÄúThat‚Äôs just Boomer talk.‚Äù And so that really had me thinking about the way all of this stuff is situated in their minds. It all can come back to that enemy outside thing. And so once the Red Pill ideas are hitting mainstream and if ‚Ä¶ let‚Äôs say Chads start building in TheRedPill, which they can do, the Red Pill isn‚Äôt an exclusively incel thing, then where do you go. You have to start moving into this space where it‚Äôs like, Okay this ideology is now not somewhere that I feel safe. I have to take it further. And that just gets encouraged by everyone else who feels the same way and feels like you have to be edgy. You have to be on the edge, you have to be on the fringe.

AT: I feel like that pertains to this other result here. I looked at all three-word combinations used in any /pol post with the words ‚Äúred pill‚Äù and the one that stuck out to me was any terms that include the word ‚Äúultimate‚Äù. This idea of the ultimate red pill, and how commonly that factors into these discussions, this idea of like taking the final red pill. Nick Land‚Äôs decision to title his manifesto the ‚ÄúDark Enlightenment‚Äù bears on this as well, I think, which implies a forbidden knowledge.

It makes me think of something Mark Fisher said in this essay he wrote about Joy Division. ‚ÄúThe depressive is always confident of one thing: that he is without illusions.‚Äù There‚Äôs this idea that the more nihilistic the idea is, the truer is must be. And I wonder if that becomes its own reinforcement to people who traffic in the language of the red pill and the black pill and the dark enlightenment. That because it‚Äôs so nihilistic, it is this form of realism.

EY: It reminds me of how incels will go and they‚Äôll post pictures of themselves to be rated on, fully expecting and knowing for a fact that all they‚Äôre going to get is criticism and reaffirmation that their worldview, that they‚Äôre never going to be able to fuck, is true. It‚Äôs almost comforting in the most depressing sense: Everything I‚Äôm thinking is right, I am worthless, I am terrible, any hope is illusion.
What is a single view of the customer?

A single customer view is a centralized location that consolidates all the knowable data and information of your customers within your organization and enables you to view, find, and understand every aspect of your customers. Having a single view of customers within your environment helps you understand what and how your customers purchase. These intelligence servers as fuel to your sales and marketing team‚Äôs formulation and development of strategy.

Insight into the difficulties?

Extraction, consolidation, and integration are what you need on the face of it. The implementation and the solution of it are not straightforward at all, though. Customer interaction with your services or/and products can be long, stretching across multiple online and offline channels and touchpoints. Managing, governing, consolidating, and transforming valuable data and information with a tremendous variety across different channels and touchpoints is very much challenging.

Data management practice helps address the challenges

Reviewing the end-to-end process and mechanics of building a single customer view reveals what elements of data management practice help solve the challenges.

Step 1 Extraction: extraction of all the knowable data of your customers from different systems by similar column names, by documentation from business analysts, and by knowledge from system analysts. But

A. what knowable customer data do you have in your organization?

B. how do you know which system(s) to extract the customer data that you search for?

Elements: Data Dictionary, Enterprise Data Flow, Data Lineage

Step 2 Data Cleansing: removal of all the noises of your customer data by rules provided by IT. But
Cython will give your Python code super-car speed

Want to be inspired? Come join my Super Quotes newsletter. üòé

Python is a community favourite programming language! It‚Äôs by far one of the easiest to use as code is written in an intuitive, human-readable way.

Yet you‚Äôll often hear the same complaint about Python over and over again, especially from the C code gurus out there: Python is slow.

And they‚Äôre not wrong.

Relative to many other programming languages, Python is slow. Benchmark game has some solid benchmarks for comparing the speed of various programming languages on different tasks.

There‚Äôs a couple of different ways to speed things up that I‚Äôve written about before:

(1) Use multi-processing libraries to use all the CPU cores

(2) If you‚Äôre using Numpy, Pandas, or Scikit-Learn, use Rapids to accelerate the processing on GPU.

Those are great if what you‚Äôre doing can, in fact, be parallelized, such as data pre-processing or matrix operations.

But what if your code is pure Python? What if you have a big for-loop that you just have to use and can‚Äôt put into a matrix because the data has to be processed in sequence? Is there a way to speedup Python itself?

That‚Äôs where Cython comes in to speed up our raw Python code.

What is Cython?

At its core, Cython is an intermediate step between Python and C/C++. It allows you to write pure Python code with some minor modifications, which is then translated directly into C code.

The only adjustment you make to your Python code is adding type information to every variable. Normally, we might declare a variable in Python like this:

x = 0.5

With Cython, we‚Äôre going to add a type to that variable:

cdef float x = 0.5

This tells Cython that our variable is floating point, just like we would do in C. With pure Python, the variable‚Äôs type is determined on the fly. The explicit declaration of the type in Cython is what makes the conversion to C possible, since explicit type declarations are required+.
Caveat: As of this writing, I‚Äôve used the following database-like systems in a production environment: MySQL, PostgreSQL, Hive, MapReduce on Hadoop, AWS Redshift, GCP BigQuery, in various mixes of on-prem/hybrid/cloud setups. My optimization knowledge largely stems from those. I‚Äôll stick to strategies/thinking process here, but there are definitely features and quirks in other popular databases that I‚Äôm not familiar with, especially from SQL Server and Oracle.

This article is about speed, common strategies for making things go FASTER while avoiding specific implementation details. I‚Äôm trying to express the thought process of optimization, not the specific mechanics. Before I knew it, it‚Äôs turned into a monster of an article. There‚Äôs a lot to cover!

Intro and Background

Optimizing queries is a hard topic to write about because it involves specifics. Specifics about database engines, software, and sometimes even hardware and network architecture. I‚Äôve been asked to write about this topic multiple times, and I‚Äôve always resisted because I couldn‚Äôt see a way to write a generally useful article for something that very quickly gets into the weeds.

There are entire books written about how to optimize different database systems, which includes queries, but also details in the tuning of the systems themselves. They‚Äôre always written about a specific platform, not in general. It‚Äôs for good reason ‚Äî every platform is different and the tuning parameters you need depend on your workload and setup (write heavy vs read heavy, SSDs vs Spinning disk, etc).

But on the way home during a nasty heatwave, I had an sudden flash of insight as to what threads tie optimization together. So I‚Äôm giving this crazy article a try. I‚Äôm going to avoid too many specifics and focus on the core thinking process that goes into identifying the things that will make your queries go faster. There will be forays into specifics only for illustrative purposes, and no real code examples. Also for brevity, I can‚Äôt be super thorough, but I‚Äôll link to examples and further reading as I go.
Predicting vs. Explaining

A directed acyclic graph depicting the causal pathways to foetal alcohol spectrum disorders

The Cultural War in Cognitive Science

I recently stumbled across this really juicy debate on natural language processing that took place a few years ago between the old guard of the field, Noam Chomsky, who‚Äôs considered as ‚Äúthe father of modern linguistics,‚Äù and the new guard, Peter Norvig, Director of Research at Google. When commenting on where the field was heading, Chomsky said the following:

‚ÄúSuppose that somebody says he wants to eliminate the physics department and do it the right way. The ‚Äúright‚Äù way is to take endless numbers of videotapes of what‚Äôs happening outside the video, and feed them into the biggest and fastest computer, gigabytes of data, and do complex statistical analysis ‚Äî you know, Bayesian this and that ‚Äî and you‚Äôll get some kind of prediction about what‚Äôs gonna happen outside the window next. In fact, you get a much better prediction than the physics department will ever give. Well, if success is defined as getting a fair approximation to a mass of chaotic unanalyzed data, then it‚Äôs way better to do it this way than to do it the way the physicists do, you know, no thought experiments about frictionless planes and so on and so forth. But you won‚Äôt get the kind of understanding that the sciences have always been aimed at ‚Äî what you‚Äôll get at is an approximation to what‚Äôs happening.‚Äù

Chomsky reinforced that sentiment repeatedly elsewhere: that the current definition of success in natural language processing ‚Äî namely predictive accuracy ‚Äî is not science. Throwing ‚Äúsome immense corpus of text‚Äù into a ‚Äúcomplicated machine‚Äù is merely ‚Äúapproximating unanalyzed data,‚Äù or ‚Äúbutterfly collecting,‚Äù that would not lead to ‚Äúreal understanding‚Äù of the language. He argues that the main goal of science is to ‚Äúdiscover explanatory principles‚Äù of how a system actually works, and the ‚Äúright approach‚Äù to achieve that goal is to ‚Äúlet the theory guide the data‚Äù: study the system‚Äôs basic nature by abstracting away ‚Äúirrelevant intrusions‚Äù through carefully designed experiments ‚Äî the same way modern science has been conducted since Galileo. In his own succinct words: ‚ÄúJust trying to deal with the unanalyzed chaotic data is unlikely to get you anywhere, just like as it wouldn‚Äôt have gotten Galileo‚Ä¶
After several failed ML projects due to unexpected ML degradation, I wanted to share my experience in ML models degradation. Indeed, there is a lot of hype around model creation and development phase, as opposed to model maintenance.

Assuming that a Machine Learning solution will work perfectly without maintenance once in production is a faulty assumption and represents the most common mistake of companies taking their first artificial intelligence (AI) products to market.

The moment you put a model in production, it starts degrading.

Why Do ML Models Degrade With Time?

As you may already know, data is the most crucial component of a successful ML system. Having a relevant data set that provides you with accurate predictions is a great start, but how long will that data continue to provide accurate predictions?

In all ML projects, it is key to predict how your data is going to change over time. In some projects, we underestimated this step and it became hard to deliver high accuracy. In my opinion, as soon as you feel confident with your project after the PoC stage, a plan should be put in place for keeping your models updated.

Indeed, your model‚Äôs accuracy will be at its best until you start using it. This phenomenon is called concept drift, and while it‚Äôs been heavily studied in academia for the past two decades, it‚Äôs still often ignored in industry best practices.

Concept drift: means that the statistical properties of the target variable, which the model is trying to predict, change over time in unforeseen ways. This causes problems because the predictions become less accurate as time passes.

The key is that, in contrast to a calculator, your ML system does interact with the real world. If you‚Äôre using ML to predict demand and pricing for your store, you‚Äôd better consider this week‚Äôs weather, the calendar and what your competitors are doing.
Recurrent Neural Network

The Recurrent neural networks are a class of artificial neural networks where the connection between nodes form a directed graph along a temporal sequence. Unlike the feed-forward neural networks, the recurrent neural networks use their internal state memory for processing sequences. This dynamic behavior of the Recurrent neural networks allows them to be very useful and applicable to audio analysis, handwritten recognition, and several such applications.

Simple RNN implementation in Keras.

Mathematically the simple RNN can be formulated as follows:

Where x(t) and y(t) are the input and output vectors, W·µ¢‚Çï, W‚Çï‚Çï, and W‚Çï‚Çí are the weight matrices and f‚Çï and f‚Çí are the hidden and output unit activation functions.

The implementation of RNN with 2 Simple RNN layers each with 32 RNN cells followed by time distribute dense layers for 10 class classification can be illustrated as follows:
Need for dimensionality reduction

In machine learning projects we often run into curse of dimensionality problem where the number of records of data are not a substantial factor of the number of features. This often leads to a problems since it means training a lot of parameters using a scarce data set, which can easily lead to overfitting and poor generalization. High dimensionality also means very large training times. So, dimensionality reduction techniques are commonly used to address these issues. It is often true that despite residing in high dimensional space, feature space has a low dimensional structure.

Two very common ways of reducing the dimensionality of the feature space are PCA and auto-encoders. I will only provide brief introduction to these, for a more theoretically oriented comparison read this post.

PCA

PCA essentially learns a linear transformation that projects the data into another space, where vectors of projections are defined by variance of the data. By restricting the dimensionality to a certain number of components that account for most of the variance of the data set, we can achieve dimensionality reduction.

Autoencoders

Autoencoders are neural networks that can be used to reduce the data into a low dimensional latent space by stacking multiple non-linear transformations(layers). They have a encoder-decoder architecture. The encoder maps the input to latent space and decoder reconstructs the input. They are trained using back propagation for accurate reconstruction of the input. In the latent space has lower dimensions than the input, autoencoders can be used for dimensionality reduction. By intuition, these low dimensional latent variables should encode most important features of the input since they are capable of reconstructing it.

Comparison

PCA is essentially a linear transformation but Auto-encoders are capable of modelling complex non linear functions. PCA features are totally linearly uncorrelated with each other since features are projections onto the orthogonal basis. But autoencoded features might have correlations since they are just trained for accurate reconstruction. PCA is faster and computationally cheaper than autoencoders. A single layered autoencoder with a linear activation function is very similar to PCA. Autoencoder is prone to overfitting due to high number of parameters. (though regularization and careful design can avoid this)

When to use which?

Apart from the consideration about computational resources, the choice of technique depends on the properties of feature space itself. If the features have non-linear relationship with each other than autoencoder will be able to compress the information better into low dimensional latent space leveraging its capability to model complex non-linear functions. What does it mean for the features to have non-linear relationships? Let us do a couple of simple experiments to answer these questions and shed some light on comparative usefulness of both techniques.

Experiments 2D

Here we construct two dimensional feature spaces (x and y being two features) with linear and non-linear relationship between them (with some added noise). We will compare the capability of autoenocoders and PCA to accurately reconstruct the input after projecting it into latent space. PCA is a linear transformation with a well defined inverse transform and decoder output from autoencoder gives us the reconstructed input. We use 1 dimensional latent space for both PCA and autoencoders.

It is evident if there is a non linear relationship (or curvature) in the feature space, autoencoded latent space can be used for more accurate reconstruction. Where as PCA only retains the projection onto the first principal component and any information perpendicular to it is lost. Let us look at the reconstruction cost as measured by mean squared error (MSE) in the table below.

Experiments 3D

Conducting similar experiments in 3D. We create two three dimensional feature spaces. One is a 2D plane existing in 3D space and the other is a curved surface in 3D space.

We can see that in case of a plane there is a clearly two dimensional structure to the data and PCA with two components can account for 100% of the variance of the data and can thus achieve perfect reconstruction. In case of a curved surface two dimensional PCA is not able to account for all the variance and thus loses information. The projection to the plain that covers the most of variance is retained and other information is lost, thus reconstruction is not that accurate. On the other hand autoencoder is able to reconstruct both plane and surface accurately using two dimensional latent space. So 2D latent space is able to encode more information in case of autoencoder because it is capable of non-linear modelling. Reconstruction cost is provided in the table below.

Random Data Experiment

Here we create a random data without any collinearity. All features are independently sampled from a uniform distribution and have no relationship with each other. We use two dimensional latent space fro both PCA and Autoencoder.

We see that PCA is able to retain the projection onto the plane with maximum variance, and loses a lot of information because the random data did not have a underlying 2 dimensional structure. Autoencoder also does poorly since there was no underlying relationship between features.

Conclusion

For dimensionality reduction to be effective, there needs to be underlying low dimensional structure in the feature space. I.e the features should have some relationship with each other.

If there is non-linearity or curvature in low dim structure than autoencoders can encode more information using less dimensions. So they are a better dimensionality reduction technique in these scenarios.

All code for the experiments can be found here:
An Introduction to Recurrent Neural Networks for Beginners

A simple walkthrough of what RNNs are, how they work, and how to build one from scratch in Python. Victor Zhou ¬∑ Follow Published in Towards Data Science ¬∑ 10 min read ¬∑ Jul 25, 2019 -- 2 Share

Recurrent Neural Networks (RNNs) are a kind of neural network that specialize in processing sequences. They‚Äôre often used in Natural Language Processing (NLP) tasks because of their effectiveness in handling text. In this post, we‚Äôll explore what RNNs are, understand how they work, and build a real one from scratch (using only numpy) in Python.

This post assumes a basic knowledge of neural networks. My introduction to Neural Networks covers everything you‚Äôll need to know, so I‚Äôd recommend reading that first.

Let‚Äôs get into it!

1. The Why

One issue with vanilla neural nets (and also CNNs) is that they only work with pre-determined sizes: they take fixed-size inputs and produce fixed-size outputs. RNNs are useful because they let us have variable-length sequences as both inputs and outputs. Here are a few examples of what RNNs can look like:

Inputs are red, the RNN itself is green, and outputs are blue. Source: Andrej Karpathy

This ability to process sequences makes RNNs very useful. For example:

Machine Translation (e.g. Google Translate) is done with ‚Äúmany to many‚Äù RNNs. The original text sequence is fed into an RNN, which then produces translated text as output.

(e.g. Google Translate) is done with ‚Äúmany to many‚Äù RNNs. The original text sequence is fed into an RNN, which then produces translated text as output. Sentiment Analysis (e.g. Is this a positive or negative review?) is often done with ‚Äúmany to one‚Äù RNNs. The text to be analyzed is fed into an RNN, which then produces a single output classification (e.g. This is a positive review).

Later in this post, we‚Äôll build a ‚Äúmany to one‚Äù RNN from scratch to perform basic Sentiment Analysis.

Note: I recommend reading the rest of this post on victorzhou.com ‚Äî much of the math formatting looks better there.

2. The How

Let‚Äôs consider a ‚Äúmany to many‚Äù RNN with inputs x_0‚Äã, x_1‚Äã, ‚Ä¶ x_n‚Äã that wants to produce outputs y_0‚Äã, y_1‚Äã, ‚Ä¶ y_n‚Äã. These x_i‚Äã and y_i‚Äã are vectors and can have arbitrary dimensions.

RNNs work by iteratively updating a hidden state h, which is a vector that can also have arbitrary dimension. At any given step t,
Often in data-constrained scenarios, scene comprehension has to occur with few time series observations - whether that‚Äôs audio, visual, or even radar. We do this using a surprisingly underrated technique called wavelet scattering.

Wavelet scattering (or scatter transform) generates a representation that‚Äôs invariant to data rotation/translation and stable to deformations of your data. Uninformative variations in your data are discarded ‚Äî e.g. an audio sample time-shifted by various amounts. Information for downstream tasks like classification is preserved. Wavelet scattering requires no training and works great with low data.

Its main computation is convolution, making it fast and applicable to images and 1D signals. We focus on signals in this article. We will retrace findings by the signal processing community and relate it to modern machine learning concepts. I show that, yes, we can do great without learning, using 20 samples. Recreate experiments and illustrations in this article with the colab notebook in this link.

Wavelets

A wavelet can be convolved with the signal in the same sense that filters can. I think of convolution as the continuous analog to inner products, where large activation (commonly said in ML) or wavelet coefficient is caused by similarity between the continuous objects. By convolving elements from a dictionary to the signal under inspection, we capture local, spatial dependencies.

Convolution is a pivotal computation in the emergence of deep learning ‚Äî it is extremely fast. The wavelet scattering implementation used by this article calls a deep learning backend solely for the efficient convolution! Kymatio is a great Python package built by passionate researchers that implement wavelet scattering, leveraging the PyTorch framework.

Real and imaginary components of the Morlet Wavelet from M. Adamczyk et al., Automatic Sleep Spindle Detection and Genetic Influence Estimation Using Continuous Wavelet Transform (2015)

The basic building block of wavelet scattering is the Morlet wavelet. It is a Gaussian windowed sinusoid with deep connections to mammal hearing and vision. By convolving wavelets œà·µ• indexed by different frequency locations v, the wavelet transform of x is the set of scatter coefficients

{ x ‚àó œà·µ• }·µ•

When the wavelet‚Äôs sine component has room to dilate (sine wave ‚Äòslowing‚Äô its oscillation), it decomposes the signal at decorrelated scales. That‚Äôs good for revealing the signal‚Äôs frequency structure, but doing so over the course of a longer time range. The consequence is that a wider Gaussian window trades temporal resolution for increased frequency resolution ( itself a consequence of Heisenberg‚Äôs Uncertainty Principle). In practice, the width of the Gaussian window that tapers the sine wave is an important parameter [M. Cohen 2018].

Wavelet Scattering

The historic context of wavelet scattering starts with Fourier transform, the canonical signal processing technique. The shortcoming of Fourier representation includes its instability to signal deformations at high frequency. For a signal x perturbed slightly by a high frequency deformation into xÃÉ, their spectrogram representations look different ( large ‚Äñ FFT(x) -FFT(xÃÉ) ‚Äñ ) even if they remain similar signals to the human eye. This instability is due to sine wave‚Äôs inability to localize frequency information, since sine itself has non-localized support.

November, Golden Gardens credit: u/purebredcrab at reddit.com/r/analog

Wavelet transform fixes this by decomposing the signal with a family of wavelets, with various dilation, where every wavelet has localized support (flattening out eventually like the Morlet wavelet). The resulting wavelet representation localizes high frequency components of the signal. Yet because the wavelet operator commutes with translations, the resulting representation becomes translation covariant ‚Äî shifting a signal also shifts its wavelet coefficients. This makes comparison between translated signals difficult, and translation invariance is key to tasks like classification. How do we achieve a signal representation Œ¶(x) that is translation invariant, stable under deformations, and offers good structural information at all frequencies?

Wavelet scattering builds a signal representation Œ¶(x) with a redundant dictionary of Morlet wavelets. While the space of signals X can be really high dimensional, the transform forms a kernel metric over the space of signals, inducing a lower dimensional manifold. Watch St√©phane Mallat discuss the manifold interpretation with visualization.

Readers have probably trained a neural convolution network to encode an image into a latent manifold Z, whose code/latent representation is used for classification or structure discovery ‚Äî and that‚Äôs what is happening in analogy. Wavelet scattering encodes the dataset X where uninformative variability in X: translation, rotation, and scaling ‚Äî the action of groups ‚Äî are discarded in the process.

The key benefits of transforming a signal by Œ¶ [J. Bruna and S. Mallat, 2013] is that

Œ¶ is invariant to signal translation.

Denote by x‚Çú a signal identical to x, except translated in time, then Œ¶(x) = Œ¶(x‚Çú).

Œ¶ is stable under signal deformations.

I.e. Œ¶ is Lipschitz continuous to deformations ‚Äî difference between scatter representations of a signal with its deformed version is linear. A deformation can be some local displacement/distortion (or a ridiculous amount of distortion, as a later example shows). For Lipschitz constant C>0 and a displacement field œÑ(u) causing deformations to create xÃÉ,

‚ÄñŒ¶( x ) - Œ¶( xÃÉ )‚Äñ ‚â§ C‚Äñx‚Äñ sup·µ§|‚àáœÑ(u)|

Where ‚Äñx‚Äñ= ‚à´ ‚Äñx(u)‚Äñ¬≤du and sup·µ§|‚àáœÑ(u)| is the global deformation amplitude.

Œ¶ does not require learning.

The priors introduced by wavelet scattering are nice enough that its performance often makes learning redundant; plus it comes with interpretable features and outputs. In data-constrained scenarios, if comparable data is publicly available, a nice plan is to pipe your small dataset through a pretrained model. But in the difficult situation that your dataset is small and unique, consider wavelet scattering as an initialization for ConvNets and other models. I suspect the future of ‚Äòdata constrained learning‚Äô will be in synergizing predefined filters alongside learned filters.

Figure below illustrates stability under deformations. Left we applied scatter transform to the voice of a speaker saying ‚Äòzero‚Äô. The scatter representation consists of the coefficients derived from averaging/low pass filter, order 1 wavelets, and order 2 wavelets. Right After applying a displacement field that has mostly masked the structure of the original signal with a sine wave, Œ¶( xÃÉ ) is barely affected; the deformation‚Äôs effect has been linearized by Œ¶‚Äôs transformation.
Time Series must be handled with care by data scientists. This kind of data contains intrinsic information about temporal dependency. it‚Äôs our work to extract these golden resources, where it is possible and useful, in order to help our model to perform the best.

With Time Series I see confusion when we face a problem of dimensionality reduction or clustering. We are used to think about these tasks in more classical domains, while they remain a tab√π when we deal with Time Series.

In this post, I try to clarify these topics developing an interesting solution where I work with multidimensional Series coming from different individuals. Our purpose is to cluster them in an unsupervised way making use of deep learning, being wary of correlations, and pointing a useful technique that every data scientist must know!

THE DATASET

I got the data from UCI Machine Learning repository; I selected the Public Dataset of Accelerometer Data for Human Motion Primitives Detection. These data are a public collection of labeled accelerometer data recordings to be used for the creation and validation of acceleration models of human motion primitives.

Different types of activities are tracked, i.e. drinking, eating, climbing and so on. For a particular activity of a specific individual measured, we have 3 different sensor series at disposal: X-axis (pointing toward the hand), Y-axis (pointing toward the left), Z-axis (perpendicular to the plane of the hand).

I figure myself in this situation because it allows to carry out our initial problems of clustering (multiple individuals) and dimensionality reduction (multiple series for every individual) all in one single case.

Below I plot 2 examples of data at our disposal coming from a male and female individuals. In total, we have 20 individuals with the same measurement length.

DIMENSIONALITY REDUCTION
The Little Robot that Lived at the Library

How we built an emotive social robot to guide library customers to books Minja Axelsson ¬∑ Follow Published in Towards Data Science ¬∑ 9 min read ¬∑ Jul 25, 2019 -- 1 Listen Share

The Oodi library

Our team at Futurice designed and built a social robot to guide people to books at Helsinki‚Äôs new central library, Oodi. Opened in 2018, Oodi is the biggest of Helsinki‚Äôs 37 public libraries. It has 10,000 visitors a day, and an estimated 2 million visitors a year (compared to Finland‚Äôs 5.5 million population, that is a significant portion).

Automatic returns system

The MiR200 wagon moving books and their boxes

Oodi is big on automation and robotics. It has an automatic returns system: customers set their books on a conveyor belt, which brings the books to the basement, where they get sorted into boxes, which are picked up by a mobile MiR200 robot, which brings the books to the 3rd floor. At the 3rd floor, librarians place the books back on the shelves.

At the start of our project, we brainstormed how Oodi could use social robots: helping kids learn to read, instructing people on using equipment such as 3D printers, giving information about the library in several languages, and helping people find their way at the library.

We eventually settled on a robot that would help customers find the books and book categories they want. Since Oodi is so big, customers have a hard time getting around, and library employees spend a significant amount of time advising people how to find things. But this is not the work librarians are meant to be doing, or want to be doing. Librarians are very knowledgeable about literature. Their expertise is better used in in-depth service, helping visitors find specific books that fit their needs best. This type of work can take 30‚Äì40 minutes. In comparison, ‚ÄúWhere is the psychology section?‚Äù takes 1‚Äì3 minutes to answer. Stacked together, a whole day of 1‚Äì3 minute tasks becomes tedious, and a waste of skills.

This is where the robot steps (or rather, rolls) in. A whole day of menial tasks would not bother a robot. We realized we could re-purpose the MiR200 mobile robots that the library already had, and was using to move books between the basement and the 3rd floor.

The robot design team: Oodi librarians, Oodi‚Äôs customers, and Futurice‚Äôs roboticists

The robot would have the advantage of being able to access Oodi‚Äôs database directly, and provide real-time information on which books are currently on the shelf. The robot could be more approachable to people who have social anxiety, and are afraid to approach library employees. Additionally, it could save both the customers‚Äô time (no need to queue for a librarian), and the librarians‚Äô time (who can help customers with more meaningful tasks).

First draft

A Mobile Robot with (the Illusion of) a Personality

The design team, consisting of Oodi‚Äôs librarians, Oodi‚Äôs customers, and Futurice‚Äôs roboticists, defined design guidelines for the robot that would be built on top of the MiR200 robot, using these social robot co-design canvases (available as open source):

The robot is sincerely a machine ‚Äî it beeps expressively, and doesn‚Äôt talk

The robot has a touch-screen UI, and users don‚Äôt talk to the robot

The robot uses lights, sounds, and movement to communicate

The use of the robot should not depend on how familiar the user is with technology

The design needs to account for accessibility, the level of background noise, the library‚Äôs changing layout and furniture, and dodging customers

The design team decided that the robot should not be too humanoid. We wanted a more abstract form for the robot, with expressive, non-speaking forms of communication. We wanted a design with a bit of imagination and whimsy.

The team also wanted to make sure that the robot aligned with Oodi‚Äôs strategy and policies. The following ethical considerations were underlined:

GDPR (the EU‚Äôs data regulation) needs to be followed. Data about the person who looks for the book should not be combined with data about which book they were looking for.

Accessibility is important. The library‚Äôs principle is that everyone is served equally. Physical limitations, different languages, and impaired vision need to be taken into account.

The customer should be able to choose to be served by a human librarian.

If the robot is not working, it may cause frustration and rude behaviour by customers. This should be prepared for, so that librarians are not negatively affected.

We started testing the robot at the library, mapping out appropriate routes, and building the user journey. Luckily, we had some very excited testers.
Generating the dataset

We generate an artificial dataset. The first thing that came to my mind is an order registry, in which we store:

id of the client

of the client name of the product

of the product date of purchase

of purchase amount of product purchased

of product purchased the unit price of a certain product

As this is only a toy example, we do not dive deeply into the logic behind the dataset. We can agree that it vaguely resembles a real-life scenario. For testing the performance of different approaches, we generate 10 million rows of data.
Made this in Microsoft Paint

So, for my first write-up, I am tackling a problem I encountered whilst working on my first data science project at my company. I work as a machine learning researcher at ALC- Innovative Transportation Solutions and my problem arose during a predictive modeling project. Multi-class imbalance. I had encountered class imbalance before in classroom projects and had employed the use of the ROSE package but never had I been exposed to a multi-class imbalance issue.

Google Images: Binary class imbalance. I‚Äôve dealt with binary class imbalance before and there are plenty of tools and articles about tackling this common data issue.

Binary class-imbalance is a common headache in data science but can be easily solved(great article about it: https://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/). Unfortunately for this project I had on my hands, there weren‚Äôt as many resources. I wanted to share my thought process for overcoming multi-class imbalance so someone else might find it useful in mediating their own distribution conflicts.

The Distribution

My company‚Äôs data is not open for sharing but here is a simulated version of the issue I was facing:

This data distribution across classes is completely imbalanced. This is bad because the goal of my project is to build a multi-class classifier that can correctly assign what ‚ÄòClass‚Äô a data point belongs to. Specifically, this imbalance is an issue because the predictive model, whichever I end up pursuing, would be biased towards Class1 and, to less of a degree but still, Class2. It would achieve decent accuracy by classifying the majority of the train and test set as Class1 or Class2 because of the imbalance; this is the ‚ÄòAccuracy Paradox.‚Äô My model might achieve good accuracy on classification but that‚Äôs because the model would only be modelling the imbalanced distribution.

Google Images: Confusion Matrix. If a ML model trains on an imbalanced data set it will over-classify towards the majority class. In the image, for reference, the model would predict all classes to be ‚ÄòP‚Äô while some of those should have been ‚ÄòN.‚Äô In the case of multi-class imbalance the effects would be even more drastic where the model would predict ‚ÄòP‚Äô (because it‚Äôs the majority class in this example) when the actual class was ‚ÄôN‚Äô or ‚ÄòO‚Äô or ‚ÄòM‚Äô etc.

This issue is compounded by the fact that the distinguishing characteristics between the classes is quite thin, in the case of my actual work project. Their longitude/latitude features are only different by a minuscule amount. Sure, there are other predictive features but Geo-spatial data makes up the bulk of the predictive and interpret-ability aspects of the model. Yet, this difference in classes is important to the company and the decision making of the model and therefore this issue must be tackled.

Google Images: Imbalanced Classifier. This image is an example of how an imbalanced data set would create an ‚Äòaccurate‚Äô classifier that, in production, would really be a weak classification model hiding behind the guise of ‚ÄòHigh Accuracy.‚Äô

ROSE

ROSE, or the Random Over Sampling Experiment, is a fantastic R package that deals with class imbalance quite well, but only binary class imbalance (two classes). How could I use this package to fix the multi-class data I was looking at?

After trial-and-error, researching of options, and dropping a quick email to one of my great professors I determined the best course of action: writing a function that takes the whole data set, divides it up into 6 subsets using ‚ÄòClass‚Äô as the dividing feature, and then use ROSE to balance those subsets out to my desired distribution. Then they would be compiled back into one, reasonably balanced data set. Each subset contains Class1 and then one of the minority classes. Class2 was left out because it‚Äôs not under-represented.

The subset is then given to a ROSE argument to over-sample the minority class. The ROSE code for doing so can be seen below. The identity of the minority class is used as the formula and the ‚Äòp‚Äô argument is the probability of sampling the rare class, an alternative is setting N to the desired size of the data set. Below, with p=0.5, these arguments return a data set where the minority class is now represented in 50% of the data.

library(ROSE) BalancedData <- ovun.sample(MinorityClassi~, ImbalancedData, method="over", p=0.5, subset=options("subset")$subset, na.action=options("na.action")$na.action, seed) index = createDataPartition(y=BalancedData$Class, p=0.7, list=FALSE)

train = BalancedData[index,]

test = BalancedData[-index,] BalTrain <- droplevels.data.frame(train)

BalTest <- droplevels.data.frame(test)

After over-sampling the minority class and bringing the distribution out of imbalanced hell I ran this balanced data set through my classifier. The best model was a Random Forest (compared against a logistic regression model as the baseline, a gini-criteria decision tree, an information-gain decision tree, and a neural network). The results were great, but not so great on the test set. Why? Because the over-sampled minority class data weren‚Äôt new data points. So the model couldn‚Äôt conceive that new points could fit into that class.

I considered using SMOTE, synthetic-minority over-sampling technique but it‚Äôs results were negligible at best. I settled on using the ROSE formula in the ROSE package which: ‚Äúcreates a sample of synthetic data by enlarging the features space of minority and majority class examples‚Äù (cran.r-project.org/web/packages/ROSE/ROSE.pdf).

SynthBalData <- ROSE(MinorityClassi~, ImbalancedData, p=0.5, hmult.majo=1, hmult.mino=1, subset=options("subset")$subset, na.action=options("na.action")$na.action, seed) index = createDataPartition(y=SynthBalData$Class, p=0.7, list=FALSE)

SynthBalTrain = SynthBalData[index,]

SynthBalTest = SynthBalData[-index,] SynthBalTrain <- droplevels.data.frame(SynthBalTrain)

SynthBalTest <- droplevels.data.frame(SynthBalTest)

By sticking to my original method of subset creation and applying ROSE I got back synthetic yet balanced data samples and compiled a new data set. I trained all the models on a simple random sample 70:30 train/test split of the data. The accuracy was high, and the models responded very well to new data points as they come in from my company‚Äôs server.
This article will go through the theory to demystify this insufficiently known part of NLP. Then, in a second article, we will suggest tools to help you understand how to easily implement a Dependency Parser.

When we think about a word‚Äôs neighbors, we could think about the neighborhood as their location in a sentence, their relation to other words (subject, determinant, etc.), called syntax, or as their meaning similarity, called semantics. What interests us here is the syntactical neighborhood.

Vocabulary

First, let‚Äôs define some vocabulary to make it clearer for everyone.

Semantics is the linguistic and philosophical field that studies meaning and interpretation. It relies a lot on links between words to understand the sentence, and it analyzes the changes in meaning. In programming, semantics is the expected output of a program.

is the linguistic and philosophical field that studies meaning and interpretation. It relies a lot on links between words to understand the sentence, and it analyzes the changes in meaning. In programming, semantics is the expected output of a program. Syntax is the linguistic field of grammar. It is the study of the rules for word patterns in sentences. Known in programming too, errors in syntax often lead to bugs, because rules are often much stricter than in oral language.

What is a Dependency Parser ?

A Dependency Tree is a structure that can be defined as a directed graph, with |V| nodes (vertices), corresponding to the words, and |A| Arcs, corresponding to the syntactic dependencies between them. We may also want to attribute labels to dependencies, called relations. These relations give details about the dependency type (e.g. Subject, Direct Object Complement, Determinant‚Ä¶). You can find all the relations from Universal Dependencies by following this link : https://universaldependencies.org/u/dep/index.html.

Example of Dependency Tree : ‚ÄúWhat is a parser ?‚Äù

In an arc h ‚Üí d, h is the head and d is the dependent. The head is the most important node in a phrase, while the Root is the most important node in the whole sentence: it is directly or indirectly the head of every other node.

A Dependency Parser simply transforms a sentence into a Dependency Tree.

Metrics : how to recognize a good parser ?

An accurate Dependency Parser recognizes the dependencies and relations between words well. Two Metrics (scores) are useful for this:

- Unlabeled Attachment Score (UAS), which corresponds to the number of correctly predicted dependencies over the number of possibilities;

- Labeled Attachment Score (LAS), which corresponds to the number of correctly predicted dependencies and relations over the number of possibilities.

LAS is always less than or equal to UAS, because an incorrect dependency leads to a suboptimal UAS and LAS, while an incorrect relation (or label) only leads to a LAS decreasing.

Algorithm : How does it work ?

As you might have thought, we could create a Dependency Parser through rules developed by linguists. These parsers are called Rationalists. They are not at all efficient, since languages are very complex, and they change over time. Any small change in the language would lead to tremendous changes in the parser. Machine Learning allows for the development of Empiric parsers, which are data driven. Fed by many sentences, probabilities of dependencies or relations can be drawn. Linguistic knowledge may be used, but does not have the last word, which is a good point if you, like me, have forgotten your primary school lessons‚Ä¶

Several steps are needed to create a Dependency Parser. Our inputs are the words of the sentence with their properties (index, Part of Speech tag, Lemma, Features); then, we must calculate features for all possible arcs in the sentence. Thanks to these features, we compute a score for each possibility, and we finally decode scores with a decoder.

Features and Score

Each word in the sentence has some attributes, like Part of Speech tags or Lemmas. You might know them if you have already read about NLP. You can check it out here, if not:

With these features, we train a Machine Learning regression model that returns the score to be exploited by the decoder.

Feature selection is crucial, and some models allow us to bypass this part via a deep learning part. This is the case with the algorithm we will present in the following section.

Decoders

There are a lot of different decoders already developed. However, we can divide them into two categories: Transition-based decoders and Graph-based ones. Transition-based decoders are faster and need less memory to decode scores, but they are generally less accurate than Graph-based decoders. I will only go through Graph-based model principles in this article.

Other algorithms can apply different transitions, but this one allows us to understand the main principle.

Graph-Based Decoders

It is necessary to deal with graph theory to understand these algorithms.

A graph G=(V, A) is a set of vertices V (called also nodes), that represent the tokens, and arcs (i, j)‚àà A where i, j ‚àà V. The arcs represent the dependencies between two words.

In a Graph-based dependency parser, graphs are directed, which means links have different directions, and there can be multiple arcs between nodes, this is called a multi-digraph.

Weighted Multi Directed Graph (G)

You can note that some arrows are thicker than others. This represents the weight of arcs. The more an arc weighs, the stronger the link between two nodes. We could interpret this as the strength of the syntactic dependency for our parser. For Example, C and A seem to be very dependent on B, but B does not seem very dependent on C and A.

Graph G is too connected. In order to get a Dependency Tree, we want:

To link each word only with its dependents ‚Äî not with all the words. The total number of arcs should be equal to the number of nodes minus 1 (|A| = |V|-1).

To keep the same nodes (or tokens or words).

To make it acyclic: we do not want a head to be dependent on one of its dependents (direct or indirect).

Fortunately, all of this already has a name: what we want is a Spanning Tree!

Example of Spanning Tree from the graph G

An other example of Spanning Tree

If I was clear on what a Spanning Tree is, you should know that there are multiple possibilities, since we only have a few conditions to get one. Here comes a trick: we want the best one, certainly, but how could we determine the ‚Äúbest‚Äù one?

We have 3 nodes here, and we want to keep them. However, we have 6 arcs and we want to keep only 2. The ‚Äúbest‚Äù Dependency Tree is the one that has the highest weights: this is called the Maximum Spanning Tree (MST).

Maximum Spanning Tree of G

Minimum Spanning Tree of G

This Maximum Spanning Tree gives us our Dependency Tree, which we will use to find the closest syntactic neighbors in the sentence.

Conclusion

The insight given here is very light compared to the different existing algorithms. However, this should improve your intuition when developing your Dependency Parser.
Member-only story A gentle introduction to Recommendation Systems

Introduction to Recommendation Systems

If you are here, reading about Recommendation Systems, surely you already know what we‚Äôll be talking about, so maybe you can just jump over this brief chapter. But if you came here attracted by the cover image, or if you want to know more about how Recommendation Systems emerged and grew up in the last years, then stay tunned with this section of the article.

Let‚Äôs go back in time and try to picture this: it‚Äôs Friday night and you want to rent a videotape in your closest Blockbuster. You head over there with your girlfriend, already talking about which movie are you going to rent. Maybe a comedy? Or perhaps an action movie? A thunderstorm is coming, what settles the perfect mood for a horror movie or a thriller.

While you‚Äôre thinking, your girlfriend tells you: ‚Äòdid you search for any good recommendation on the Internet?‚Äô. Unfortunately, the Internet was down all day at your job, and anyway, you‚Äôre still trying to get yourself around with all that mumbo jumbo around the World Wide Web.

You finally arrive there, but after almost an hour of deliberations, you still don‚Äôt know what to chose. You already saw all the popular movies, and the premier section of the store is full of crap. You don‚Äôt want to be nitpicking, but there‚Äôs just nothing good. Your girlfriend asks the Blockbuster mate behind the desk, but he seems to know less about movies than your grandpa, and he is really not down with the kids. It‚Äôs already getting late and you‚Äôre knackered, so long story short, you decide to rent a good old movie you already saw. Is better to keep it sound and safe, than winging about your Friday‚Äôs night plan.

This example may seem old, but in so many aspects of life, people keep making decisions in a similar way. I can assure you that my grandpa doesn‚Äôt use Google when he wants to buy a book for himself. However, and luckily for the youngsters, the decision making today in‚Ä¶
Sentiment Analysis: a practical benchmark

With hands-on practical Python code, we demonstrate limitations of simple recurrent neural networks and show how embeddings improve fully connected neural networks and convolutional neural networks for the classification of sentiment.

We show how to work with sequence data, by doing sentiment classification on a movie review dataset. Sentiments are basically feelings which include emotions, attitude and opinions written in natural language.

IMDB movie reviews dataset

We start by loading the IMDB dataset using Keras API. The reviews are already tokenized. We want to have a finite vocabulary to make sure that our word matrices are not arbitrarily small. We also want to have a finite length of reviews and not have to process really long sentences. Our training dataset has 25,000 customer reviews, together with their correct labels (either positive or negative).
The Complete Guide to Decision Trees

A complete introduction to decision trees, how to use them for regression and classification, and how to implement the algorithm in a project setting Marco Peixeiro ¬∑ Follow Published in Towards Data Science ¬∑ 9 min read ¬∑ Jul 25, 2019 -- 2 Share

They are‚Ä¶ don‚Äôt even try something else

Tree-based methods can be used for regression or classification. They involve segmenting the prediction space into a number of simple regions. The set of splitting rules can be summarized in a tree, hence the name decision tree methods.

A single decision tree is often not as performant as linear regression, logistic regression, LDA, etc. However, by introducing bagging, random forests, and boosting, it can result in dramatic improvements in prediction accuracy at the expense of some loss in interpretation.

In this post, we introduce everything you need to know about decision trees, bagging, random forests, and boosting. It will be a long read, but it will be worth it!
I love how simple and clear Keras makes it to build neural networks. It‚Äôs not hard to connect Keras to Tensorboard but that has always felt to me like a heavyweight solution is overly complicated for many of Keras‚Äôs users who often want to take a quick look at the underlying model.

With wandb, you can visualize your network‚Äôs performance and architecture with a single extra line of python code.

To show how this works, I modified a few scripts in the Keras examples directory.

To install wandb, just run ‚Äúpip install wandb‚Äù and all of my Keras examples should work for you.

1. Simple CNN‚Äç

I started with the requisite mnist_cnn.py.

I added the ‚Äúfrom wandb import magic‚Äù line below ‚Äî you can also look at my mnist_cnn.py forked from the Keras examples with the one line change.

Now when the model runs, wandb starts a process in the background saving relevant metrics and streaming them to wandb.com. You can go to https://app.wandb.ai/l2k2/keras-examples/runs/ovptynun/model and look at the output of my run.

‚Äç

I can see exactly the data that my model is labeling and view the loss and accuracy curves automatically.

2. Resnet on Cifar‚Äç

Next, I forked cifar10_resnet.py and made the same one line change. You can see a nice visualization of a resnet at https://app.wandb.ai/l2k2/keras-examples/runs/ieqy2e9h/model.

On the system page, I can see that this model is using a little more of my single GPU than the mnist example.

3. Siamese network‚Äç

Next I tried the siamese network example. Here I might want to look at the TensorFlow graph, luckily with our one line of code we automatically instrument and host TensorBoard. You can find this run at https://app.wandb.ai/l2k2/keras-examples/runs/fsc63n6a?workspace=user-l2k2.

This instrumentation took me under a minute per model, adds very little compute overhead, and should work for any Keras model you are working on. As you want to track more things you may want to replace the one line with:

import wandb wandb.init(magic=True)

Then you can use our custom wandb.log() function to save anything you want. You can learn more in our documentation.

I really hope you find this useful!
SQL Subqueries

Photo by Henri L. on Unsplash

The code I used for this blog can be found on my GitHub.

Every time I learn something new about SQL I start to find that there are so many applications that I am unaware of. After learning the basic syntax of queries and a few basic tools, I moved onto subqueries in SQL.

Subqueries (aka inner queries or nested queries) are useful tools when you‚Äôre performing multiple steps. It‚Äôs feels Inception-like, since you‚Äôre querying into queries.

Subqueries can be used in several areas within the query, so today we‚Äôll cover using them in the most common areas: SELECT , FROM , and WHERE clauses.

Setting Up

For these examples, we‚Äôll use the Chinook database which is also used in the SQL Basics blog.

Let‚Äôs set up the database file, libraries, and functions:
Street Lanes Finder - Detecting Street Lanes for Self-Driving Cars üöó

Lanes Detection with Computer Vision Greg Surma ¬∑ Follow 5 min read ¬∑ Jul 25, 2019 -- 1 Share

In today‚Äôs article, we are going to use basic Computer Vision techniques to approach the street lanes detection problem which is crucial for self-driving cars. By the end of this article, you will be able to perform real-time lane detection with Python and OpenCV.

Real-Time Lane Detection

Implementation

You can find the full codebase for this project on GitHub and I encourage you to check it and follow along.

Let‚Äôs start with defining our problem.

Given an image of the road, we would like to detect street lanes on it.

In order to do it, let‚Äôs provide an image path and load it with OpenCV, then let‚Äôs invoke find_street_lanes pipeline with it.

test_image = cv2.imread(INPUT_FOLDER + TEST_IMAGE)

street_lanes = find_street_lanes(test_image)

And this is how our find_street_lanes pipeline looks like
This is a continuation of my earlier post on compositional data analyses where I showed the pitfalls of treating compositional data as absolute data instead of relative data. In this post, I will summarize the techniques we can use to correctly analyze compositional data with specific examples demonstrated using RNA-Seq data.

Two main strategies exist for treating Compositional Data and specifically NGS data:

1. Normalization to get back the absolute counts

2. Compositional Data Analysis (CoDA) methods that transform the data using within sample references (Ex: ALR, CLR)

Normalization to Absolute Counts

This is the most widely used technique in NGS data pre-processing when comparing across samples is desired. The relative read counts are ‚Äònormalized‚Äô to the total read depth to ‚Äòrecover‚Äô the absolute counts. This, however, does not recover the absolute counts when the total absolute amounts of RNA or cells or the amount of relevant biological material significantly changes across samples. This more often leads to a false sense of security for the analyst and leads to treating these ‚Äònormalized‚Äô samples as absolute counts. This can result in erroneous conclusions when comparing across samples. Let‚Äôs prove that to ourselves using simulated data.

Simulation Details

Here, I simulated data for 100 genes, where

a. 5 genes have the true log fold change of 1 between control and experimental conditions (approximates tolerance or no growth under selection),

b. 2 genes have the same true log fold change of > 1 in the experimental conditions (resistant and exhibit growth under selection), and

c. 2 genes have the same true log fold change of < 1 in the experimental conditions (not resistant or tolerant),

I simulated 5 different cases where different proportions of the remaining 91 genes are changed. Of the genes that change, ~90% are depleted, and ~10% are enriched in each case.

The depletion/enrichment of the other genes affects the relative count values and the read-depth normalized counts even though the total read depth is fixed at 200K reads

Read Depth Normalized (RDN) Counts

Differential Expression or Abundance: Even though all the reads have the same total depth (sum of counts), the log fold changes (LFCs) of genes calculated using the read depth normalized counts (RDN counts) are shifted compared to the true log fold changes (See Fig 1 below). Interestingly, the direction of the shift is not always predictable based on the fraction of genes changed. For example, when ~70% of the genes are changed, the LFCs calculated using the RDN counts are shifted down compared to the true LFCs. On the other hand, the LFCs calculated using the RDN counts are shifted up compared to the true LFCs when 90% of the genes are changed. This is because the absolute true counts in the former case are higher than the latter case. In general, we cannot anticipate or estimate the true total absolute counts for a sample.

Fig 1: Comparing True Log-Fold Changes to Log-Fold Changes Calculated using RDN Counts

2. Correlation Between Genes: To see how things compare between relative counts and absolute counts., I calculated the correlation for the non-constant genes across all the 5 samples (each with either 0.1, 0.2, 0.4, 0.6, 0.9 fractions of changed genes). I used both the true counts and relative counts using Polyester simulated count data at 200K read depth.

Fig 2: Comparing True Correlations Between Genes to Correlations Calculated using RDN Counts

As we can see from the figure above, some of the correlation coefficients calculated using the RDN counts, are significantly different from the true correlation coefficients, with a negative bias.

The 2 examples above show the pitfalls of using RDN counts to estimate the differential expression or correlation between genes. Instead of using RDN counts, one should always use spike-in controls when trying to recover absolute counts from relative compositional data. We will show that next

Spike-in Normalized Counts

To truly correct for the change in the absolute counts, we need spike-in controls or genes that we add into all our samples at the same abundance (amount) just before the sequencing step. Doing this will normalize all the samples to the same total abundance scale and makes the comparisons correct. This only works when the data are closed due to sequencing (because we are adding the spike-ins just before sequencing), and will not help if the constraint is biological or happens upstream of the sequencing step. In that case, we need to add in the spike-ins before this constraining step, but it is not always possible to do so due to physical and biological limitations of adding the spike-ins.

Let‚Äôs see how this works using our data. In our data, we have 92 different controls or spiked-in genes that have the true absolute abundance. Let‚Äôs use these to ‚Äònormalize‚Äô the data and therefore bring all samples to the same absolute count scale.

Differential Expression or Abundance: Fig 3 below is analogous to Fig 1 but with spike-in normalized counts instead of RDN counts. The plot has artificial jitter (noise) added to show all data, but the true data all lie along the diagonal. This indicates the power of spike-ins. Properly designed spike-ins can recover the absolute counts (up to a constant multiplicative factor), provided the spike-ins are added just before the step that leads to closure or constraints in the data, which is not always possible.

Fig 3: Comparing True Log-Fold Changes to Log-Fold Changes Calculated using Spike-in Normalized Counts

2. Correlation Between Genes: Looking at correlations between genes, we see that the coefficients calculated using the spike-in normalized counts can recover the true coefficients. Fig 4 below:

Fig 4: Comparing True Correlations Between Genes to Correlations Calculated using Spike-in Normalized Counts

So, it seems like we found the solution to our problem. All we have to do is add some controls and we are good! Not so fast, unfortunately. In this simulated case, the source of closure for the compositional data is sequencing and we were able to add some controls right before we simulated the sequencing data. In real-life data generation process, the sources of closure can occur anywhere in the usually complicated workflow of extracting DNA/RNA. Also, the biological system itself could be inherently compositional (For example the capacity for a cell to produce RNA is limited), in which case no spike-ins introduced outside the cell can recover the true absolute counts.

Compositional Data Analysis (CoDA) Methods

An alternative to spike-in normalization is using CoDA methods that typically transform the count data with respect to an in-sample reference. Additive Log-Transformation (ALR) and Centered Log-Transformation (CLR) are examples of some commonly used CoDA transformations. These methods are first proposed by John Aitchison originally in 1986. The core idea being that the log-ratio transformations of the components relative to another reference component can be treated as any other unconstrained data. This transforms the data from the original simplex space (as in our ternary diagram in the first part) to the Euclidean space. This allows us to use all classical analyses techniques on these data.

A cautionary note: These techniques do not claim to open the data as do the ‚Äònormalization‚Äô methods from the previous section. These techniques are also applicable to all data, whether they are relative or absolute. Another point to note is normalizing using spike-ins is the same as using the Additive Log-Ratio (ALR) transformation. The benefit of using the general ALR transformation is that it is applicable even when we do not have spike-ins that have constant abundance across samples. The disadvantage with the general ALR transformation is we need to choose the reference properly to make sense of the data and answer the relevant questions.

Lets now look at the CoDA methods in more detail using the same data set that we used as before.

1.Differential Expression or Abundance: There are many methods to find changes in compositional data before and after treatment. Many of these methods surprisingly come from the Microbiome literature, whereas the gene expression literature mostly relies on traditional methods like DESeq2 and EdgeR, which do not explicitly take into account the compositional nature of the data. DESeq2 and EdgeR implicitly assume that the absolute abundances do not change due to the treatment. This is equivalent to using the Centered Log-Ratio (CLR) transformation from the CoDA methods. This transformation uses the geometric mean of the genes or components as the reference, and therefore all results have to be interpreted with respect to the geometric mean. At this stage, it is tempting to translate this assumption to mean that the geometric mean of the genes does not change between control and treatment. Maybe the geometric mean changes, maybe it does not, there is no way to know for sure without orthogonal information beyond the relative counts from sequencing. Most users of DESeq2 and other Differential Expression tools fall for this trap and conclude any significant changes called by the algorithms to mean significant changes in the absolute counts. Instead, these are just significant changes with respect to the geometric mean of all components.

There are emerging methods to apply statistical rigor to DA in compositional data. The most popular methods are ALDEx2 and ANCOM. The main philosophy of these methods is to rely on log-ratio tests of transformed relative data with respect to a reference component and to carefully interpret these results. The main issue with these methods is that the results can only be interpreted with respect to the chosen reference, and no guidance is provided on how to choose the reference. Giuliano Cruz pointed me to a more recent methodology that uses Differential Ranking (DR) and lays out a more reasoned approach to choosing a reference. This is what I will use here briefly, and hopefully, in some future post go into the gory details of running some of these algorithms.

The main idea of DR is to choose some random reference component to calculate the log ratios for all components in both treatment and control. In the next step, these components are ranked in the order of their the difference Œî(log-ratio) between treatment and control conditions. This rank-order of Œî(log-ratio) values calculated using the known relative counts should be identical to the rank of the Œî(log-ratio) values calculated using the unknown true absolute counts. For example, I show below the Œî(log-ratio) values calculated using the relative counts vs. Œî(log-ratio) values calculated using absolute counts, for the case where 90% of the genes are differentially expressed:

Fig 5: Œî(log-ratio) values Calculated using Absolute vs. Relative Counts

As you can see, the magnitude of the Œî(log-ratio) values are different depending on whether we use the relative or absolute counts, but the rankings of Œî(log-ratio) values stay the same. This does not mean that the top-ranking genes have higher counts in treatment vs control, and the low-ranking genes have lower counts. It could so happen, that the top-ranking genes have depleted absolute counts in the treatment conditions compared to the control condition, but the lower-ranked genes have even worse depletion in the treatment condition. In short, we cannot say anything about the changes in absolute reads between treatment and condition.

I will now choose the top-ranking gene as my reference and again calculate the Œî(log-ratio) values using this new reference.

Fig 6: Œî(log-ratio) Values Calculated using the Top-Ranking Gene as Reference

From this plot, we can use an arbitrary cut-off of 0.5 and choose any genes beyond this as our potential DA genes to test further. Of course, if we want more genes to test, we can relax the cut-off.

Another recommendation to get around choosing reference is to have some sort of positive or negative controls in the population. Suppose, we know a gene that will increase in absolute abundance in the treatment condition, then we can use this gene as the natural reference for calculating log-ratios and rank-order the Œî(log-ratio) values. Any log-ratio greater than 1 implies that the gene is better than the positive control, and log-ratio less than 1 implies worse than the positive control. Even better, is to have 2 controls to bound the effect size, and interpret the log-ratios with reference to both of these genes.

In my simulation, I only have one sample per replicate, and therefore could not do any statistical analyses. In a future post, I will generate multiple replicates per condition and play with ALDEx2, ANCOM, and DR algorithms to test their sensitivity and specificity.

2. Correlation Between Genes: As shown in part 1 of this series, correlation is not sub-compositionally coherent and therefore does not follow one of the principles of CoDA. Briefly, the correlation coefficient between any two genes depends on the other components or genes present in the data. Aitchison first proposed using the variance of log-ratio transformed values (VLR) to estimate the dependency of 2 variables. For example, to calculate the dependency between features or genes, g, and h, across n samples, we would use:

VLR is sub-compositionally coherent and therefore doesn‚Äôt lead to spurious correlations. The main issue with using VLR is that even though it equates to 0 when genes g and h are perfectly correlated, it doesn‚Äôt have an upper limit when the genes are perfectly independent. And that makes it difficult to compare VLR for one gene pair against VLR for another gene pair because of this scaling issue. Several methods/metrics are proposed based on VLR to estimate the dependencies between compositions, the most notable being SparCC, SPIEC-EASI, and proportionality. In this blog, I only review proportionality in some detail. All these methods attempt to use VLR to derive metrics that are analogous to correlation coefficients and therefore can be compared across different pairs of components.

Three proportionality based metrics are proposed in the R package propr based on work by Lovell et. al. and Quinn et. al. These metrics are calculated on log-transformed data. For definitions see the propr package. Ai below refers to the log-transformed values for a gene or component ‚Äòi‚Äô in the data. Ai could be absolute or relative counts and the definitions still apply.

phi (Œ¶) = var(Ai -Aj)/var(Ai) rho (‚ç¥) = var(Ai -Aj)/(var(Ai) + var(Aj)) phis (Œ¶s) = var(Ai -Aj)/var(Ai +Aj)

The closest metric to traditional correlation coefficient is rho which ranges from -1 to 1. phi is unbounded and can vary from 0 to Inf, and phis is a symmetric variant of phi and is a monotonic function of rho. I will focus on rho in the rest of the blog.

a. Using Absolute Counts: We can recover the absolute counts from relative counts if we have a spike-in control. Since we already have spike-in data available to us, I will calculate the rho values using the spike-in transformed data, i.e. A¬π = log(TPM_counts¬π/Spike_TPM_counts) for gene 1 using spike_TPM_counts as the normalization counts. This will recover the original absolute counts. Now, we can calculate the rho values using the equation above. I plot the correlation between absolute counts and the rho values below:

Fig 7: Correlations of True Absolute Counts Between Genes vs. Rho Values Calculated using Spike-Normalized Data

As can be seen from this plot, using proportionality we can capture most of the original correlations between the true absolute counts. Of course, this is a contrived example, where we have a good spike-in available to retrieve the absolute counts. Even with this contrived example, we still see some differences between the true correlations and the proportionality values calculated using the spike-in normalized counts. This is due to the way the proportionality based metrics are calculated which makes them extremely sensitive to the estimates of the variances of the log-transformed values. Here we only have 5 samples to calculate the variances and in most cases, the first 3 samples have the same values. This I suspect leads to the formulae to calculate the metrics to break down. Have to grok on this a little bit more. The evidence for this hypothesis is that, if we only look for components that have distinct values in at least 4 different samples, then the correlation values and the proportionality metrics match pretty well as can be seen below:

Fig 8: Correlations of True Absolute Counts Between Genes vs. Rho Values Calculated using Spike-Normalized Data: Only for Genes with At Least 4 Distinct Values out of 5 Samples

In general, rho and other proportionality based measures have good precision and poor recall, and having more samples gives better estimates for the variances and therefore for the rho values. Also, boot-strapping is generally used to establish a cut-off for calling relationships significant. For example, in the plot above, the cut-off for significant ‚Äòrho‚Äô values could be 0.75.

b. Using Relative Counts: The world is unfairly complex, and we don‚Äôt usually have a nice spike-in lying around for us to use, unfortunately üò¢ . So we have to instead use relative data, or more specifically, the additive log-transformed (ALR) relative data. Or we can use centered log-transformation (CLR) if we are confident that the geometric mean of the counts does not change across samples (which we know does not hold for our simulated data here). In essence, the best we can do in such cases is to calculate the relationships between relative data. So, let‚Äôs compare the rho values for relative data (with respect to a chosen reference gene) against the correlations between true absolute counts. The plots below show this for the correlation between relative counts calculated using 2 randomly chosen reference genes:
Linear regression is arguably one of the most important and most used models in data science. In this blog post, I will walk you through the process of creating a linear regression model and show you some cool data visualization tricks.

We will be using the Ames Housing dataset, which is an expanded version of the often cited Boston Housing dataset. The dataset has approximately 1,700 rows and 81 columns, and the goal is to predict the selling price in the market.

Let‚Äôs Begin!

Just like any other project, the first step is to import all the libraries. You can import more libraries as you figure out all the tools you will need to build your model.

Now, before we process or change the data, let‚Äôs just get an idea of what we are dealing with.

The short description of the data set shows us that it contains various data types and null values. It‚Äôs important to remember this as we process the data and build our model.

Data Preprocessing

Datasets in the real world are never perfect. There will always be missing values and outliers that skew the dataset, affecting the accuracy of our predictive model. That‚Äôs why it is always a good idea to clean up the data before you start building your model.

Earlier, I had mentioned that this particular dataset had 1,700 rows and 81 columns. If a value is missing in a particular column, it would be unwise to delete that whole row because we would be losing a datapoint in every other column as well. There are two ways to solve this issue:

Replace every null value in a given column with the median value of that particular column. (This is only applicable to columns with numerical values) Calculate your statistics while ignoring all null values. (I‚Äôll show you what methods to use later in the blog).

I opted for the second method, so I left the null values as they were in my dataset.

Calculating Outliers

There are multiple ways to calculate outliers‚Äî z-score, inter-quartile range (IQR), and the Tukey method are just a few methods out there. I chose to use the IQR. For all intents and purposes, I am assuming that you are familiar with the concepts of IQR, so I will only go over how to code it in Python. If you feel like you could use a short overview, this blog post does a pretty solid job of explaining the key ideas behind inter-quartile range.

In order to calculate the first and third quartile, I used the describe() function on the dataset.

summary = data_set.describe()

The describe() function yields a neat and concise data frame with important statistics from each numerical column of the original dataset. As you can see in the image above, I now have access to the mean, standard deviation, and percentile values with just one line of code!

Now, I store each statistic for every column in a data series. This allows me to access the percentile values in all the columns iteratively.

Now let‚Äôs dive into the deep end. I‚Äôll first explain at a high level how I calculated the IQR before I dump my code onto here.
300,000. That‚Äôs the number of people each year who are estimated to be infected with Lyme disease in the United States.

But even more concerning, only 10% of those cases get reported annually to the Center for Disease Control and Prevention (CDC). That means that there‚Äôs possibly 270,000 people who get infected with this disease every year and may not even know they have it.

Figure 1: Number of Reported Cases of Lyme Disease to the CDC from 2000‚Äì2017 (Data Source)

And the number of people affected is growing too. As seen in Figure 1, the annual cases of Lyme disease have more than doubled in the last 20 years. Mostly concentrated in the Northeast and upper Midwest, it continues to spread to new counties every year.

And it‚Äôs all as a result of the Black-legged tick, also known as the Deer tick. If you get bitten by an infected tick, the bacteria, Borrelia burgdorferialone, will enter your blood stream and begin its quiet destruction of your immune system.

Within about a week, 70 to 80 percent of people get the characteristic ‚Äúbull‚Äôs-eye‚Äù rash, known as Erythema migrans. And this is generally followed by typical flu-like symptoms, like a fever, headache, chills, and general fatigue.

This doesn‚Äôt seem like much, but if left untreated, it will cause widespread inflammation to almost every body system. Some of the symptoms include debilitating arthritis and swelling of the joints, facial paralysis, heart palpitations, shortness of breath, and swelling of the brain and spinal cord leading to severe nerve pain and memory loss.

But it doesn‚Äôt have to be this way. If diagnosed in the early stages, Lyme disease can be completely cured with certain antibiotics. It all comes down to increasing public awareness and educating healthcare providers in at-risk areas.

This is where machine learning comes in. My idea was to build a classification model that could predict which U.S. counties would have a high incidence of Lyme disease. In doing so, counties at high risk could be informed by the CDC in advance in order to proactively take measures against the infection.

Acquiring the Data

But here‚Äôs the issue. Funding for research of Lyme disease is disproportionately lower than other diseases. Thus, there is very little research and surveillance being done on Lyme disease at the moment and publicly available data is extremely limited. That meant that I would have to build my dataset completely from scratch.

Challenge accepted. Let the data wrangling commence.

If you want to follow along with my code, check out my GitHub repository; I‚Äôve organized it chronologically with this article for your convenience.

Target Variable

First, I needed to engineer my target variable. Luckily, the CDC has data on the number of reported cases per county from 2000 to 2017. Although the disease is severely under-reported as previously mentioned, this still gives a good picture of the geographic distribution of the disease.

For consistency, I decided to restrict the data to only 2010‚Äì2017 because, as shown in Figure 1, there is an unusually large spike in 2009. Also, it is likely that data from many years ago is not as representative of today, so removing data before 2010 would likely be beneficial for the predictability of the model.

But still, the data in this form won‚Äôt work for a classification problem. One issue is that the number of cases is heavily dependent on how many people live in each county. To solve this, I acquired county population estimate data from the Census Bureau. I then merged the datasets together and divided the number of cases per county by that county‚Äôs estimated population at the time.

This resulted in a rate of Lyme disease cases per person. Now we have a metric that allows us to more reasonably compare counties to one another.

There‚Äôs still one more step though. For supervised learning algorithms, the target variable must be labeled. I found out that the CDC defines a high incidence county as having 10 or more confirmed cases per 100,000 persons. This allowed me to bin the data such that anything above that cutoff was considered a high incidence county and anything below it was a low incidence county.

Features

Okay, but how do we get features for this model? I decided to dive into the research to find out.

In 1998, Stafford et al. showed that tick abundance had a strong positive correlation with Lyme disease infections. So you‚Äôd think that by now we would have a well-established national tick surveillance program.

Figure 2: CDC Map Showing Geographic Distribution of the Black Legged Tick (Source)

Well, no. As shown above, all the CDC offers today is a qualitative map showing where the Black-legged Tick might be. Not very helpful.

In the last few years, some states, like Connecticut and North Dakota, have taken the initiative themselves to measure tick concentrations , but until there is a coordinated national surveillance program, nation-wide tick population data will not exist.

Luckily, there is a good proxy for this. Khatchikian et al. showed that environmental factors such as extreme winter temperatures and summer and winter precipitation directly regulate tick population dynamics. This is likely a result of the climate‚Äôs effect on acorns. If the climate is optimal for acorn growth, mice and deer populations that eat them will thrive. The ticks that live off of these host animals will also flourish as a result.

This is great news, as there is a plethora of climate data available from the National Oceanic and Atmospheric Administration (NOAA).

I soon found out though that this would be a little harder that expected because there is a limit of 10,000 API requests per day. Given that I needed data for every weather station in every U.S. county for 7 years, it would have taken me about 56 days to download all the data. Not ideal.

I was able to get around this though. Fortunately, I found a page that allowed access to every Global Summary of the Year file and I downloaded all of it to my local computer. Of course, this meant that I now had data for every station in the world starting from around 1900, which amounted to approximately 78,000 CSV files, each corresponding to a particular station.

Obviously, I only wanted a small percentage of that. So, I had to write code to manually parse through each CSV file.

Figure 3: Example of NOAA Global Summary of the Year CSV Files

Above, you can see the general format of each file (there are many more columns that aren‚Äôt shown here). Essentially, I took the latitude and longitude from each file and used that as input for the reverse-geocoder python library, which outputs the country, state and county of that location. If it was a U.S. station, I then took only the rows from 2010 to 2017 and appended them to a global dataframe with their associated state and county as new columns.

After over 24 hours of parsing, I was left with a mess of data set. First of all, there were 209 columns with cryptic names, like ‚ÄúDX90‚Äù and ‚ÄúEMSD‚Äù. I used the provided documentation to determine the meaning of each feature and then through careful research, meticulously removed the ones that weren‚Äôt relevant. I also relabeled the columns to have more understandable names.

Figure 4: The Percentage of Missing Values for Each of the 35 Selected Features

The next problem was the large number of missing values as seen in Figure 4. For whatever reason, many of the stations were not consistent in recording and/or reporting their measurements.

I didn‚Äôt want to get rid of columns because then I would have lost some of the information that could have been gleaned from the climate data. So instead, I chose to impute all the missing values.

Now, this is an area where we have to be super careful. We need to put in a placeholder that would be reasonably representative of the actual value.

For example, let‚Äôs say that the total precipitation value for Montgomery County, Pennsylvania in 2017 is missing. It is reasonable to believe that the average total precipitation of all the counties in Pennsylvania will come close to Montgomery County‚Äôs actual precipitation. And so that‚Äôs what I did for all the missing values; I found the average for that county‚Äôs state in that particular column and imputed the value.

Hawaii and Washington D.C. had no data in several of the columns, so I had to remove them completely. Hawaii is geographically isolated and has no incidence of Lyme disease so there is no issue removing it. Washington D.C. is significantly smaller than any state so it should have no effect on the modeling.

Merging the Target with the Features

Alright, so now that the data was nice and clean, I was almost ready to merge the features with the target.

Since I was trying to build a predictive model, I would need to use the previous year‚Äôs climate data to predict the current year‚Äôs Lyme disease incidence.

To do this, I created a new column in the features data that contained the next year. I then merged the target data on that column in order to create a one year offset.

Preparation for Modeling

Class Imbalance

Before any modeling, it‚Äôs important to check if the data will work well for the algorithms you‚Äôll be using. In the case of classification models, the classes need to be relatively balanced. That means that there should be an equal number of instances for the positive and negative classes.

Figure 5: Class Imbalance in Data

As seen above, this data has a significant class imbalance problem with 85% of the counties having low Lyme disease incidence and only 15% of the counties having high Lyme disease incidence.

But why is this such an issue? Well, if a majority of the counties have a low Lyme disease incidence, then a model will get the best accuracy by guessing that all the counties have a low incidence. In this case, 85% of the time the model would be right. But this comes at a terrible cost of 0% accuracy for the minority class. In other words, our model would be utterly useless for the counties we care about that have a serious Lyme disease problem.

Resampling Techniques

I experimented with many resampling techniques to counteract this. Random undersampling is where you randomly select a small percentage of the majority class and delete the rest of the data points. This removal results in a balance of the majority class with the minority class. But this comes with the obvious downside of losing information, which could reduce the model‚Äôs predictability.

Then there‚Äôs random oversampling, which randomly selects a portion of the infrequent class and just duplicates those data points. This also results in balanced classes; the caveat here is that having a bunch of rows that are exactly the same could lead to overfitting, where the model just memorizes the over-represented synthetic data and loses its generalizability to real world unseen data.

That‚Äôs where Synthetic Minority Over-sampling Technique, also known as SMOTE, comes in. Instead of just copying parts of the minority class, it seeks to create data that are similar but not exactly the same as the original data. On a high level, it randomly generates new points directly between the infrequent class‚Äôs data points, leading to completely unique synthetic data.

And lastly, there‚Äôs the Adaptive Synthetic Sampling Approach or ADASYN, which is like SMOTE but creates more synthetic data for minority class samples that are harder to learn and fewer synthetic data for minority samples that are easier to learn. This results in a distribution of synthetic data that will strengthen the model‚Äôs ability to distinguish between the classes at the decision boundary.

Splitting the Data

Before we can see how these techniques perform, we must partition the data. I split the data into three parts: 60% for training the different models, 20% for validating and optimizing the best model, and 20% as a test set to demonstrate the generalizability of the final model.

Initial Modeling

Now we get to the fun part. I built a modeling pipeline using the imbalanced-learn python package in combination with scikit-learn‚Äôs GridSearchCV. This allowed me to do an exhaustive grid search for every combination of hyperparameters and data transformations (like scaling and resampling techniques), while also doing 5-fold cross validation to more robustly test how each combination performed.

The algorithms I ran through this pipeline were:

I‚Äôve provided links to helpful articles in case you are interested in learning more about each model.

Evaluating Each Model‚Äôs Performance

Figure 6: ROC Curves for Each Model

After optimizing each algorithm individually, I tested the resulting five models on the validation set. I then plotted them against each other using the Receiver Operating Characteristic Curve, also known as the ROC curve, in Figure 6. The area under the ROC curve (ROC AUC) was used to compare the models.

Simply put, this metric represents the cost versus benefit of the model at every threshold, quantifying how good the model is at distinguishing between the classes. The best models will have curves that are close to the upper left corner and take up the most area in the plot.

As you can see in Figure 6, Random Forest (in pink) is superior to the other models at basically every threshold, with Support Vector Machines (in orange) coming in second; the models had ROC AUC scores of 0.947 and 0.934 respectively. A score of 1 means it perfectly predicts the data, so these are very respectable results.

Interestingly enough, random oversampling produced the best results for both of these models, which demonstrates that sometimes simplicity can outperform even the most sophisticated methods.

Optimizing the Best Model

Figure 7: Distribution of the Best Random Forest Model‚Äôs Outputs

Above is the distribution of the predicted probabilities outputted by the best Random Forest model. These numbers represent how likely it is that a particular county will have a high incidence of Lyme disease.

The outputs are right skewed indicating that the model is predicting that most the counties have a low probability of having a high incidence. This is exactly what we want to see, especially since the classes are imbalanced.

Right now, the default threshold for the model is 0.5, meaning that any county with a probability above 0.5 will be classified as high incidence and any county below 0.5 will be classified as low incidence. But what if this threshold is not the most optimal?

That‚Äôs a major reason why I picked ROC AUC as my evaluation metric, as it is independent of threshold. This meant that I could then tune the threshold to optimize the model for the costs associated with making mistakes. I set aside an additional 20% of my data specifically for this purpose.

So now we need to determine these costs. This is arguably the most important step in the process as we are going to place the model in the context of the real world, and not just in the vacuum of mathematics. I had to approximate the costs myself but ideally the stakeholder you are working with would be able to give you this information.

As I mentioned earlier, the idea behind this model is to give the CDC a tool to determine which areas they need to target their efforts. If a county is classified as having high Lyme incidence, the CDC would take two specific actions:

Increase public awareness in order to prevent infections from happening in the first place Provide educational resources for healthcare providers to increase the number of early diagnoses.

False Positive Cost

Okay, but what if the model predicts that a county is going to have a high incidence of Lyme disease but it actually doesn‚Äôt? This is called a false positive. One implication of this is that people would needlessly limit their time spent outdoors and possibly even decide not to travel to these areas. This decreased outdoor recreation and tourism could hurt local economies.

To approximate this, I used the estimated total economic contributions that National Parks have on local communities, which is about $20.2 billion annually. Obviously, this doesn‚Äôt take into account state parks or any other services that would be affected, but it will work for now.

I then tried to find research that quantified how consumer sales decreased during an established epidemic and there wasn‚Äôt alot available. Chou et al. found that the SARS epidemic in Asia cost countries between .2% and 1.56% in consumer related industries. SARS is a very different disease but it‚Äôs pretty much all there is at the moment to go off of. I chose to use 1% to roughly approximate the economic cost.

Here is how I arrived at the cost per county of a false positive:

Total Cost of a False Positive:

(20.2 billion x .01) / $64,311 lost per county (20.2 billion x .01) / 3,141 counties

False Negative Cost

Now what about the cost of a false negative? That would be where our model predicts that a county won‚Äôt have a problem with Lyme disease when it really does.

This is obviously going to be way more expensive of a mistake. More people would needlessly get infected by Lyme disease due to lack of public awareness. And more people would suffer from chronic Lyme disease, as fewer doctors would be educated on early Lyme disease diagnosis.

Zhang et al. found that the average early Lyme disease patient costs $1,310 annually, while a chronic Lyme disease patient costs $16,199. That‚Äôs over 12 times more expensive. This was done in 2000, so I had to account for an inflation factor of 1.41.

Additionally, Aucott et al. approximated that 64% of new Lyme disease cases each year were early while 36% were chronic. As previously mentioned, there are about 300,000 of these new cases every year.

Also, to simplify my calculation, I made the assumption that the CDC‚Äôs intervention would be completely effective, resulting in all the new Lyme disease cases being caught early.

To find the cost of a false negative, I would just need to find the cost difference between when the CDC does not intervene and when it does intervene. Here were all my calculations:

Number of Annual Early and Chronic Cases:

.64 x 300,000 = 192,000 cases of early Lyme disease

.36 x 300,000 = 108,000 cases of chronic Lyme disease Inflation-Adjusted Cost of Early vs Chronic:

$1,310 x 1.49 = $1,952 per patient with early Lyme disease

$16,199 x 1.49 = $24,137 per patient with chronic Lyme disease Average Cost for High Incidence County Without CDC Intervention:

192,000 early cases / 3,141 counties = 61 early cases per county

61 early cases * $1,952 = $119,065 108,000 chronic cases / 3,141 counties = 34 chronic cases per county

34 chronic cases * $24,137 = $820,641 $119,065 + $820,641 = $939,706 Average Cost for High Incidence County With CDC Intervention:

300,000 early cases / 3,141 counties = 95 early cases per county

95 early cases * $1,952 = $185,430 ------------------------------ Total Cost of a False Negative:

$939,706 - $185,430 = $754,276 lost per county

Finding the Best Threshold

Finally, I calculated the ratio of the costs between the false negatives and positives:

Ratio of False Negative vs False Positive:

$754,276 / $64,311 = 11.73

According to this, false negatives are almost 12 times more costly than false positives. I then plugged this value into my code and found the optimal threshold was 0.17. This means that any county above 0.17 will be classified as high incidence and any below will be low incidence. Refer back to Figure 7 to visualize where that cutoff would be.

Final Results

To prove the generalizability of my final model, I trained the model on all 80% of the previously used data (the 60% training set and the 20% threshold optimizing set) and then tested with the completely unseen 20% test set from the beginning.

The results were even better than during training with a ROC AUC of 0.961.

Figure 8: Confusion Matrix of Final Model‚Äôs Outputs

Additionally, the model had a great recall of about 0.949, meaning that it correctly classified about 95% of the high incidence counties. As seen in Figure 8, it only had 28 false negatives out of the 554 high incidence counties.

This of course came at a cost to the precision which was 0.503. This means that only about 50% of the counties that the model predicted to be high incidence were actually high incidence. As seen in Figure 8, the false positives and true positives are roughly the same.

Because false negatives were so much more costly than false positives, these results make complete sense. It was necessary for the model to be imprecise in order for it to correctly classify as many of the high incidence counties as it did.
So, what can we learn from our models? Well, first of all, our average price models seem to show us that avocado prices reach their peak each year in the Autumn. In October 2017, conventional avocados peaked at $1.65 and our model forecasts that Autumn spikes will continue to occur through 2018 and 2019. However, the model predicts that conventional avocado prices will trend downwards over the following two years. Between 2015 and 2018, the average conventional avocado price was $1.09. Between 2018 and 2020, the forecasted average price is $1.01, a decrease of about 7 percent.

In stark contrast, the model forecasts that organic avocado prices will experience moderate growth, bolstered by less volatility. The forecasted average price between 2018 and 2020 is $1.84, compared to $1.55 between 2015 and 2018. The model certainly demonstrated a moderate upward trend for organic avocado prices.

However, the growth in average prices is also due in part to decreased volatility, with the standard deviation of prices down to $0.16 for the forecasted period from the $0.36 standard deviation experienced between 2015 and 2018.

Both conventional and organic avocados are forecasted to experience strong growth in terms of units sold over the 2018‚Äì2020 period. Conventional avocado production is forecasted to grow over 40 percent from 33.7 million to 47.6 million avocados sold each week on average.

Organic avocados are forecasted to experience even more meteoric growth. A consistent growth trend between 2015 and 2018 is predicted to continue through 2019 with a whopping 74 percent growth between 2018 and 2020. Between 2015 and 2018, suppliers sold about 970,000 organic avocados weekly. Between 2018 and 2020, the number is forecasted to jump to 1.68 million organic avocados.

Discussion

I think these models are really exciting. First, in terms of pattern recognition, it is interesting to see the models picking up on seasonal trends. The price models captured the Autumn price hikes and the volume models captured the spikes that occur in avocado volume each February.

Additionally, the diagnostics look pretty good. Mean Absolute Percentage Error, or MAPE, measures the error between the forecast and observed values, so a lower value indicates a better fitting model. The highest MAPE amongst the four models is for the organic volume model, at around 6 percent, indicating the model is about 94 percent accurate. I am very happy with this level of fit and I am confident in all of the models. At least, I am as confident as an forecasting newbie could be.

If I had to editorialize (which I will gladly do) I would say that the growth forecast for organic avocado volume is somewhat bullish. I am not sure how realistic 74 percent growth is over only two years, but testing several models yielded similar results.

Alternatively, I think that the conventional avocado price model is somewhat bearish. With the Autumn price hike increasing each year since 2015, I was surprised that the model forecasted overall price decreases. However, if it ends up being the case through 2020, you will not see me complaining at brunch.
How does Facebook define Terrorism in Relation to Artificial Intelligence?

How useful is the terminology terrorism? I would argue it is not useful because it obscures the specific debates into a reactionary pattern of violence against violence. However in a political science perspective this would to some degree be a social constructivist approach. Artificial intelligence being increasingly securitised will inevitably be mixed up in the policy process of these large social media companies. So let me explore how Facebook is addressing this issue.

In this article I will look at:

(1) Facebook and its definition of terrorism;

(2) into the stated approach to artificial intelligence;

(3) Facebook‚Äôs growing security team;

(4) the practical side and possible trauma of human moderation;

(5) the question of a US-centric focus on terror on social media;

(6) government requests for user data;

(7) the coming creation of the global oversight board that may set a precedence for the use of AI for both organisations and governments;

(8) vague Snapchat terrorism, a comparative outlook ‚Äì an outro.

1. Facebook and its Definition of Terrorism

In 2018 one of the largest social platforms on the planet decided to attempt defining terrorism, and it reads as the following:

‚ÄúAny nongovernmental organization that engages in premeditated acts of violence against persons or property to intimidate a civilian population, government or international organization in order to achieve a political, religious or ideological aim.‚Äù

In the blog post made the 23rd of April 2018 called Hard Questions: How Effective Is Technology in Keeping Terrorists off Facebook? A central paragraph by my own approximation reads:

The democratizing power of the internet has been a tremendous boon for individuals, activists, and small businesses all over the world. But bad actors have long tried to use it for their own ends. White supremacists used electronic bulletin boards in the 1980s, and the first pro-al-Qaeda website was established in the mid-1990s. While the challenge of terrorism online isn‚Äôt new, it has grown increasingly urgent as digital platforms become central to our lives. At Facebook, we recognize the importance of keeping people safe, and we use technology and our counterterrorism team to do it. [bold added]

The claims Facebook makes through this blog post:

Our definition is agnostic to the ideology or political goals of a group. Our counterterrorism policy does not apply to governments. Facebook policy prohibits terrorists from using our service, but it isn‚Äôt enough to just have a policy. We need to enforce it.

Despite making this claim they simultaneously say their focus lies on ISIS, al-Qaeda, and their affiliates ‚Äî the groups that currently pose the broadest global threat. However these are additionally of most interest and priority to the United States.

2. How does Facebook use Artificial Intelligence to Counter Terrorism?

This blog post additionally refers to a post written by Facebook called Hard Questions: How We Counter Terrorism. It is written by Monika Bickert, Director of Global Policy Management, and Brian Fishman, Counterterrorism Policy Manager. This post was made already on the 15th of June 2017.

The top point of this post is Artificial Intelligence. We want to find terrorist content immediately, before people in our community have seen it. Facebook has clearly been using AI since at least 2017 to remove posts associated with terrorism (they claim it was recent at the time). At the time they seemed to focus their efforts on ISIS and Al-Qaeda.

Image matching: When someone tries to upload a terrorist photo or video, their systems look for whether the image matches a known terrorism photo or video. This way they can avoid people uploading the same video.

Language understanding: Facebook had started to experiment with using AI to understand text that might be advocating for terrorism. They were at the time experimenting with removing text relating to what they had already seen as previously removed (historic data)

Removing terrorist clusters: Facebook claims to know from studies of terrorists that they tend to radicalize and operate in clusters. This offline trend is reflected online as well. They use signals like whether an account is friends with a high number of accounts that have been disabled for terrorism, or whether an account shares the same attributes as a disabled account.

Recidivism: Facebook said they had gotten much faster at detecting new fake accounts created by repeat offenders. Through this work, they have been able to dramatically reduce the time period that terrorist recidivist accounts are on Facebook. They argue this process is ‚Äòadversarial‚Äô that the other party keeps developing new methods.

Cross-platform collaboration: Because they didn‚Äôt want terrorists to have a place anywhere in the family of Facebook apps, they have began work on systems to enable us to take action against terrorist accounts across all our platforms, including WhatsApp and Instagram.

In the first quarter of 2018 they reported to have taken down 837 million pieces of spam and 2.5 million pieces of hate speech and disabled 583 million fake accounts globally. This was in relation to the statement saying it was assisted by using technology like: ‚Äú‚Ä¶machine learning, artificial intelligence and computer vision..‚Äù to detect ‚Äòbad actors‚Äô and move more quickly. They mentioned this particularly in relation to the election.

In 2019 They removed what they call ‚Äòinauthentic behaviour from Iran, Israel and Russia (focused on Ukraine) in particular.

Live-streamed attacks like Christchurch shooting require human moderation. LeCun said at a recent event that Facebook was years away from using AI to moderate live video at scale. LeCun the problem with the lack of training data. ‚ÄúThankfully, we don‚Äôt have a lot of examples of real people shooting other people,‚Äù you could train for recognition of violence using footage from movies, but then content containing simulated violence would be inadvertently removed along with the real thing.

Automated systems are claimed to be used mainly as assistants to human moderators.

AI is not a silver bullet to moderation. Understanding artificial intelligence in this context is of course not enough. Facebook has a community operations team that has to distinguish from a personal profile or a news story. This ‚Äòmore nuanced approach‚Äô requires human expertise. Understanding how Facebook uses artificial intelligence is of course not enough without understanding how their actual safety and security team manages these tools as well as frameworks.

3. Facebook‚Äôs Growing Safety and Security Team

Facebook feed, since the company‚Äôs 200-person counterterrorism team removed them. (In the wake of the Cambridge Analytica privacy scandal, Facebook is under pressure to show that it can police itself.) Reported in 2018.

Facebook was scheduled to be growing by 3,000 people over 2017‚Äî that work 24 hours a day and in dozens of languages to review these reports and determine the context. The link refers to a post made by Mark Zuckerberg stating that they already have 4,500 people hired in addition to those they had scheduled to hire.

In July the 6th 2018 (updated the 4th of December) Ellen Silver from Facebook as VP of operations claimed to be scaling globally, covering every time zone and over 50 languages. They had also rapidly grown their staff in safety and security:

‚ÄúThe teams working on safety and security at Facebook are now over 30,000. About half of this team are content reviewers ‚Äî a mix of full-time employees, contractors and companies we partner with.‚Äù

4. Insecurity Causing Trauma for Facebook Workers

In February 2019 The Verge published an article called The Trauma Floor: The secret lives of Facebook moderators in America. This article does of course describe the challenging conditions in which these moderators work, however it also mentions a stat of 15,000 moderators working around the world. It did seem rather a few of these were subcontracted through companies such as Cognizant having to sign NDAs, with secrecy supposedly protecting employees.

‚ÄúCollectively, the employees described a workplace that is perpetually teetering on the brink of chaos. It is an environment where workers cope by telling dark jokes about committing suicide, then smoke weed during breaks to numb their emotions. It‚Äôs a place where employees can be fired for making just a few errors a week ‚Äî and where those who remain live in fear of the former colleagues who return seeking vengeance.‚Äù

It is perhaps ironic that in attempting to handling terror there is a degree of trauma caused to the handlers. Certain of they key findings by the report by The Verge seems interesting to stress or at least consider:

Moderators in Phoenix will make just $28,800 per year ‚Äî while the average Facebook employee has a total compensation of $240,000.

Employees are micromanaged down to bathroom breaks. Two Muslim employees were ordered to stop praying during their nine minutes per day of allotted ‚Äúwellness time.‚Äù

Moderators cope with seeing traumatic images and videos by telling dark jokes about committing suicide, then smoking weed during breaks to numb their emotions. Moderators are routinely high at work.

Employees are developing PTSD-like symptoms after they leave the company, but are no longer eligible for any support from Facebook or Cognizant.

Employees have begun to embrace the fringe viewpoints of the videos and memes that they are supposed to moderate. The Phoenix site is home to a flat Earther and a Holocaust denier. A former employee tells us he no longer believes 9/11 was a terrorist attack.

According to the article these centres operate through accuracy standards which means posts reviewed are being reviewed. Facebook has set a goal of 95% accuracy, but Cognizant is usually never that high (closer to 80‚Äì92%). A moderator must suggest the correct community standard violation or risk loosing accuracy. The Verge article mentions a few different set of truths that a moderator has to consider.

Community Guidelines, publicly posted ones and internal documents. Known Questions. A 15,000-word second document with commentary. Discussions amongst moderators attempting to reach a consensus. Facebook‚Äôs own internal tools for distributing information.

Further it is said that the challenge of keeping a job may be rather difficult: ‚ÄúThe job resembles a high-stakes video game in which you start out with 100 points ‚Äî a perfect accuracy score ‚Äî and then scratch and claw to keep as many of those points as you can. Because once you fall below 95, your job is at risk.‚Äù

Fired employees regularly threatened to return to work and harm their old colleagues. An NDA usually seem to stop you from talking about the work you were doing or even state that you ever worked for Facebook, according to The Verge: ‚ÄúThey do the work as long as they can ‚Äî and when they leave, an NDA ensures that they retreat even further into the shadows. To Facebook, it will seem as if they never worked there at all. Technically, they never did.‚Äù

Facebook has a clear idea of how their policies should be managed:

‚ÄúWe want to keep personal perspectives and biases out of the equation entirely ‚Äî so, in theory, two people reviewing the same posts would always make the same decision.‚Äù

In a statement that contradicts the article by The Verge they state: ‚ÄúA common misconception about content reviewers is that they‚Äôre driven by quotas and pressured to make hasty decisions.‚Äù They is stated to have four clinical psychologists across three regions who are tasked with designing, delivering and evaluating resiliency programs. Yet it is questionable whether this decentralised mental care without professionals on-the-ground is advisable given the work these employees have to go through.

5. US-Centric Global Moderation of Terror

We can ask a simple question: when policy and guidelines are designed in US for the world what perspectives are prevalent in the given framework? As you may have guessed for the section title I am sceptical whether a universal framework based on one location can work well across the planet.

Their enforcement have focused heavily on Islamic Terrorist groups rather than right-wing extremism or other forms of ‚Äòterror‚Äô. They have had a partnership with Microsoft, Twitter and YouTube on hashes relating to terrorist content. These are all companies based in the United States.

Counterspeech programs. Facebook support several major counterspeech programs. For example, last year we worked with the Institute for Strategic Dialogue to launch the Online Civil Courage Initiative. The project challenge was to design, pilot, implement and measure the success of a social or digital initiative, product or tool designed to push back on hate and violent extremism. Reportedly it engaged with more than 100 anti-hate and anti-extremism organizations across Europe.

They‚Äôve also worked with Affinis Labs to host hackathons in places like Manila, Dhaka and Jakarta, where community leaders joined forces with tech entrepreneurs to develop innovative solutions to push back against extremism and hate online.

We want Facebook to be a hostile place for terrorists.

Saying this they quoted the 1984, the Irish Republican Army (IRA) statement after a failed assassination: ‚ÄúToday we were unlucky, but remember that we only have to be lucky once ‚Äî you will have to be lucky always.‚Äù In one way the statement resounds, yet you cannot avoid everything forever. If there is no room for failure, then any smudge on the perfect surface can stain the image ‚Äî of course this is important for Facebook. We can ask whether this decision of decentralised moderation makes it easier to blame external actors for any ‚Äòexternalities‚Äô relating to safety and security.

6. Government Requests for User Data

It is of course possible to access Facebook‚Äôs data if there is a security event that requires access. Government requests for user data increased globally by 7% from 103,815 to 110,634 in the second half of 2018. With the United States continues to submit the highest number of requests, followed by India, the United Kingdom, Germany and France. This reflected a normal growth according to Facebook.

As part of the requests 58% included a non-disclosure order prohibiting Facebook from notifying the user. In an internal review of their US national security reporting metrics Facebook found that it had undercounted requests from the Foreign Intelligence Surveillance Act (FISA). Facebook divides these requests into emergency requests and legal processes.

Facebook may voluntarily disclose information to law enforcement where we have a good faith reason to believe that the matter involves imminent risk of serious physical injury or death.

It may be useful to understand these two different data requests:

Legal Process Requests: Requests we receive from governments that are accompanied by legal process, like a search warrant. We disclose account records solely in accordance with our terms of service and applicable law.

Emergency Disclosure Requests: In emergencies, law enforcement may submit requests without legal process. Based on the circumstances, we may voluntarily disclose information to law enforcement where we have a good faith reason to believe that the matter involves imminent risk of serious physical injury or death.

‚ÄúGovernment officials sometimes make requests for data about people who use Facebook as part of official investigations. The vast majority of these requests relate to criminal cases, such as robberies or kidnappings‚Äù

During this period Facebook and Instagram took down 2,595,410 pieces of content based on 511,706 copyright reports; 215,877 pieces of content based on 81,243 trademark reports; and 781,875 pieces of content based on 62,829 counterfeit reports.

Facebook recently started partnering with ethics institutions focused on artificial intelligence. The focus of this partnership seem to be in the direction of safety, at least in Munich the Institute they have partnered with will address issues that affect the use and impact of artificial intelligence, such as safety, privacy, fairness and transparency. I have previously described that this can be problematic: an issue of self-policing ethical conduct.

7. The Global Oversight Board Ensuring a Global Perspective

Facebook is creating a global oversight board. In a post by Nick Clegg, the new VP of Global Affairs and Communications in January 2019 a draft charter was released. The draft lists 11 questions alongside considerations and suggested approaches. More recently in late June 2019 another post was made by Facebook on this topic.

It was stated they (Facebook) had traveled around the world hosting six in-depth workshops and 22 roundtables attended by more than 650 people from 88 different countries. They had personal discussions with more than 250 people and received over 1,200 public consultation submissions.

Subsequently a 44-page report was released by Facebook called Global Feedback & Input on the Facebook Oversight Board for Content Decisions. This talks of a global constitution, board membership, content decisions and governance. Nick Clegg states in the introduction:

‚ÄúOur task is to build systems that protect free expression, that help people connect with those they care about, while still staying safe online. We recognize the tremendous responsibility we have not only to fairly exercise our discretion but also to establish structures that will evolve with the times. Our challenge now, in creating this Oversight Board, is to shore up, balance, and safeguard free expression and safety for everyone on our platforms and those yet to come onto them, across the world.‚Äù

The report argues that there needs to be more democracy in Facebook. There needs to be a system to appeal decisions. The report gives different examples of moderation. It also states that Facebook undertook research to study the range of oversight models that exist globally which identified six ‚Äùfamilies‚Äú of oversight design. The grid they presented looks like this.

According to the report public reason giving will be a crucial feature of the Oversight Board, one which drives at the heart of the legitimacy of its decisions.

The Draft Charter suggests that Facebook will select the first cohort of members, with future selection to be taken over by the Board itself. The report stated that questioned were raised to this proposal of leaving future selection up to the Board itself, as this could result in both a ‚Äúrecursion problem‚Äù and possibly the ‚Äúperpetuation of bias.‚Äù A few approaches were suggested for membership in the board:

Membership be left to a fully democratic vote by Facebook users. A hybrid approach, combining selection procedures so that Facebook, outside groups, and users could all participate. Soliciting public comment on a slate of applicants. Inviting civil society groups to select some of the Board members. Asking governments to weigh in on names and candidates. Opening a public nomination process. A randomised lottery system to select members from Facebook users.

There was an agreed importance of diversity, though it was mentioned that perfect representation is not possible. It was mostly agreed that Facebook employees (current and former) should be excluded from the Board. It was suggested a fixed term of three years, renewable once.

In the report it is suggested two ways to submit both for Facebook to send important or disputed content and for the users. Facebook has proposed that smaller panels, not the Board as a whole, will hear and deliberate on cases. It was clear that: ‚ÄúA strong consensus emerged that the Board‚Äôs decisions should influence Facebook‚Äôs policy development.‚Äù

It was noted that Facebook was to establish an independent trust to remunerate (pay) board members. It was argued this board needed its own staff and that these be wholly independent of Facebook. The scope for the board will be content governance. However it was indicated that the board could hear other policy issues, such as: ‚Äú‚Ä¶algorithmic ranking, privacy, local law, AI, monetization, political ads, and bias.‚Äù

Thus it can be said that Facebook and the field of artificial intelligence may be rather influenced by decisions made by this board in the future should it possibly be established. Indeed considering the scale of Facebook this can both influence private companies to adopt certain practices or nations to make legislation based on the decisions made by this semi-independent council. In the conclusion of the internal report it is stated:

‚ÄúFacebook finds itself in a historically unique position. It cannot deprive or grant anyone the freedom of expression, and yet it is a conduit through which global freedom of expression is realized.‚Äù

Vague Snapchat Terrorism? A comparative look ‚Äî an outro

In their Community Guidelines Snapchat does not define terrorism, yet they write: ‚ÄúTerrorist organizations are prohibited from using our platform and we have no tolerance for content that advocates or advances terrorism.‚Äù Yet we may ask ourselves two questions: what is a terrorist organisation and what does advocating terrorism mean in practice if it remains undefined? You could take the: ‚ÄúI know terrorism when I see it‚Äù-approach yet that leaves a lot up to ambiguous choices without transparency of decisions involved. This seems part of the wicked problem of terrorism: definitions.

Terrorism in international politics is hard to define, and how you define it may also says a lot about how you think about politics more broadly. Although it is notoriously difficult to define it may be one of the future discussions to be undertaken should an oversight board from Facebook appear. The focus that Facebook has had on Islamic terror as opposed to right-wing extremism or gun violence in the United States is a worrying example. Yet their move to establish a board may be an appropriate response.

The policing or ways that different governments request user data should continue to be under strong scrutiny with transparency. The state is an actor that can inflict violence; state-inflicted violence can be ambiguous, particularly when there are claims to state-sponsoring of terrorism. Most certainly the state can act using terror, and it is occurring, so does terrorist have to be in a minority group; is it genocide or terror; and does this distinction matter?

Terror in some cases is about scaring people ‚Äî violence is used in a restrictive case. Is it illegitimate use of violence by non-state actors aiming for the spread of ideology? If so whose ideology in a board run by Facebook, and the concerns of diversity is real. When to justify intervention and not alongside how it is justified may be important as pragmatic definitions arise as products of the prevailing interests.

When is an act of violence a weapons of the weak in asymmetrical power distributions? What is the difference between narco traffickers and large resource interests that funds political power? The goals aspect is worth considering: knowing someone‚Äôs intention, yet the environment that shapes this intention is equally as important. Moderating terror in terrible working conditions is just one example of many.

If we take seriously that we are individuals with ideas, there are some patterns, but a lot of it is quite hard to predict. If it is hard to predict human behaviour then it is hard to know people‚Äôs aims and quite difficult to see the people‚Äôs intention.

Where is the money coming from? We have data brokers and there is not currently enough regulation to ensure that the flow of data is responsible or is sold of unintentionally to groups intending to use the data for such purposes. Terrorism obscures ‚Äî it is not a value-neutral term. Technology is not value-neutral at all. It ties into ideas of securitization and state powers alongside its ethical discourse of technological for good.

Slapping the terrorist label puts it into a different category. Understanding can be an important tool in how to prevent it. Putting the T-word on it is tempting in a rapid pace of content moderation, yet we need to engage with it.

As much as there is a need to be respectful of the way large companies are trying to moderate and cooperate with state institutions we need to be critical. Robert Cox said it well: ‚ÄúTheory is always for someone or for something.‚Äù In this respect perhaps technology is always for someone or something too. I will end with a video that was shared in my class today that proposes a critical view on the labelling of terrorism:
Whenever you are manipulating data, the very first thing you should do is investigating relevant statistical properties. In particular, you might be interested in knowing whether your data follow a known distribution.

Why is this important? Think about the goal of your data analysis: once you are provided with a sample of observations, you want to compute some statistics (i.e. mean, standard deviation‚Ä¶) as well as build confidence intervals and conduct hypotheses tests. To do so, you need to assume your data to be following a known distribution, such as Normal, X-square or T-student.

Unfortunately, most of the time your data are presented to you without having a known distribution, hence you don‚Äôt know the shape of their density function. Here Bootstrap sampling comes to aid: the aim of this technique is assessing stats and properties of a potential distribution without actually knowing its shape.

How does it work? Imagine you are provided with a set of data (your population) and you get a sample of size n of them.

Now, if you proceed with a re-sampling of your initial sample for B times (you can set B as large as you want. In general, it is set equal to or greater than 10000), you will generate B further samples, each with length n (with the possibility of one or more values to be repeated).

Now, for each sample, you can compute the estimation of the parameter you are interested in. It will be a generic function of each sample T(x^1*) and we will refer to it as Œ∏ÃÇ1*.

Now, the idea is that, if we collect all the statistics we computed, we can generate an approximation of the probability function of our initial population. One standard choice for an approximating distribution is the empirical distribution function of the observed data.

In statistics, an empirical distribution function is the distribution function associated with the empirical measure of a sample. It is a cumulative distribution which jumps‚Ä¶
Photo by Mario Gogh on Unsplash

Introduction

We know that larger companies contain more than thousand employees working for them, so taking care of the needs and satisfaction of each employee is a challenging task to do, it results in valuable and talented employees leave the company without giving the proper reason.

Employee churn is a major problem for many firms these days. Great talent is scarce, hard to keep and in high demand. Given the well-known direct relationship between happy employees and happy customers, it becomes of utmost importance to understand the drivers of employee dissatisfaction.

This post emphasizes on predicting retention of an employee within an organization such that whether the employee will leave the company or continue with it. It uses the data of previous employees who have worked for the company and by finding a pattern it predicts the retention in the form of yes or no.

The parameters we are using, such as salary, number of years spent in the company, promotions, number of hours, work accident, financial background, etc. Through this paper, an organization can choose its strategies to keep great representatives from leaving the organization. The data has 14,999 examples (samples). Below are the features and the definitions of each one:

satisfaction_level: Level of satisfaction {0‚Äì1}.

last_evaluationTime: Time since last performance evaluation (in years).

number_project: Number of projects completed while at work.

average_montly_hours: Average monthly hours at the workplace.

time_spend_company: Number of years spent in the company.

Work_accident: Whether the employee had a workplace accident.

left: Whether the employee left the workplace or not {0, 1}.

promotion_last_5years: Whether the employee was promoted in the last five years.

sales: Department the employee works for.

salary: Relative level of salary {low, medium, high}.

The source code that created this post can be found here.

Data Set

loading the data into the data frame and separating the result column.

FinalCode.py hosted by GitHub

Data Preprocessing

The dataset has ‚Äòsalary‚Äô and ‚Äòsales‚Äô column as categorical data, So we have to perform OneHotEncoding & LabelEncoding to convert this data into numerical form and To create dummy features we have to drop the first one to avoid linear dependency where some learning algorithms may struggle.

After that, we will split the data into training and testing datasets.

Preprocessing

Regression Models

Because we want results in the form of ‚ÄòYes‚Äô or ‚ÄòNo‚Äô such that whether an employee will leave the company or not, So the best suitable regression model is Logistic Regression for this dataset. Logistic regression is a classification algorithm used to assign observations to a discrete set of classes. Some of the examples of classification problems are Email spam or not spam, Online transactions Fraud or not Fraud, Tumor Malignant or Benign.

To calculate the accuracy of the result our model has generated we will be going to use Confusion Matrix as an evaluation parameter.

Logistic Regression

Classifiers

A decision tree is a flowchart-like tree structure where an internal node represents feature(or attribute), the branch represents a decision rule, and each leaf node represents the outcome. The topmost node in a decision tree is known as the root node. It learns to partition based on the attribute value. It partitions the tree in a recursive manner call recursive partitioning.

Random forests is a supervised learning algorithm. A forest is comprised of trees. It is said that the more trees it has, the more robust a forest is. Random forests create decision trees on randomly selected data samples, gets a prediction from each tree and selects the best solution through voting. It also provides a pretty good indicator of the feature importance.

Here in our dataset, we will use these two classifiers to classify our result in the form of Yes and No.

Decision Tree and Random Forest Classifier

Conclusion

Factors affecting the employment

Here in the above graph numbers on x-axis from 0 to 6 are representing WithHigherProjects,WithLowerSalary,WithHigherTime,WithPromotion,WithWorkAccident,WithorNotWorkAccident,WithoutWorkAccident. Each of these are the factors which can affect employment as WithHigherTime represents, employees who have more than four year of work experience but still haven‚Äôt got any promotion is 1750 which is a significant amount, WithLowerSalary represents employees whose salary level is low even when their evaluation score was higher than 3 such employees are 750.

Thus after evaluating this dataset, we get to know that lower salary levels, no promotions even when employees are working more than 4 years are the two main reasons for the employees to leave the organization.
Regularization for Machine Learning Models

A common problem in machine learning is overfitting, where a model falsely generalizes noise in the training data:

A popular approach to remedy this problem and make the model more robust is regularization: A penalty term is added to the algorithm‚Äôs loss function. This changes the model‚Äôs weights which result from minimizing the loss function.

The most popular regularization techniques are Lasso, Ridge (aka Tikhonov) and Elastic Net. For the exemplary case of simple linear regression with only one weight parameter w (the slope of the linear fit), their penalty terms look like this (including a scaling parameter Œª):

Lasso (L1) : Œª¬∑|w|

: Œª¬∑|w| Ridge (L2) : Œª¬∑w¬≤

: Œª¬∑w¬≤ Elastic Net (L1+L2): Œª‚ÇÅ¬∑|w| + Œª‚ÇÇ¬∑w¬≤

The different terms have different effects: Compared to L1, the quadratic L2 regularization becomes negligible at small weights (close to zero), but stronger at large weights. This leads to the following behaviours, casually phrased:
These days I have been obsessed with researching data maps. I challenged myself and made a data map using Excel. The graphic below is the dynamic map of Hurricane Irma that I have drawn with Excel.

If you are interested, I will be happy to share with you the process of making a hurricane map with Excel. It is to use the bubble chart to outline the dynamic path of the hurricane and to show the change in wind strength. Here are the specific steps for making a hurricane map.

1. Prepare Materials

‚ë† Find a map of the US Atlantic that includes lines of latitude and longitude.

Note that the map we need must have latitude and longitude lines. Many maps provided on the public network are inaccurate. I downloaded this map from National Hurricane Center and Central Pacific Hurricane Center.

‚ë° Download the storm track statistics of Hurricane Irma from Weather Underground, including date, time, latitude, longitude, wind, and so on.

2. Process Data

‚ë† Remove the units of data such as latitude, longitude, wind speed, wind pressure, etc. And the date and time should be converted into a format that is easy for Excel to process.

‚ë° We can see that the hurricane statistics are recorded every six or three hours in the data material. Here we keep the data recorded every six hours.

3. Draw a Bubble Chart

‚ë† Insert the above map into the table, and then draw a bubble chart on it. The X axis of the bubble chart represents the longitude data, the Y axis represents the latitude data, and the bubble size depends on the value of the wind pressure.

‚ë° Design the format of Chart Area. The maximum and minimum values of the coordinate axes in the bubble chart are set according to the latitude and longitude readings. And make the spacing on the coordinates coincide with the spacing of the latitudes and longitudes on the map. Here I set the spacing to 5, which ensures that the data points drawn by the bubble chart match the actual latitude and longitude on the map.

‚ë¢ Drag and drop the border of the plot area to make it coincide with the coordinate axes in the bottom Atlantic map. Then hide the axis data and set the border to ‚ÄúNo line‚Äù.

4. Make Dynamic Effects

To achieve the dynamic effects of the hurricane trajectory, we can use Slider Control to control the time. Each time we slide, the time is increased by 6 hours. The cell linked by the slider passes the data to the table area on the left side of the chart. And then it obtains the corresponding latitude and longitude data and wind data by querying the time point. At the same time, two sets of data sources of the chart are generated from this time data. The entire control chain is thus formed and the final control of the chart by the slider is completed.

Further than that, if I want to implement automatic control of the slider, I will need VBA code to make the chart automatically show dynamic effects.

Operation Process:

‚ë† Write two macros with VBA. One macro controls the start, the other controls the stop.

‚ë° Draw 4 controls, representing start, stop, loop, and slider. Specify the above macros.

‚ë¢ The format of the slider control specifies the left cell, which records the sliding data. And the upper cell ‚ÄúData & Time‚Äù calls the data of this cell.

‚ë£ Prepare dynamic data.

In order to achieve the above dynamic effects, two sets of data are actually prepared in the bubble chart. One set shows all the path points that the hurricane has traveled before the current time point, and the other set marks the location of the hurricane at the current time point.

Here we need to use the LOOKUP function and call the data of the cell ‚ÄúData & Time‚Äù.

For the first set of data, we select all the data less than or equal to the current time point and set the other data points to #N/A.

For the second set, we select the data that matches the current time, and the other data is also set to #N/A. Here, #N/A doesn‚Äôt display data points in the chart.

‚ë§ Bind the bubble chart to the data source.

In the final step, we bind the bubble chart to the data source and the hurricane trajectory map is complete.

The textbox in the chart can get the information of the data points directly from the cells by linking with them. In addition to the bubble chart, a set of column charts is added to the map to show the wind, and its data binding operation form is the same as that of the bubble chart.

Well, the general idea of making a hurricane map with Excel is like this. Below is an official map of Hurricane Irma. Is it very similar to the one I made with Excel?

From Weather Underground

Tips

Excel is very powerful, but if you want to use it to make some complicated charts, you must have a code base and learn VBA language, which is time consuming. I shared an article 4 Uses of Data Maps in Business Analysis, in which all maps are made with the zero-code visualization tool FineReport. The operation is very simple. If you don‚Äôt understand the code, you can try this tool to create data visualization charts.

You might also be interested in‚Ä¶

Top 16 Types of Chart in Data Visualization

How Can Beginners Design Cool Data Visualizations?

A Beginner‚Äôs Guide to Business Dashboards
Trees in data science

One of the most easy to interpret models in machine learning are CART‚Äôs(Classification and Regression Trees) known popularly as decision trees. In this post I wish to give an overview of Decision Trees, some primary concepts surrounding them and finally Random Forests. The contents are as follows

Understanding Decision Trees

Purity

Bootstrapping and Bagging

Random Forests

Lets Go!

Decision Trees

Basic structure of a Decision Tree (Source: cway-quan)

In the machine learning universe trees are actually upside down versions of real trees. Suppose we have a dataset consisting of our features ‚ÄòX‚Äô and a target ‚ÄòY‚Äô. What a decision tree does is that it finds patterns within X and splits the dataset into smaller subsets based on these patterns.

Visualize these splits in the slightly simplified image above. ‚ÄòY‚Äô here is whether or not a job offer is to be accepted. The ‚ÄòX‚Äô contains features like ‚Äúcommute_time‚Äù, ‚Äúsalary‚Äù, ‚Äúfree_coffee‚Äù.

Based on patterns in ‚ÄòX‚Äô the tree is split into branches until it reaches a point where it arrives at a pure answer to ‚ÄòY‚Äô. In our scenario, job offers which are accepted have to provide a salary > 50k, commute time < 1hr and free coffee. In this manner the tree reaches the last leaf which is a pure decision about ‚ÄòY‚Äô.

Purity in decision Trees

Decision trees conduct splitting based on the purity of the node. This purity is measured based on the distribution of ‚ÄòY‚Äô. If our ‚ÄòY‚Äô is continuous our problem is a regression problem and the nodes are split based on MSE(Mean Squared Error). If ‚ÄòY‚Äô is discrete, our model is dealing with a classification problem and a different measure of purity is required.

A widely used metric to measure in classification cases is Gini Impurity. The formula for Gini impurity is given as follows:

Source: General Assembly DSI curriculum (Authors:David Yerrington, Matt Brems)

While deciding which split to make at a given node, it picks the split that maximizes the drop in Gini impurity from the parent node to the child node.

Bootstrapping and Bagging

To understand bootstrapping and bagging, the first step would be to understand why they are needed in the first place. It is basically trying to emulate the ‚Äúwisdom of the crowd‚Äù principle where in the aggregated result of multiple models is better than the results of a single model. The following image by Lorna Yen gives a great idea about boot strapping.

(Author: Lorna yen, Source)

Bootstrapping as shown above is just the random sampling of data with replacement. Bagging is just a process of building decision trees on each of these samples and getting an aggregate prediction. So to summarize Bagging involves the following steps:

From the original data of size n, bootstrap k samples of size n with replacement Build a decision tree on each bootstrapped sample. Pass test data through all trees and develop one aggregate prediction

Bagging is therefore also called Bootstrapped Aggregating.

Random Forest Model

A closer look at the below image gives a basic intuition on random forests.

A basic hurdle in bagging is that the individual decision trees are highly correlated as the same features are used in all trees. So the predictions of our models suffer from the issue of variance. To know more on variance or bias you can read this link. Decorrelating our models is a solution and is exactly what Random Forests do.

Random forests follow similar steps in bagging except that they use at each split in the learning process, a random subset of the features. This mitigates the variance problem in bagging and generally produces much better results. This efficient and simple methodology has made Random forests a widely implemented Machine learning Model.

Bonus

Code for importing the explained three classification models in sklearn.
Missing data points can sometimes feel like missing jigsaw pieces (photo: author)

For a few months now, we‚Äôve been looking at different ways to use SQL in data science ‚Äî most recently on using SQL to detect outliers (not the same as detecting anomalies, although that‚Äôs in the future).

In the first post in this sporadic series of posts, we looked at how to assess missing data by counting the proportion of nulls in a column. However, simply finding and counting missing data points is of limited use ‚Äî you need to have tools to use if you find these missing data points.

The core ways to handle missing values should be familiar to all data scientists, a phrase which here means ‚Äòif you aren‚Äôt familiar, you should memorise the following list‚Äô:

Listwise deletion: if a variable has so many missing cases that it appears useless, delete it. Casewise deletion: if there are too many factors missing for a particular observation, delete it. Dummy Variable Adjustment: if the variable is missing for a particular case, use an assumed value in its stead. Depending on the problem the median may appear the intuitive choice or a value that represents a ‚Äòneutral‚Äô setting. Imputation: use an algorithm to fill in the value, from a simple random number at the most basic end of the spectrum, to a value imputed by its own model at the more complex end.

SQL is clearly better at some of these than others. Listwise deletion is as simple as leaving the column name out of the SELECT statement; casewise deletion may be as simple as a WHERE clause testing for nulls.

Using these two methods comes with risks ‚Äî you can easily introduce bias if the data are not Missing Completely at Random. It‚Äôs not hard to think of situations where measurements aren‚Äôt collected due to specific circumstances reflected in the data. An example given by Frank Harrell in his book ‚ÄòRegression Modeling Strategies‚Äô is blood pressure measurements which are not taken on patients expected to die shortly ‚Äî hence excluding patient observations missing blood pressure measurements in a model of patient outcomes could selectively exclude patients who died, creating an obvious skew.

While the above example illustrates the risk of casewise deletion, the disadvantage of listwise deletion can also be seen from the‚Ä¶
Reference Paper: Reducing Network Agnostophobia: https://arxiv.org/pdf/1811.04110.pd

For classification models for many domains and scenarios it is important to predict when the input given to the model does not belong to the classes it was trained on.

For computer vision / object detector models author provide following justification:

Object detectors have evolved over time from using feature-based detectors to sliding windows [34], region proposals [32], and, finally, to anchor boxes [31]. The majority of these approaches can be seen as having two parts, the proposal network and the classification network. During training, the classification network includes a background class to identify a proposal as not having an object of interest. However, even for the state-of-the-art systems it has been reported that the object proposals to the classifier ‚Äústill contain a large proportion of background regions‚Äù and ‚Äúthe existence of many background samples makes the feature representation capture less intra-category variance and more inter-category variance (...) causing many false positives between ambiguous object categories‚Äù [41]. In a system that both detects and recognizes objects, the ability to handle unknown samples is crucial. Our goal is to improve the ability to classify correct classes while reducing the impact of unknown inputs.

It is also an important problem to tackle in many domains including healthcare, robotics irrespective of vision or NLP.

Nomenclature:

Past approaches to handle this have relied on two fundamental approaches:

Given an input provide uncertainty score based on how close is the input to inputs seen in training. ==> P (U | x) Given an input x predict probability of that input belonging to all classes Ci (i=1 to n) the model was trained on. We then threshold on having a minimum probability for class with highest probability to reject input as out of set or unknown.

Here are summary of some of the related approaches:
Reinforcement Learning Introduction

An introduction to reinforcement learning problems and solutions Y Tech ¬∑ Follow 4 min read ¬∑ Jul 25, 2019 -- Share

This post will be an introductory level on reinforcement learning. Throughout this post, the problem definitions and some most popular solutions will be discussed. After this article, you should be able to understand what is reinforcement learning, and how to find the optimal policy for the problem.

The Problem Description

The agent-environment interaction in reinforcement learning

The Setting

The reinforcement learning (RL) framework is characterized by an agent learning to interact with its environment.

learning to interact with its environment. At each time step, the agent receives the environment‚Äôs state (the environment presents a situation to the agent), and the agent must choose an appropriate action in response. One time step later, the agent receives a reward (the environment indicates whether the agent has responded appropriately to the state) and a new state .

(the environment presents a situation to the agent), and the agent must choose an appropriate in response. One time step later, the agent receives a (the environment indicates whether the agent has responded appropriately to the state) and a new . All agents have the goal to maximize the expected cumulative reward.

Episodic vs. Continuing Tasks

Continuing tasks are tasks that continue forever, without end.

are tasks that continue forever, without end. Episodic tasks are tasks with a well-defined starting and ending point.

* In this case, we refer to a complete sequence of interaction, from start to finish, as an episode.

* Episodic tasks come to an end whenever the agent reaches a terminal state.

Cumulative Reward

The discounted return at time step t is G(t) = R(t+1) + Œ≥*R(t+2) + Œ≥^2*R(t+3) + ...

The agent selects actions with the goal of maximizing expected (discounted) return.

The discount rate Œ≥ is something that you set, to refine the goal that you have the agent.

* It must satisfy 0 ‚â§ Œ≥ ‚â§ 1 .

* If Œ≥ = 0 , the agent only cares about the most immediate reward.

* If Œ≥ = 1 , the return is not discounted.

* For larger values of Œ≥ , the agent cares more about the distant future. Smaller values of Œ≥ result in more extreme discounting.

MDPs and One-Step Dynamics
Throughout history, we have seen how organisations across entire industries have embraced robotic technology, and how today, it is almost impossible for some of these organisations to operate without it. Every day, we are witnesses of how technology is integrating into nearly every aspect of our life at work, and in many cases, we depend on it to perform most of our daily tasks.

Over many decades, organisations have been using robots to automate daily processes. In fact, every day we are surrounded by them, whether at home, in the street or at the office ‚Äî from ATMs and vending machines to more sophisticated surgical robots and self-driving cars. These robots come in different forms and shapes, and perhaps they don‚Äôt look like humans, but the enterprise loves them, and they surely are a key driver for massive productivity.

Now, imagine a regular day ‚Äî wake up in the morning, go to your workplace, grab a coffee, and start a conversation with your colleague‚Ä¶ Everything seems normal, you engage in the conversation, exchange thoughts and new ideas, then finish and move on to your desk to start working. An hour later, your manager asks you to analyse and compare five long reports. You don‚Äôt have time for that, it would take too many hours of work and tons of energy; plus there are other things that you have to do. So, you ask your colleague for help and amazingly get the results back within an hour.

This sounds very unrealistic, right? No normal colleague would do this work for you or even finish it so fast. But what if I tell you that your colleague is a ROBOT? A robot that looks and acts the same as any other human being. It can talk like you, think like you, and almost reason like you do. But it is still a robot. How would you feel?

SOURCE: ¬© ADOBE STOCK

Artificial Intelligence, Machine Learning & Robotics:

Many robots are not artificially intelligent, they are programmed to perform repetitive tasks or movements to manipulate objects in the real world. However, some experts suggest that what constitutes a robot is its ability to think and make decisions, but doesn‚Äôt it imply the use of AI?

On the other hand, Artificial Intelligence (AI) aims to replicate human intelligence behaviour by addressing skills like problem-solving, learning, perception, and reasoning. It can involve some level of Machine learning (ML), which is based on the idea of granting machines access to data that will allow them to learn by themselves.

AI together with ML and Robotics intend to create a man-made machine/robot with human intellectual capacities that can be able to formulate original ideas by itself. We have not achieved this yet, but we have made a lot of progress.

You have probably already heard about Sophia, the social robot that looks like a human. If you know what I‚Äôm talking about, think about a more sophisticated version of Sophia working and collaborating with humans.

Other big developments include the so-called ‚ÄúRobotic Process Automation‚Äù (RPA). These are software robots that help businesses and employees do simple jobs by replicating human interaction. It is only a software and not a physical AI Robot, but this is definitely a significant breakthrough.

The real challenge is to make AI understand how natural intelligence works because we know how our brain functions, and we can teach AI how to think and learn, but we still don‚Äôt know exactly how all these connections can lead AI to the use of abstract reasoning or ‚Äúcommon sense‚Äù.

SOURCE: ¬© ADOBE STOCK

My Colleague is a Robot:

Going back to where we started ‚Äî robots designed to share a workspace with humans by carrying out physical and intellectual tasks, building up ideas with humans, learning from our environment, questioning decisions, and finding solutions together, will reduce many risks and exponentially increase productivity.

Robots can already do many things much better than humans, but it still takes humans to interpret their work and apply the results in strategic and creative ways. For this reason, we need to make sure that robots are seen as complementary rather than competitive, and assign them the work that no one wants to do, the one that is intense and repetitive; leaving the part of the work that involves judgement and expertise, to humans.

Imagine one more time, the same scenario where you engage in a conversation with your colleague, but now you know for sure and from the beginning that it is a robot. Would it feel weird?

Probably, and I know it sounds scary as well, but if we give this deeper thought and think about the benefits and not the drawbacks of having an artificially intelligent coworker, it might just change our perspective. Some of these benefits include:

Safety: Tasks that involve the use of heavy machinery, sharp objects, very high or low temperatures, chemicals, and others, will be performed by robots. This will protect workers in dangerous and unhealthy working conditions.

Speed and Consistency: AI robots work fast and without any distractions, they have no need for vacations and are available 24/7.

No Errors: Robots have almost no room for mistakes, they are precise and deliver quality.

Happiness & Productivity: Most importantly, all these perks are intended to increase both, employee happiness and productivity. As mentioned before, these robots will take over those tasks that we don‚Äôt enjoy. From dangerous, tedious, and repetitive basic tasks to more complicated ones that require highly analytical skills.

SOURCE: ¬© ADOBE STOCK

Examples of Work That Can Be Complemented By AI Robots:

Data Journalists: This type of journalists are those who are focused on analysing data. AI robots could perfectly perform these work much faster and efficiently.

Secretaries: Administrative tasks like answering phone calls, sending emails, scheduling meetings, and others (including physical, manual tasks), can be done by AI robots.

Document-Review (Attorneys): Many attorneys have to search through thousands of documents looking for specific information. AI robots can filter information in a flash, they can also analyse it and generate reports. This work applies to other related fields as well.

Pharmacists: When you go to a pharmacy, an AI robot could scan your prescription and get your medicine. For over-the-counter medicine, you could indicate your symptoms and the robot will suggest a recommendation. Also, these robots could potentially have access to data from hospitals and your health records to make suggestions more accurate.

AI Police and Intelligence Assitant: AI Robots could potentially assist the police and agencies like the CIA by collecting, storing, sorting through, and highlighting key data that is necessary for investigations. They could also perform some physical duties like patrolling, arresting, and even directing traffic.

Botenders: Robots can be taught how to mix and serve drinks. Anything from beers to signature cocktails. They can make hundreds of them within minutes.

There are many concerns about robots replacing people and eliminating jobs, but these robots could potentially work alongside humans, collaborate and complement our work rather than taking over jobs. In fact, technology will create more jobs than it will eliminate. Many jobs will change, and the new ones will require a new set of skills that we must acquire through advanced education and training systems boosted by AI.

If we are going to share a workspace with robots and see them as partners rather than adversaries, they must first experience the world as a human, meaning that they would need to be able to learn about us. This will make the interaction between humans and robots much easier and can also allow them to keep learning much faster.

Morality and AI Coworkers

What do we expect from artificially intelligent coworkers in terms of morality?

SOURCE: ¬© ADOBE STOCK

AI and robotics will have a huge impact on society, values, and human rights.

If we want machines to operate autonomously, at one point, they will require to collect a lot of data. But how much of these data do we want to share with robots? If a robot causes an accident, who would be responsible? Are we willing to give up our privacy to interact with robots? Can we even trust a robot?

More questions will continue to arise as technology develops, and it is up to us to answer those questions by working together to implement a structure that regulates and protects the contour of these innovations.

There are always two sides to every story, times of uncertainty will come and technology if gone wrong, can become dangerous. Therefore, we have to manage robots carefully, in the end, it is humans who will take charge of controlling, checking, and running the bots.

AI and robotics should no longer be feared, but rather be seen as a tool for collaboration.
Machine Learning for Content Moderation ‚Äî Challenges

Overview

For an introduction to the topic of machine learning for content moderation, read the Introduction of this series:

Now that we have gone over an overview of machine learning systems used for automatic content moderation, we can address the main challenges faced by these systems. These potential problems can lead to difficulties in evaluating the model, determining approaching classifier thresholds, and using it fairly and without unintentional bias. Since content moderation systems act on complex social phenomena, they face problems that are not necessarily encountered in other machine learning contexts.

Varying definitions

For many applications of content moderation, it is difficult to provide an explicit definition of the phenomenon of interest. These topics are often very complex social phenomena, whose definitions are topics of constant academic debate. For example, cyber-bullying has various definitions in academic texts, and so it is difficult to create an all-encompassing definition that everyone can agree on. For this reason, instructions provided to manual content labelers may not be clear enough to produce very reliable labels.

This leads to two problems.

First, it may appear to users that the content moderation system is inconsistent, if some things that users deem to be violating rules are removes and some are not. This may lead users to not trust the content moderation mechanisms or believe that it is unfairly targeting certain users. Since, from the user perspective, these systems are nebulous black boxes, it is difficult to explain how such inconsistencies may arise.

The other problem is that the labeled training data may have contradictory data points. If labeling inconsistencies cause two very similar data points to have opposing labels, then‚Ä¶
Linear Regression

Linear Regression is a famous supervised learning algorithm used to predict a real-valued output. The linear regression model is a linear combination of the features of the input examples.

A note on the notation. x_{i} means x subscript i and x_{^th} means x superscript th.

Representation of the Data

As discussed in the definition, linear regression is a supervised learning algorithm, therefore, has a set of N labelled examples, represented as :

Data used to construct a linear regression model.

Here, x_{i} represents a set of properties corresponding to the i_{^th} example. These set of properties are collectively called a feature vector. All the examples from i=1,2,3,‚Ä¶,n have a corresponding real-valued y, which denotes a physical quantity such as cost, temperature, or any other continuous value.

Here each feature vector is 3-dimensional consisting of the area of house in square metres, the number of rooms and the age of house in years. The target variable is the price of house in USD.

Model

Now, as we have our examples ready we want to make our model f(x) that will help us to predict the output y for an unseen x.

Model of linear regression learning algorithm. Here, w is an R-dimensional parameter vector (x is an R-dimensional feature vector) and b is the bias.

The job of the model is to predict a real-value y for an unseen value of the feature vector x. But, we want to find a model such that it does the best job in predicting the values of y, therefore, we want to find values of w and b such that the predictions are as close as possible to the actual answers. It is obvious that different values of w and b result in producing different models, of varying capabilities. Therefore, our job is to find the optimal set of values w* and b* which will minimize the error between the predictions made by the model f(x) and the actual results y for the training set.

The Best Model

As discussed earlier we have N examples and a model f(x) for which we need to find the optimal values of w and b. Let us use all these N examples for finding the optimal values of w and b, popularly called as training our model. We need to find values of w and b such that the following expression is minimum.

Cost function for linear regression.

This is our objective function as we are going to minimize it. Learning algorithms have functions which we try to minimize or maximise. These functions are called as loss function or the cost function. This particular form is called the mean squared error loss function.

If you observe the loss function:

It is simply subtracting the model‚Äôs output, f(x_{i}) and the actual output y_{i},

Squaring it,

And finally taking its average.

To understand this better let us assume John, recently appeared for an examination having 10 mathematical questions and the answer key has been published. Now John decides to find out how well has he performed? so, he compares his answer, f(x)_{i} with the corresponding answer y_{i} on the answer key. If the difference between John‚Äôs answer and the actual answer f(x)_{i}-y_{i} is 0 he answered that question correctly. If he answered all the questions correctly then the average will also be 0 which corresponds to the best performance, implying the best model. Squaring the error helps to accentuate the error of the model. We could have also taken a cube or higher power but then the derivatives would have been more difficult to work out. We worry about the derivatives of the cost function as setting them to zero gives the optimal value w* and b* for the model.

General Questions and Examples

Let us discuss a few questions that perplexed me while studying about linear regression. But, before we start let‚Äôs take a look at a very primitive example of linear regression.

So, John and his friends decided to start studying linear regression from scratch so they began by collecting examples themselves. The examples they collected are shown below.

The tabular form of data. Here x is a single dimensional feature vector any y is a real-valued output corresponding to each feature vector.

After having collected the data, John decides to fit a linear regression model to it.

The linear regression model of form f(x)=wx+b.

This is a model of form f(x)=wx+b where w is a scalar, as x, the feature vector is one dimensional. A better comprehension of this model is to compare this to the equation of a straight line y=mx+c where m is analogous to w and c to b. This is a linear model.

But, can we do better? Can we come up with a model that performs better than the current one? Yes, we can. It is a common confusion that linear regression only comprises of models that are straight lines. However, we can also fit curves to our data by transforming the data. Let‚Äôs transform our feature vector by squaring each x_{i} value.

After having transformed our feature vector let us try to fit a model on the new feature vector x¬≤ and the output y (original feature vector x is not considered for training the model instead, it‚Äôs transformation x_{^ 2} has been used to train the model).

The model is a better fit than the previous linear model. Here the original feature vector x_{i} is transformed to it‚Äôs square and then the model is computed.

So, now we have predicted a polynomial model that is better than the linear one by transforming the original feature vector x_{i} to its square. The new model corresponds to f(x)=wx¬≤+b.

This is a plot of the polynomial regression model. Note that this plot is between X and Y with the polynomial model of degree two. The previous plot was between X¬≤ and Y, therefore, it was linear.

The capability of the model to predict better results has increased by transforming the feature vectors but we need to be aware of over fitting. Over fitting happens when the model predicts too well during the training phase but makes an error while predicting unseen examples. Over fitting does not reflect the real-world scenario of being dynamic. It does not produce generalised models.

This is an example of overfitting where the model is too accurate on the training examples.

Let‚Äôs say that the feature vector is R-dimensional. We have seen the case where R=1 and also predicted a linear and a polynomial model. If R=2 a plane is predicted as the model. Generally linear regression models a hyper plane for a data set with R-dimensional feature vector, x and 1-dimensional output, y.

Hyper plane is a subspace with one less dimension than that of its surrounding space. In case of a 1 dimensional line the point is a hyper plane, in case of a 2 dimensional region a line is a hyper plane, in case of a 3 dimensional space the plane is a hyper plane and so on.

The Bias Term

Let‚Äôs discuss the utility of the bias term. Consider the equation of a straight line y=mx. In this case m controls the slope of the line and can rotate the line anywhere but only about the origin.

Suppose you decide to use this model for a trivial linear regression problem. However, any hypothesis that you generate will always pass through the origin and might fail to generalise. Adding the bias term will result in the hypothesis y=mx+c thereby, allowing you to move your line anywhere in the plane. The bias term helps in generalising the hypothesis.
This article is the 2nd and last article on Dependency Parsing. We will give you some easy guidelines for implementation and the tools to help you improve it.

Vocabulary

A TreeBank is a parsed text corpus that annotates syntactic or semantic sentence structure. Dependency TreeBanks are created using different approaches : either thanks to human annotators directly, or using automatic parsers to provide a first parse, then checked by annotators. A common approach consists in using a deterministic process to translate existing TreeBanks into new language through head rules. Producing a high-quality TreeBank is both time-consuming and expensive.

is a parsed text corpus that annotates syntactic or semantic sentence structure. Dependency TreeBanks are created using different approaches : either thanks to human annotators directly, or using automatic parsers to provide a first parse, then checked by annotators. A common approach consists in using a deterministic process to translate existing TreeBanks into new language through head rules. Producing a high-quality TreeBank is both time-consuming and expensive. CoNLL-U ‚Äî Computational Natural Language Learning-Universal is a revised version of the CoNLL-X format. Sentences from TreeBanks are separated, and each word or punctuation mark is disposed on a distinct line. Each of the following items follows the word, separated by tabulations:

‚ÄìID: word index in the sentence, starting at 1

‚ÄìFORM: word form or punctuation

‚ÄìLEMMA: Lemma or stem of word form

‚ÄìUPOS: Universal part of speech tag

‚ÄìXPOS: Language-specific part of speech tag; will not be used in our model

‚ÄìFEATS: Unordered list of morphological features, defined by Universal Dependencies; indicates the gender and number of a noun, the tense of a verb, etc.

‚ÄìHEAD: Head of the word, indicates the index of the word to which the current one is related

‚ÄìDEPREL: Universal Dependencies relation; indicates the relation between two words (subject or object of a verb, determiner of a noun, etc.)

‚ÄìDEPS: Language-specific part dependencies; will not be used in our model

‚ÄìMISC: Commentary or other annotation

An example of CoNLL-U format

An Entry is a word, or a punctuation mark in a sentence. It has multiple attributes, defined above. A sentence is typically a concatenation of entries (a word itself is an attribute of an entry: its form), separated by space.

The Implementation

The implementation of the Bist-Parser comes from the authors of its paper. An update has been published on GitHub by Xiezhq Hermann. You can find it here. It works on Python 3.x, with torch 0.3.1 (with or without Cuda). It is very complete and can be used as is. However, in order to adapt the code to your data or upgrade it, you must get through every module, which can be a difficult task. This part of the article will lead you through all files and processes.

Universal Dependencies (UD) is an open-community framework for grammatical annotation. It provides corpora and tools that greatly help to develop a Dependency Parser.

From UD, you can download a corpus of sentences of your choice (in any language available, even Old French!), use them as is, and start training your Bist-Parser with this type of command:

python src/parser.py --outdir [results directory] --train training.conll --dev development.conll --epochs 30 --lstmdims 125 --lstmlayers 2 [--extrn extrn.vectors]

You can detail hyperparameters here, caught by the model thanks to the file parser.py

As you may know, when you train a model on a corpus, the model is biased towards this corpus. You could train your model on multiple corpora in order to generalize it more. Several techniques allow you to increase scores, with TreeBank Embedding as an example. Here, we have just concatenated some TreeBanks, without any further processing.

utils

Create a ConllEntry class: every entry has well-known attributes: id, form, lemma, Universal PoS tag, language Specific PoS tag, morphological features, head of current word, dependency relation, enhanced dependency relation and commentary. These attributes are defined from the Universal Dependencies CoNLL-U format. This format is useful for the model to understand what its inputs are, and what it should predict.

class: every entry has well-known attributes: id, form, lemma, Universal PoS tag, language Specific PoS tag, morphological features, head of current word, dependency relation, enhanced dependency relation and commentary. These attributes are defined from the Universal Dependencies CoNLL-U format. This format is useful for the model to understand what its inputs are, and what it should predict. Read a CoNLL-U file and transform each sentence into a ConllEntry.

Count vocabulary: This function creates a Counter of ConllEntry attributes and allows you to know how these attributes are distributed through your dataset. If you want to determine the most frequent words or relations in your dataset, this function can be useful.

mstlstm

This file contains your model. All your hyper-parameters and most of your monitoring work happen in this file.

The method forward iterates through each entry in the sentence. It first computes the vectors for each entry attribute. With our model, we get multiple vectors that describe the word, the PoS tag and the feats. Those vectors are then concatenated to form a vector with a bigger dimension for each entry. These entries are then concatenated together to form the sentence vector.

First, it converts entries into vectors. Here, the principal attributes are the embedding of words, lemmas (onto) and PoS tags (pos). However, we advise you to add as many features as possible. For example, you may have access to features of words that indicate whether the noun is singular or plural, its gender, or tense‚Ä¶ Embedding these features allows your BiLSTM to find many more patterns.

Evolution of PoS embedding on two dimensions

Then, it feeds the BiLSTM with these vectors (for = forward, back = backward). Line 52 evaluates the scores of the sentence. This is the part where the full Weighted Digraph is created. On line 57, it evaluates the relation score. This is an interesting trick in this model: rather than evaluating all the possibilities at the same time (|possibilities|=|arcs|.|labels|, which is way too high), it predicts the dependencies first, then the relations.

We will see about errs, lerrs and e later.

In the illustrations below, you can see the evolution of dependency evaluation through batches. A dark blue cell corresponds to a weighted arc. The example comes from a typical French sentence, ‚ÄúLes commotions c√©r√©brales sont devenu si courantes dans ce sport qu‚Äôon les consid√®re presque comme la routine.‚Äù You can spot spelling mistakes in the sentence; this is not rare in TreeBanks.
Photo by fabio on Unsplash

‚ÄúLook at the data ‚Äî the numbers don‚Äôt lie.‚Äù It‚Äôs an often given piece of advice, but a less often understood one. Because what the person giving the advice really means is ‚ÄúLook at the data, and think about what it means for the situation we‚Äôre facing. Once you consider it in the broader context, you‚Äôll see ‚Äî and intuitively know ‚Äî what to do next.‚Äù That‚Äôs a very different challenge to meet, but an infinitely more valuable one when you‚Äôre trying to make sense of a complex situation. And it‚Äôs possible only when you realise that connecting your data is even more important than collecting it.

Why Collection is no Longer Enough

Data collection is now mainstream. Storage is cheap, source are abundant and most products being built today are done so with the expectation that organisations will want access to their data in ways outside of the product‚Äôs offerings, via bulk export or automated delivery. Despite this helpful (and rapidly becoming expected) type of access, most data is still viewed within the single perspective that it was collected. Sure ‚Äî graphs, charts and dashboards are possible and certainly helpful, but they‚Äôre often created using data that‚Äôs already been actioned, and usually to just re-tell the same story that‚Äôs already been told. The innovation factor is low, because the connection factor is also low.

But apart from these technical considerations, there is another more fundamental reason why disconnected data isn‚Äôt as helpful as it could be. Human beings are evolutionary creatures, and at our most basic level we are instinctively wired to consider the entirety of a situation before responding to it. Early human tribes didn‚Äôt seek out short-term gain if it compromised their chances of long-term survival (though it could be argued that the modern world has reduced our ability for this type of thinking) because to do so would contradict our unique awareness for considering relationships in our environment. The same holds true for data collected from technical systems ‚Äî a broader context is lacking by default, and context is key to understanding.

What Connection Makes Possible

Data is enriched through connection, which offers a far more realistic representation of complex environments. When we look at data that is connected through a relevant framework, we engage the part of our mind that understands consequences, dependencies and how the quality of relationships affect the outcome we are seeking.

The argument is sometimes made that this type of thinking is still possible with disconnected data, it just requires us to put the pieces together in our head. This is the same argument that says human beings can multitask, but multiple studies have confirmed that we don‚Äôt ‚Äî rather, we single-task with quick switching. This switching comes at a mental cost though, which can lead to poor outcomes through reluctance to gather all of the disconnected pieces every time a decision needs to be made.

Connecting data into a seamless, holistic perspective removes the need for switching. It enables us to understand the contributing factors of an event and act appropriately. Over time, this heightened awareness leads to the development of new knowledge, which if delivered back into the framework creates a unique feedback loop whereby insight gives rise to further insight. As knowledge is shared, unexpected opportunities for collaboration emerge. The innovation factor rises in response to the rising connection.

Connection is Always Relevant ‚Äî Especially When Humans Are Involved

Understanding how to connect data in an organisation can sometimes be difficult. Over time people tend to specialise in a given area, and that specialisation can lead to the belief that one type of data has limited relevance to another. But if looked at closely, these same areas will usually be found to have unseen ties and concerns to others. Connecting the data of all areas in an organisation helps to generate a wider context that speaks to our evolutionary understanding of nature and our place in it, where no one entity truly exists in isolation.

This is even more true for organisations that work with human networks. We are deeply connected beings, with our behaviours often influenced far more by those around us (and even those around them) than we believe. These social ties often vary in type and strength, both of which affect the flow of influence and change. Proper consideration of these diverse relationships is fundamental to making informed decisions, and only possible when data is properly connected.

This article first appeared on InsightableTimes.org at https://insightabletimes.org/in-the-new-era-of-knowledge-connection-beats-collection-everytime
In this post, I will outline the steps I took to discover similar and dissimilar coffee-neighborhoods in Istanbul locals‚Äô favorite districts.

As part of my IBM data science course project, I was made to come up with a problem and find a solution to it by gathering, exploring and analyzing location data. Being a lover of Istanbul and coffee, I decided to come up with something that strides the two subjects.

Istanbul is one of the biggest and most populous cities in the world, the only city that exists on two continents. Both parts of the city are divided by the Bosphorous strait. Two Districts loved by Istanbul residents are Be≈üikta≈ü (be-shik-tash) and Kadƒ±k√∂y (ka-di-koy) on the European and Asian side respectively. While these districts have a lot in common, they have their fair share of differences as well, the surge of coffee shops for one. In fact, according to Foursquare, 8 of 15 best coffee shops in Istanbul are located in Be≈üikta≈ü and Kadƒ±k√∂y.

There is a fierce debate among residents about the neighborhood to best enjoy a cup of coffee. This report will address the issue by providing insights drawn from data. This study will be of interest to both visitors of Istanbul and locals who yet to discover the hidden similarities between the two most sought after neighborhoods. The report will help readers to:

Be more familiar with the discussed neighborhoods Understand the relationship between coffee shops and other neighborhood attributes Discover the similarities between neighborhoods in terms of coffee shops and other attributes Be able to make better-informed decisions about where to coffee in Istanbul like a resident

The neighborhoods that will be examined are shown on the map with the red markers.
Hey Alexa, Tell Me a Joke!

I don‚Äôt remember what her response was, but I do remember being so excited with the new device that I posted an AMA (which is an acronym for Ask Me Anything) on Reddit. The headline was: I just got Alexa, type in what you want me to ask her!

For the next day, Redditors would send me questions to ask, I‚Äôd ask Alexa, and I‚Äôd reply to those Redditors with her response. Some of them might‚Äôve been Alexa engineers themselves; they asked questions that gave Easter Egg answers, sort of like ordering from a secret menu (if you have Alexa, try asking her: Alexa, who is the fairest of them all?). That day, I also earned the highest amount of Karma I‚Äôd ever earned on Reddit (Karma is a points system on Reddit that is equivalent to likes on Facebook or Twitter). Thanks, Alexa.

Before long, I was purchasing smart bulbs and smart outlets, all switched on and off by Alexa and thus, via the transitive property (if A=B and B=C, then A=C), my voice. It was just too convenient compared to finding your phone, unlocking it, opening up the smart bulb app, and then manually adjusting lighting with your fingers. And while Alexa was already convenient throughout my home, she was even better in bed‚Äî I no longer needed to get up to turn off the lights before going to sleep.

A Human Connection
Mae Fah Luang University Campus on March 2019. (Photo by MFU Photoclub with permission)

In the previous blog, I looked at the winter air pollution in Bangkok. The main source of pollution comes from particles smaller than 2.5 micrometer (PM 2.5 particles). These particles are smaller than the width of a human hair and can easily enter our bodies, even making their way into our blood. Last week (March 17, 2019), many provinces in the northern part of Thailand had the worst Air Quality Index (AQI) in the world due to particle pollution. So far, no long term solution has been proposed because the source of the PM 2.5 particle pollution has not been clearly pinpointed. In this notebook, I identify the sources of high PM 2.5 particles in Bangkok through a machine learning model. The code can be found in my GitHub page.

High PM2.5, Who Are the Culprits ?

There are three major theories regarding the source of air pollution in Bangkok: (1) The temperature inversion effect where cold air along with pollution is trapped close to the surface of the Earth. This theory was proposed by the government at the beginning of the 2019 winter season. The government blamed emission from old diesel engines for the pollution. (2) Agricultural burning, either locally or from surrounding provinces. During winter, a lot of open agricultural burning occurs throughout the country. Some officials have tried to tackle the air pollution problem by reducing open agricultural burning. (3) Pollution from other provinces or countries. Some NGOs blamed the pollution on near by power plants.

My analysis procedure is as follows: Build a machine learning model(ML) to predict the air pollution level in Bangkok using environmental factor such as weather, traffic index, and fire maps. Include date-time features such as local hour, and weekday versus weekend in the model to capture other effects from human activities. Identify dominant sources of pollution using the feature of importance provided by the ML model.

If the source of the pollution is local, then the AQI will depend on factors such as weather patterns (wind speed, humidity, average temperature), local traffic, and hour of day. If the pollution is from agricultural burning, the AQI will depend on active fires with some time lag to account for geographical separation. Fire activities are included based on the distance from Bangkok. On the other hand, if the pollution not correlated with the fire map, then the model should put more weight on weather patterns, such as wind direction and wind speed.

Here are a list of features I considered and their data sources:

Active fire information from NASA‚Äôs FIRMS project

Weather pattern: temperature, wind speed, humidity, and rain, scraped from the Weather Underground website

Traffic index from Longdo Traffic

Date time features: hour of day, time of day, and holiday patterns (explored in the Part I blog post)

Let me first walk through all the features included in the model.

Agricultural Burning is a Major Problem !

Farmers in Southeast Asia pick January ‚Äî March as their burning season. For the north and northeastern provinces in Thailand, these burning activities are large enough to make these provinces among the most polluted places in the world during this time. For Bangkok, one might argue that because the region is heavily industrial rather than agricultural, it may not be affected as much by agricultural burning. But this is not the case.

Because of the tiny size of PM 2.5 particles, they remain suspended in the atmosphere for prolonged periods and can travel over very long distances. From the weather data, the average wind speed is 10 km/hour. The reported PM 2.5 level is a rolling average over 24 hours. A rough estimate is that the current PM 2.5 reading may be from sources as far as 240 km away. The picture below shows the fire map measured by NASA‚Äôs satellites, indicative of agricultural burning, on Jan 8, 2018 and on Feb 8, 2018. The yellow circle indicates the area within 240 km of Bangkok. The number of fires on Jan 8, which has an acceptable level of pollution, is much lower than the number of fires on Feb 8, which has an unhealthy level of pollution.

Fire spots from NASA‚Äôs satellites

In fact, the fire pattern closely aligns with the PM 2.5 pattern.

The number of fires aligns with spikes in PM 2.5 levels

Weather Patterns

The temperature inversion effect often occurs during winter because the temperature is cooler near the ground. The hotter air on top traps the cool air from flowing. This stagnant atmospheric condition allows the PM 2.5 particles to remain suspended in the air for longer. On the other hand, higher humidity or rain will help remove particles from the atmosphere. This is one reason why in the past when the air pollution was high, the government has sprayed water in the air. Unfortunately, this mitigation does not appear to be effective, since the volume of water is minuscule compared to actual rain. How much influence does weather pattern have on air pollution? Let‚Äôs compare the weather in winter versus other seasons.

compare the weather pattern in winter and other seasons

Temperature, wind speed and humidity are all lower in winter, but not by a large amount. Now, let‚Äôs look at the relationship of each of these with the PM 2.5 level.

Effect of temperature, wind speed, and humidity on PM 2.5 level in winter

Higher temperature (which disrupts the temperature inversion effect), wind speed and humidity have a negative correlation with the pollution level.

Effect of wind on PM 2.5 level in winter

On windy days, the pollution is clearly better. The median of the distribution for PM 2.5 levels is lower on windy days compared to on days without wind.

In fact, the pollution level also depends on the wind direction, as seen in this plot. I selected only four major wind directions for simplicity.

PM2.5 relationship with the wind direction in winter

On the days where the wind comes from the south, the pollution level is lower likely because the Thai gulf is to the south of Bangkok. The clean ocean wind improves the air quality. Wind from the other three directions pass overland. However, having any wind is better than the stagnant atmospheric conditions on calm days.

The shift in the median PM 2.5 level is smaller between rainy days and days with no rain. There are fewer rainy days during the winter season, so the data is somewhat noisy, but a difference can be observed in the cumulative density function.
Cleaning, Analyzing, and Visualizing Survey Data in Python

A tutorial using pandas , matplotlib , and seaborn to produce digestible insights from dirty data Charlene Chambliss ¬∑ Follow Published in Towards Data Science ¬∑ 10 min read ¬∑ Mar 30, 2019 -- 6 Listen Share

If you work in data at a D2C startup, there‚Äôs a good chance you will be asked to look at survey data at least once. And since SurveyMonkey is one of the most popular survey platforms out there, there‚Äôs a good chance it‚Äôll be SurveyMonkey data.

The way SurveyMonkey exports data is not necessarily ready for analysis right out of the box, but it‚Äôs pretty close. Here I‚Äôll demonstrate a few examples of questions you might want to ask of your survey data, and how to extract those answers quickly. We‚Äôll even write a few functions to make our lives easier when plotting future questions.

We‚Äôll be using pandas , matplotlib , and seaborn to make sense of our data. I used Mockaroo to generate this data; specifically, for the survey question fields, I used "Custom List" and entered in the appropriate fields. You could achieve the same effect by using random.choice in the random module, but I found it easier to let Mockaroo create the whole thing for me. I then tweaked the data in Excel so that it mirrored the structure of a SurveyMonkey export.

Oh boy‚Ä¶here we go

Your first reaction to this might be ‚ÄúUgh. It‚Äôs horrible.‚Äù I mean, the column names didn‚Äôt read in properly, there are a ton of NaNs, instead of numerical representations like 0/1 or 1/2/3/4/5 we have the actual text answers in each cell‚Ä¶And should we actually be reading this in with a MultiIndex?

But don‚Äôt worry, it‚Äôs not as bad as you might think. And we‚Äôre going to ignore MultiIndexes in this post. (Nobody really likes working with them anyway.) The team needs those insights ASAP ‚Äî so we‚Äôll come up with some hacky solutions.

First order of business: we‚Äôve been asked to find how the answers to these questions vary by age group. But age is just an age--we don't have a column for age groups! Well, luckily for us, we can pretty easily define a function to create one.

But if we try to run it like this, we‚Äôll get an error! That‚Äôs because we have that first row, and its value for age is the word ‚Äúage‚Äù instead of a number. Since the first step is to convert each age to an int , this will fail.

We need to remove that row from the DataFrame, but it‚Äôll be useful for us later when we rename columns, so we‚Äôll save it as a separate variable.

You will notice that, since removing headers , we've now lost some information when looking at the survey data by itself. Ideally, you will have a list of the questions and their options that were asked in the survey, provided to you by whoever wants the analysis. If not, you should keep a separate way to reference this info in a document or note that you can look at while working.

OK, now let‚Äôs apply the age_group function to get our age_group column.

Great. Next, let‚Äôs subset the data to focus on just the first question. How do the answers to this first question vary by age group?

Great. We have the answers in a variable now. But when we go to plot this data, it‚Äôs not going to look very good, because of the misnamed columns. Let‚Äôs write up a quick function to make renaming the columns simple:

Remember headers from earlier? We can use it to create our new_names_list for renaming.

It‚Äôs already an array, so we can just pass it right in, or we can rename it first for readability.

Isn‚Äôt that so much nicer to look at? Don‚Äôt worry, we‚Äôre almost to the part where we get some insights.

Notice how groupby and other aggregation functions ignore NaNs automatically. That makes our lives significantly easier.

Let‚Äôs say we also don‚Äôt really care about analyzing under-30 customers right now, so we‚Äôll plot only the other age groups.

OK, this is all well and good, but the 60+ group has more people in it than the other groups, and so it‚Äôs hard to make a fair comparison. What do we do? We can plot each age group in a separate plot, and then compare the distributions.

‚ÄúBut wait,‚Äù you might think. ‚ÄúI don‚Äôt really want to write the code for 4 different plots.‚Äù

Well of course not! Who has time for that? Let‚Äôs write another function to do it for us.

I believe it was Jenny Bryan, in her wonderful talk ‚ÄúCode Smells and Feels,‚Äù who first tipped me off to the following:

If you find yourself copying and pasting code and just changing a few values, you really ought to just write a function.

This has been a great guide for me in deciding when it is and isn‚Äôt worth it to write a function for something. A rule of thumb I like to use is that if I would be copying and pasting more than 3 times, I write a function.

There are also benefits other than convenience to this approach, such as that it:

reduces the possibility for error (when copying and pasting, it‚Äôs easy to accidentally forget to change a value)

makes for more readable code

builds up your personal toolbox of functions

forces you to think at a higher level of abstraction

(All of which improve your programming skills and make the people who need to read your code happier!)

Hooray, laziness!

This is, of course, generated data from a uniform distribution, and we would thus not expect to see any significant differences between groups. Hopefully your own survey data will be more interesting.

Next, let‚Äôs address another format of question. In this one, we need to see how interested each age group is in a given benefit. Happily, these questions are actually easier to deal with than the former type. Let‚Äôs take a look:

And look, since this is a small DataFrame, age_group is appended already and we won't have to add it.

Cool. Now we have the subsetted data, but we can‚Äôt just aggregate it by count this time like we could with the other question ‚Äî the last question had NaNs that would be excluded to give the true count for that response, but with this one, we would just get the number of responses for each age group overall:

This is definitely not what we want! The point of the question is to understand how interested the different age groups are, and we need to preserve that information. All this tells us is how many people in each age group responded to the question.

So what do we do? One way to go would be to re-encode these responses numerically. But what if we want to preserve the relationship on an even more granular level? If we encode numerically, we can take the median and average of each age group‚Äôs level of interest. But what if what we‚Äôre really interested in is the specific percentage of people per age group who chose each interest level? It‚Äôd be easier to convey that info in a barplot, with the text preserved.

That‚Äôs what we‚Äôre going to do next. And ‚Äî you guessed it ‚Äî it‚Äôs time to write another function.

Quick note to new learners: Most people won‚Äôt say this explicitly, but let me be clear on how visualizations are often made. Generally speaking, it is a highly iterative process. Even the most experienced data scientists don‚Äôt just write up a plot with all of these specifications off the top of their head.

Generally, you start with .plot(kind='bar') , or similar depending on the plot you want, and then you change size, color maps, get the groups properly sorted using order= , specify whether the labels should be rotated, and set x- or y-axis labels invisible, and more, depending on what you think is best for whoever will be using the visualizations.

So don‚Äôt be intimidated by the long blocks of code you see when people are making plots. They‚Äôre usually created over a span of minutes while testing out different specifications, not by writing perfect code from scratch in one go.
Every once in a while, you notice something that you realize you probably should have noticed a long time ago. You start to see it everywhere. You wonder why more people aren‚Äôt talking about it.

For me, ‚Äúevery once in a while‚Äù was yesterday when I was scrolling through the #jobs channel in the SharpestMinds Slack workspace, and the ‚Äúsomething‚Äù is a big problem in the data science industry that I really don‚Äôt think we‚Äôre taking seriously enough: the vast majority of data science job descriptions do not convey the actual requirements of the position they‚Äôre advertising.

How do I know this? For one, quite a few of the jobs posted to our internal board included notes from the users (usually SharpestMinds mentors) who posted them, saying things like, ‚ÄúI know the posting says they‚Äôre looking for X and Y, but they‚Äôre actually fine with Z.‚Äù As often as not I‚Äôd also get direct messages from them saying the same thing.

In other words, when senior data scientists are called upon to recruit ‚Äúfor real‚Äù, their first move is often to throw away the job posting altogether.

This is not good, for several reasons. First, a misleading job description means that recruiters get a *ton* of irrelevant applications, and that candidates waste a *ton* of time applying to irrelevant positions. But there‚Äôs another problem: job descriptions are the training labels that any good aspiring data scientist will use to prioritize their personal and technical skills development.

Despite the obvious downsides of these mangled job postings, companies keep putting them out there, so a very natural question to ask is: why? Why are job postings so confusing (in that they fail to clearly specify the skills they expect from a candidate), or so outrageously over-reaching (‚Äúlooking for a machine learning engineer with 10 years‚Äô experience in deep learning‚Ä¶‚Äù)?

There are many reasons. For one, companies make hiring decisions based on a candidate‚Äôs (perceived) ability to solve a real problem that they actually have. Because there are many ways to solve any given data science problem, it can be hard to narrow down the job description to a specific set of technical skills or libraries. That‚Äôs why it usually makes sense to put in an application for a company if you think you can solve the problems they have, even if you don‚Äôt know the specific tools they ask for.

Another possible reason is that many companies don‚Äôt actually know what they want ‚Äî especially companies with relatively new data science teams ‚Äî either because the early stage of their data science effort forces everyone to be a jack of all trades, or because they lack the expertise they need to even know what problems they have, and who can help solve them. If you come across an oddly non-specific posting, it‚Äôs worth taking the time to figure out which bucket it belongs to, since the former can be a great experience builder, whereas the latter can be a recipe for disaster.

But perhaps the most important reason is that job postings are often written by recruiters, who are not remotely technical. This has the unfortunate side-effect of resulting in occasionally incoherent asks (‚ÄúMust have 10+ years‚Äô experience with deep learning‚Ä¶‚Äù, ‚Äú‚Ä¶including natural language toolkits, such as OpenCV‚Ä¶‚Äù) or asks that no human being could possibly satisfy.

The net result of this job qualifications circus is that I regularly get questions from our mentees about whether they‚Äôre qualified for an opening, despite their having read all the information available on the internet about that position. Those questions are actually surprisingly consistent ‚Äî so much so that I think it‚Äôs worth listing the answers to the most common ones here, in the form of simple rules you can follow to make sure you‚Äôre applying to the right roles (and not being scared away by fake requirements):

If a company asks for more than 6 years of deep learning experience, then their posting was written by someone who has zero technical knowledge (AlexNet came out in 2012, so this basically narrows the field down to Geoff Hinton‚Äôs entourage). Unless you want to build a data science team from the ground up (which you shouldn‚Äôt if you‚Äôre new to the field), this should be a big red flag.

If you have no prior experience, don‚Äôt bother applying to jobs that ask for more than 2 years of it.

When they say ‚Äúor equivalent experience‚Äù, they mean, ‚Äúor about 1.5X that much experience working in a MSc or a PhD where you worked on something at least related to this‚Äù.

If you meet 50% of the requirements, that might be enough. If you meet 70%, you‚Äôre good to go. If you meet 100%, there‚Äôs a good chance you‚Äôre overqualified.

Companies *usually* care less about the languages you know than the problems you can solve. If they say Pytorch and you only know TensorFlow, you‚Äôre probably going to be ok (unless they stress the Pytorch part explicitly).

Don‚Äôt ignore lines like, ‚Äúyou should be detail-oriented and goal-driven, and thrive under pressure‚Äù. They sound like generic, cookie-cutter statements ‚Äî and sometimes they are ‚Äî but they‚Äôre usually written in a genuine attempt to tell you what kind of environment you‚Äôll be getting yourself into. At the very least, you should use these as hints about what aspects of your personality you should emphasize to establish rapport with your interviewers.

None of these rules are universally applicable, of course: the odd company will insist on hiring only candidates who meet all their stated requirements, and others will be particularly interested in people who know framework X, and will disregard people who can solve similar problems, but with different tools. But because there‚Äôs no way to know that from job descriptions alone (unless they‚Äôre explicit about it), your best bet is almost always to bet on yourself, and throw your hat in the ring.

If you want to connect, you can find me on Twitter at @jeremiecharris!
R and Python are the bread and butter of today‚Äôs machine learning languages. R provides powerful statistics and quick visualizations, while Python offers an intuitive syntax, abundant support, and is the choice interface to today‚Äôs major AI frameworks.

In this article we‚Äôll look at the steps involved in creating libraries in R and Python. This is a skill every machine learning practitioner should have in their toolbox. Libraries help us organize our code and share it with others, offering packaged functionality to the data community.

NOTE: In this article I use the terms ‚Äúlibrary‚Äù and ‚Äúpackage‚Äù interchangeably. While some people differentiate these words I don‚Äôt find this distinction useful, and rarely see it done in practice. We can think of a library (or package) as a directory of scripts containing functions. Those functions are grouped together to help engineers and scientists solve challenges.

THE IMPORTANCE OF CREATING LIBRARIES

Building today‚Äôs software doesn‚Äôt happen without extensive use of libraries. Libraries dramatically cut down the time and effort required for a team to bring work to production. By leveraging the open source community engineers and scientists can move their unique contribution towards a larger audience, and effectively improve the quality of their code. Companies of all sizes use these libraries to sit their work on top of existing functionality, making product development more productive and focused.

But creating libraries isn‚Äôt just for production software. Libraries are critical to rapidly prototyping ideas, helping teams validate hypotheses and craft experimental software quickly. While popular libraries enjoy massive community support and a set of best practices, smaller projects can be converted into libraries overnight.

By learning to create lighter-weight libraries we develop an ongoing habit of maintaining code and sharing our work. Our own development is sped up dramatically, and we anchor our coding efforts around a tangible unit of work we can improve over time.

ARTICLE SCOPE

In this article we will focus on creating libraries in R and Python as well as hosting them on, and installing from, GitHub. This means we won‚Äôt look at popular hosting sites like CRAN for R and PyPI for Python. These are extra steps that are beyond the scope of this article.

Focusing only on GitHub helps encourage practitioners to develop and share libraries more frequently. CRAN and PyPI have a number of criteria that must be met (and they change frequently), which can slow down the process of releasing our work. Rest assured, it is just as easy for others to install our libraries from GitHub. Also, the steps for CRAN and PyPI can always be added later should you feel your library would benefit from a hosting site.

We will build both R and Python libraries using the same environment (JupyterLab), with the same high-level steps for both languages. This should help you build a working knowledge of the core steps required to package your code as a library.

Let‚Äôs get started.

SETUP

We will be creating a library called datapeek in both R and Python. The datapeek library is a simple package offering a few useful functions for handling raw data. These functions are:

encode_and_bind

remove_features

apply_function_to_column

get_closest_string

We will look at these functions later. For now we need to setup an R and Python environment to create datapeek, along with a few libraries to support packaging our code. We will be using JupyterLab inside a Docker container, along with a ‚Äúdocker stack‚Äù that comes with the pieces we need.

Install and Run Docker

The Docker Stack we will use is called the jupyter/datascience-notebook. This image contains both R and Python environments, along with many of the packages typically used in machine learning.

Since these run inside Docker you must have Docker installed on your machine. So install Docker if you don‚Äôt already have it, and once installed, run the following in terminal to pull the datascience-notebook:

docker pull jupyter/datascience-notebook

This will pull the most recent image hosted on Docker Hub.

NOTE: Anytime you pull a project from Docker Hub you get the latest build. If some time passes since your last pull, pull again to update your image.

Immediately after running the above command you should see the following:

Once everything has been pulled we can confirm our new image exists by running the following:

docker images

‚Ä¶ showing something similar to the following:

Now that we have our Docker stack let‚Äôs setup JupyterLab.

JupyterLab

We will create our libraries inside a JupyterLab environment. JupyterLab is a web-based user interface for programming. With JupyterLab we have a lightweight IDE in the browser, making it convenient for building quick applications. JupyterLab provides everything we need to create libraries in R and Python, including:

A terminal environment for running shell commands and downloading/installing libraries;

environment for running shell commands and downloading/installing libraries; An R and Python console for working interactively with these languages;

for working interactively with these languages; A simple text editor for creating files with various extensions;

for creating files with various extensions; Jupyter Notebooks for prototyping ML work.

The datascience-notebook we just pulled contains an installation of JupyterLab so we don‚Äôt need to install this separately. Before running our Docker image we need to mount a volume to ensure our work is saved outside the container.

First, create a folder called datapeek on your desktop (or anywhere you wish) and change into that directory. We need to run our Docker container with JupyterLab, so our full command should look as follows:

docker run -it -v `pwd`:/home/jovyan/work -p 8888:8888 jupyter/datascience-notebook start.sh jupyter lab

You can learn more about Docker commands here. Importantly, the above command exposes our environment on port 8888, meaning we can access our container through the browser.

After running the above command you should see the following output at the end:

This tells us to copy and paste the provided URL into our browser. Open your browser and add the link in the address bar and hit enter (your token will be different):

localhost:8888/?token=11e5027e9f7cacebac465d79c9548978b03aaf53131ce5fd

This will automatically open JupyterLab in your browser as a new tab:

We are now ready to start building libraries.

We begin this article with R, then look at Python.

CREATING LIBRARIES IN R

R is one of the ‚Äúbig 2‚Äù languages of machine learning. At the time of this writing it has well-over 10,000 libraries. Going to Available CRAN Packages By Date of Publication and running‚Ä¶

document.getElementsByTagName('tr').length

‚Ä¶in the browser console gives me 13858. Minus the header and final row this gives 13856 packages. Needless to say R is not in need of variety. With strong community support and a concise (if not intuitive) language, R sits comfortably at the top of statistical languages worth learning.

The most well-known treatise on creating R packages is Hadley Wickam‚Äôs book R Packages. Its contents are available for free online. For a deeper dive on topic I recommend looking there.

We will use Hadley‚Äôs devtools package to abstract away the tedious tasks involved in creating packages. devtools is already installed in our Docker Stacks environment. We also require the roxygen2 package, which helps us document our functions. Since this doesn‚Äôt come pre-installed with our image let‚Äôs install that now.

NOTE: From now on we‚Äôll use the terminal in JupyterLab in order to conveniently keep our work within the browser.

Open terminal inside JupyterLab‚Äôs Launcher:

NOTE: If you‚Äôd like to change your JupyterLab to dark theme, click on Settings at the top, JupyterLab Theme, then JupyterLab Dark:

Inside the console type R, then‚Ä¶.

install.packages("roxygen2")

library(roxygen2)

With the necessary packages installed we‚Äôre ready to tackle each step.

STEP 1: Create Package Framework

We need to create a directory for our package. We can do this in one line of code, using the devtools create function. In terminal run:

devtools::create("datapeek")

This automatically creates the bare bone files and directories needed to define our R package. In JupyterLab you will see a set of new folders and files created on the left side.

NOTE: You will also see your new directory structure created on your desktop (or wherever you chose to create it) since we mounted a volume to our container during setup.

If we inspect our package in JupyterLab we now see:

datapeek

‚îú‚îÄ‚îÄ R

‚îú‚îÄ‚îÄ datapeek.Rproj

‚îú‚îÄ‚îÄ DESCRIPTION

‚îú‚îÄ‚îÄ NAMESPACE

The R folder will eventually contain our R code. The my_package.Rproj file is specific to the RStudio IDE so we can ignore that. The DESCRIPTION folder holds our package‚Äôs metadata (a detailed discussion can be found here). Finally, NAMSPACE is a file that ensures our library plays nicely with others, and is more of a CRAN requirement.

Naming Conventions

We must follow these rules when naming an R package:

must be unique on CRAN (you can check all current R libraries here);

(you can check all current R libraries here); can only consist of letters , numbers and periods ;

, and ; cannot contain an underscore or hyphen ;

or ; must start with a letter ;

; cannot end in a period;

You can read more about naming packages here. Our package name ‚Äúdatapeek‚Äù passes the above criteria. Let‚Äôs head over to CRAN and do a Command+F search for ‚Äúdatapeek‚Äù to ensure it‚Äôs not already taken:

Command + F search on CRAN to check for package name uniqueness.

‚Ä¶looks like we‚Äôre good.

STEP 2: Fill Out Description Details

The job of the DESCRIPTION file is to store important metadata about our package. These data include others packages required to run our library, our license, and our contact information. Technically, the definition of a package in R is any directory containing a DESCRIPTION file, so always ensure this is present.

Click on the DESCRIPTION file in JupyterLab‚Äôs directory listing. You will see the basic details created automatically when we ran devtools::create(‚Äúdatapeek‚Äù) :

Let‚Äôs add our specific details so our package contains the necessary metadata. Simply edit this file inside JupyterLab. Here are the details I am adding:

Package : datapeek

: Title : Provides useful functions for working with raw data.

: Version : 0.0.0.1

: Authors@R : person(‚ÄúSean‚Äù, ‚ÄúMcClure‚Äù, email=‚Äùsean.mcclure@example.com‚Äù, role=c('aut','cre'))

: Description : The datapeek package helps users transform raw data for machine learning development.

: The datapeek package helps users transform raw data for machine learning development. Depends : R (‚â• 3.5.1)

: License : MIT

: Encoding : UTF-8

: LazyData: true

Of course you should fill out these parts with your own details. You can read more about the definitions of each of these in Hadley‚Äôs chapter on metadata. As a brief overview‚Ä¶the package , title , and version parts are self-explanatory, just be sure to keep title to one line. Authors@R must adhere to the format you see above, since it contains executable R code. Note the role argument, which allows us to list the main contributors of our library. The usual ones are:

aut : author

cre : creator or maintainer

ctb : contributors

cph : copyright holder

There are many more options, with the full list found here.

You can add multiple authors by listing them as a vector:

Authors@R: as.person(c(

"Sean McClure <sean.mcclure@example.com> [aut, cre]",

"Rick Deckard <rick.deckard@example.com> [aut]",

" Rachael Tyrell <rachel.tyrell@example.com> [ctb]"

))

NOTE: If you do plan on hosting your library on CRAN be sure your email address is correct, as CRAN will use this to contact you.

The description can be multiple lines, limited to 1 paragraph. We use depends to specify the minimum version of R our package depends on. You should use an R version equal or greater than the one you used to build your library. Most people today set their License to MIT, which permits anyone to ‚Äúuse, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software‚Äù as long as your copyright is included. You can learn more about the MIT license here. Encoding ensures our library can be opened, read and saved using modern parsers, and LazyData refers to how data in our package are loaded. Since we set ours to true it means our data won‚Äôt occupy memory until they are used.

STEP 3: Add Functions

3A: Add Functions to R Folder

Our library wouldn‚Äôt do much without functions. Let‚Äôs add the 4 functions mentioned in the beginning of this article. The following GIST shows our datapeek functions in R:

We have to add our functions to the R folder, since this is where R looks for any functions inside a library.

datapeek

‚îú‚îÄ‚îÄ R

‚îú‚îÄ‚îÄ datapeek.Rproj

‚îú‚îÄ‚îÄ DESCRIPTION

‚îú‚îÄ‚îÄ NAMESPACE

Since our library only contains 4 functions we will place all of them into a single file called utilities.R, with this file residing inside the R folder.

Go into the directory in JupyterLab and open the R folder. Click on Text File in the Launcher and paste in our 4 R functions. Right-click the file and rename it to utilities.R.

3B: Export our Functions

It isn‚Äôt enough to simply place R functions in our file. Each function must be exported to expose them to users of our library. This is accomplished by adding the @export tag above each function.

The export syntax comes from Roxygen, and ensures our function gets added to the NAMESPACE. Let‚Äôs add the @export tag to our first function:

Do this for the remaining functions as well.

NOTE: In larger libraries we would only export functions that need to be usable outside our package. This helps reduce the chances of a conflict with another library.

3C: Document our Functions

It is important to document our functions. Documenting functions provides information for users, such that when they type ?datapeek they get details about our package. Documenting also supports working with vignettes, which are a long-form type of documentation. You can read more about documenting functions here.

There are 2 sub-steps we will take:

add the document annotations

the document annotations run devtools::document()

‚Äî Add the Document Annotations

Documentation is added above our function, directly above our #‚Äô @export line. Here‚Äôs the example with our first function:

We space out the lines for readability, adding a title, description, and any parameters used by the function. Let‚Äôs do this for our remaining functions:

‚Äî Run devtools::document()

With documentation added to our functions we then run the following in terminal, just outside the root directory:

devtools::document()

NOTE: Make sure you‚Äôre one level outside the datapeek directory.

You may get the error:

Error: ‚Äòroxygen2‚Äô >= 5.0.0 must be installed for this functionality.

In this case open terminal in JupyterLab and install roxygen2. You should also install data.table and mltools, since our first function uses these:

install.packages('roxygen2')

install.packages('data.table')

install.packages('mltools')

Run the devtools::document() again. You should see the following:

This will generate .Rd files inside a new man folder. You‚Äôll notice an .Rd file is created for each function in our package.

If you look at your DESCRIPTION file it will now show a new line at the bottom:

This will also generate a NAMESPACE file:

We can see our 4 functions have been exposed. Let‚Äôs now move onto ensuring dependencies are specified inside our library.

STEP 4: List External Dependencies

It is common for our functions to require functions found in other libraries. There are 2 things we must do to ensure external functionality is made available to our library‚Äôs functions:

Use double colons inside our functions to specify which library we are relying on; Add imports to our DESCRIPTION file.

You‚Äôll notice in the above GIST we simply listed our libraries at the top. While this works well in stand-alone R scripts it isn‚Äôt the way to use dependencies in an R package. When creating R packages we must use the ‚Äúdouble-colon approach‚Äù to ensure the correct function is read. This is related to how ‚Äútop-level code‚Äù (code that isn‚Äôt an object like a function) in an R package is only executed when the package is built, not when it‚Äôs loaded.

For example:

library(mltools) do_something_cool_with_mltools <- function() {

auc_roc(preds, actuals)

}

‚Ä¶won‚Äôt work because auc_roc will not be available (running library(datapeek) doesn‚Äôt re-execute library(mltools)). This will work:

do_something_cool_with_mltools <- function() {

mltools::auc_roc(preds, actuals)

}

The only function in our datapeek package requiring additional packages is our first one:

Using the double-colon approach to specify dependent packages in R.

Notice each time we call an external function we preface it with the external library and double colons.

We must also list external dependencies in our DESCRIPTION file, so they are handled correctly. Let‚Äôs add our imports to the DESCRIPTION file:

Be sure to have the imported libraries comma-separated. Notice we didn‚Äôt specify any versions for our external dependencies. If we need to specify versions we can use parentheses after the package name:

Imports:

data.table (>= 1.12.0)

Since our encode_and_bind function isn‚Äôt taking advantage of any bleeding-edge updates we will leave it without any version specified.

STEP 5: Add Data

Sometimes it makes sense to include data inside our library. Package data can allow our user‚Äôs to practice with our library‚Äôs functions, and also helps with testing, since machine learning packages will always contain functions that ingest and transform data. The 4 options for adding external data to an R package are:

binary data parsed data raw data serialized data

You can learn more about these different approaches here. For this article we will stick with the most common approach, which is to add external data to an R folder.

Let‚Äôs add the Iris dataset to our library in order to provide users a quick way to test our functions. The data must be in the .rda format, created using R‚Äôs save() function, and have the same name as the file. We can ensure these criteria are satisfied by using devtools‚Äô use_data function:

Above, I read in the Iris dataset from its URL and pass the data frame to devtools::use_data() .

In JupyterLab we see a new data folder has been created, along with our iris.rda dataset:

datapeek

‚îú‚îÄ‚îÄ data

‚îî‚îÄ‚îÄ iris.rda

‚îú‚îÄ‚îÄ man

‚îú‚îÄ‚îÄ R

‚îú‚îÄ‚îÄ datapeek.Rproj

‚îú‚îÄ‚îÄ DESCRIPTION

‚îú‚îÄ‚îÄ NAMESPACE

We will use our added dataset to run tests in the following section.

STEP 6: Add Tests

Testing is an important part of software development. Testing helps ensure our code works as expected, and makes debugging our code a much faster and more effective process. Learn more about testing R packages here.

A common challenge in testing is knowing what we should test. Testing every function in a large library is cumbersome and not always needed, while not enough testing can make it harder to find and correct bugs when they arise.

I like the following quote from Martin Fowler regarding when to test:

‚ÄúWhenever you are tempted to type something into a print statement or a debugger expression, write it as a test instead.‚Äù ‚Äî Martin Fowler

If you prototype applications regularly you‚Äôll find yourself writing to the console frequently to see if a piece of code returns what you expect. In data science, writing interactive code is even more common, since machine learning work is highly experimental. On one hand this provides ample opportunity to think about which tests to write. On the other hand, the non-deterministic nature of machine learning code means testing certain aspect of ML can be less than straightforward. As a general rule, look for obvious deterministic pieces of your code that should return the same output every time.

The interactive testing we do in data science is manual, but what we are looking for in our packages is automated testing. Automated testing means we run a suite of pre-defined tests to ensure our package works end-to-end.

While there are many kinds of tests in software, here we are taking about ‚Äúunit tests.‚Äù Thinking in terms of unit tests forces us to break up our code into more modular components, which is good practice in software design.

NOTE: If you are used to testing in languages like Python, notice that R is more functional in nature (i.e., methods belong to functions not classes), so there will be some differences.

There are 2 sub-steps we will take for testing our R library:

6A: Creating the tests/testthat folder;

6B: Writing tests.

‚Äî 6A: Creating the tests/testthat folder

Just as R expects our R scripts and data to be in specific folders it also expects the same for our tests. To create the tests folder, we run the following in JupyterLab‚Äôs R console:

devtools::use_testthat()

You may get the following error:

Error: ‚Äòtestthat‚Äô >= 1.0.2 must be installed for this functionality.

If so, use the same approach above for installing roxygen2 in Jupyter‚Äôs terminal.

install.packages('testthat')

Running devtools::use_testthat() will produce the following output:

* Adding testthat to Suggests

* Creating `tests/testthat`.

* Creating `tests/testthat.R` from template.

There will now be a new tests folder in our main directory:

datapeek

‚îú‚îÄ‚îÄ data

‚îú‚îÄ‚îÄ man

‚îú‚îÄ‚îÄ R

‚îú‚îÄ‚îÄ tests

‚îî‚îÄ‚îÄ testthat.R

‚îú‚îÄ‚îÄ datapeek.Rproj

‚îú‚îÄ‚îÄ DESCRIPTION

‚îú‚îÄ‚îÄ NAMESPACE

The above command also created a file called testthat.R inside the tests folder. This runs all your tests when R CMD check runs (we‚Äôll look at that shortly). You‚Äôll also notice testthat has been added under Suggests in our DESCRIPTION file:

‚Äî 6B: Writing Tests

testthat is the most popular unit testing package for R, used by at least 2,600 CRAN package, not to mention libraries on Github. You can check out the latest news regarding testthat on the Tidyverse page here. Also check out its documentation.

There are 3 levels to testing we need to consider:

expectation (assertion): the expected result of a computation;

the expected result of a computation; test: groups together multiple expectations from a single function, or related functionality from across multiple functions;

groups together multiple expectations from a single function, or related functionality from across multiple functions; file: groups together multiple related tests. Files are given a human readable name with context().

Assertions

Assertions are the functions included in the testing library we choose. We use assertions to check whether our own functions return the expected output. Assertions come in many flavors, depending on what is being checked. In the following section I will cover the main tests used in R programming, showing each one failing so you can understand how it works.

Equality Assertions

expect_equal()

expect_identical()

expect_equivalent

# test for equality

a <- 10

expect_equal(a, 14) > Error: `a` not equal to 14. # test for identical

expect_identical(42, 2) > Error: 42 not identical to 2. # test for equivalence

expect_equivalent(10, 12) > Error: 10 not equivalent to 12.

There are subtle differences between the examples above. For example, expect_equal is used to check for equality within a numerical tolerance, while expect_identical tests for exact equivalence. Here are examples:

expect_equal(10, 10 + 1e-7) # true

expect_identical(10, 10 + 1e-7) # false

As you write more tests you‚Äôll understand when to use each one. Of course always refer to the documentation referenced above when in doubt.

Testing for String Matches

expect_match()

# test for string matching

expect_match("Machine Learning is Fun", "But also rewarding.") > Error: "Machine Learning is Fun" does not match "But also rewarding.".

Testing for Length

expect_length

# test for length

vec <- 1:10

expect_length(vec, 12) > Error: `vec` has length 10, not length 12.

Testing for Comparison

expect_lt

expect_gt

# test for less than

a <- 11

expect_lt(a, 10) > Error: `a` is not strictly less than 10. Difference: 1 # test for greater than

a <- 11

expect_gt(a, 12) > Error: `a` is not strictly more than 12. Difference: -1

Testing for Logic

expect_true

expect_false

# test for truth

expect_true(5 == 2) > Error: 5 == 2 isn't true. # test for false

expect_false(2 == 2) > Error: 2 == 2 isn't false.

Testing for Outputs

expect_output

expect_message

# testing for outputs

expect_output(str(mtcars), "31 obs") > Error: `str\(mtcars\)` does not match "31 obs". # test for warning

f <-function(x) {

if(x < 0) {

message("*x* is already negative")

}

} expect_message(f(1)) > Error: `f(1)` did not produce any messages.

There are many more included in the testthat library. If you are new to testing, start writing a few simple ones to get used to the process. With time you‚Äôll build an intuition around what to test and when.

Writing Tests

A test is a group of assertions. We write tests in testthat as follows:

test_that("this functionality does what it should", {

// group of assertions here

})

We can see we have both a description (the test name) and the code (containing the assertions). The description completes the sentence, ‚Äútest that ‚Ä¶.‚Äù

Above, we are saying ‚Äútest that this functionality does what it should.‚Äù

The assertions are the outputs we wish to test. For example:

test_that("trigonometric functions match identities", {

expect_equal(sin(pi / 4), 1 / sqrt(2))

expect_equal(cos(pi / 4), 1 / sqrt(10))

expect_equal(tan(pi / 4), 1)

}) > Error: Test failed: 'trigonometric functions match identities'

NOTE: It is worth considering the balance between cohesion and coupling with our test files. As stated in Hadley‚Äôs book, ‚Äúthe two extremes are clearly bad (all tests in one file, one file per test). You need to find a happy medium that works for you. A good starting place is to have one file of tests for each complicated function.‚Äù

Creating Files

The last thing we do in testing is create files. As stated above, a‚Äúfile‚Äù in testing is a group of tests covering a related set of functionality. Our test file must live inside the tests/testthat/ directory. Here is an example test file for the stringr package on GitHub:

Example Test File from the stringr package on GitHub.

The file is called test-case.R (starts with ‚Äútest‚Äù) and lives inside the tests/testthat/ directory. The context at the top simply allows us to provide a simple description of the file‚Äôs contents. This appears in the console when we run our tests.

Let‚Äôs create our test file, which will contain tests and assertions related to our 4 functions. As usual, we use JupyterLab‚Äôs Text File in Launcher to create and rename a new file:

Creating a Test File in R

Now let‚Äôs add our tests:

For the first function I am going to make sure a data frame with the correct number of features is returned:

Notice how we called our encode_and_bind function, then simply checked the equality between the dimensions and the expected output. We run our automated tests at any point to ensure our test file runs and we get the expected output. Running devtools::test() in the console runs our tests:

We get a smiley face too!

Since our second function removes a specified feature I will use the same test as above, checking for the dimensions of the returned frame. Our third function applies a specified function to a chosen column, so I will write a test that checks the result of given specified function. Finally, our fourth function returns the closest matching string, so I will simply check the returned string for the expected result.

Here is our full test file:

NOTE: Notice the relative path to the data in the test file.

Testing our Package

As we did above, we run our tests using the following command:

devtools::test()

This will run all tests in any test files we placed inside the testthat directory. Let‚Äôs check the result:

We had 5 assertions across 4 unit tests, placed in one test file. Looks like we‚Äôre good. If any of our tests failed we would see this in the above printout, at which point we would look to correct the issue.

STEP 7: Create Documentation

This has traditionally been done using ‚ÄúVignettes‚Äù in R. You can learn about creating R vignettes for your R package here. Personally, I find this a dated approach to documentation. I prefer to use things like Sphinx or Julep. Documentation should be easily shared, searchable and hosted.

Click on the question mark at julepcode.com to learn how to use Julep.

I created and hosted some simple documentation for our R datapeek library, which you can find here.

Of course we will also have the library on GitHub, which I cover below.

STEP 8: Share your R Library

As I mentioned in the introduction we should be creating libraries on a regular basis, so others can benefit from and extend our work. The best way to do this is through GitHub, which is the standard way to distribute and collaborate on open source software projects.

In case you‚Äôre new to GitHub here‚Äôs a quick tutorial to get you started so we can push our datapeek project to a remote repo.

Sign up/in to GitHub and create a new repository.

‚Ä¶which will provide us with the usual screen:

With our remote repo setup we can initialize our local repo on our machine, and send our first commit.

Open Terminal in JupyterLab and change into the datapeek directory:

Initialize the local repo:

git init

Add the remote origin (your link will be different):

git remote add origin https://github.com/sean-mcclure/datapeek.git

Now run git add . to add all modified and new (untracked) files in the current directory and all subdirectories to the staging area:

git add .

Don‚Äôt forget the ‚Äúdot‚Äù in the above command. Now we can commit our changes, which adds any new code to our local repo.

But, since we are working inside a Docker container the username and email associated with our local repo cannot be autodetected. We can set these by running the following in terminal:

git config --global user.email {emailaddress}

git config --global user.name {name}

Use the email address and username you use to sign into GitHub.

Now we can commit:

git commit -m 'initial commit'

With our new code committed we can do our push, which transfers the last commit(s) to our remote repo:

git push origin master

NOTE: Since we are in Docker you‚Äôll likely get asked again for authentication. Simply add your GitHub username and password when prompted. Then run the above command again.

Some readers will notice we didn‚Äôt place a .gitignore file in our directory. It is usually fine to push all files inside smaller R libraries. For larger libraries, or libraries containing large datasets, you can use the site gitignore.io to see what common gitignore files look like. Here is a common R .gitignore file for R:

Example .gitignore file for an R package

To recap, git add adds all modified and new (untracked) files in the current directory to the staging area. Commit adds any changes to our local repo, and push transfers the last commit(s) to our remote repo. While git add might seem superfluous, the reason it exists is because sometimes we want to only commit certain files, this we can stage files selectively. Above, we staged all files by using the ‚Äúdot‚Äù after git add .

You may also notice we didn‚Äôt include a README file. You should indeed include this, however for the sake of brevity I have left this step out.

Now, anyone can use our library. üëç Let‚Äôs see how.

STEP 9: Install your R Library

As mentioned in the introduction I will not be discussing CRAN in this article. Sticking with GitHub make it easier to share our code frequently, and we can always add CRAN criteria later.

To install a library from GitHub, users can simply run the following command on their local machine:

devtools::install_github("yourusername/mypackage")

As such, we can simply instruct others wishing to use datapeek to run the following command on their local machine:

devtools::install_github("sean-mcclure/datapeek")

This is something we would include in a README file and/or any other documentation we create. This will install our package like any other package we get from CRAN:

Users then load the library as usual and they‚Äôre good to go:

library(datapeek)

I recommend trying the above commands in a new R environment to confirm the installation and loading of your new library works as expected.

CREATING LIBRARIES IN PYTHON

Creating Python libraries follows the same high-level steps we saw previously for R. We require a basic directory structure with proper naming conventions, functions with descriptions, imports, specified dependencies, added datasets, documentation, and the ability to share and allow others to install our library.

We will use JupyterLab to build our Python library, just as we did for R.

Library vs Package vs Module

In the beginning of this article I discussed the difference between a ‚Äúlibrary‚Äù and a ‚Äúpackage‚Äù, and how I prefer to use these terms interchangeably. The same holds for Python libraries. ‚ÄúModules‚Äù are another term, and in Python simply refer to any file containing Python code. Python libraries obviously contain modules as scripts.

Before we start:

I stated in the introduction that we will host and install our libraries on and from GitHub. This encourages rapid creation and sharing of libraries without getting bogged down by publishing criteria on popular package hosting sites for R and Python.

The most popular hosting site for Python is the Python Package Index (PyPI). This is a place for finding, installing and publishing python libraries. Whenever you run pip install <package_name> (or easy_intall ) you are fetching a package from PyPI.

While we won‚Äôt cover hosting our package on PyPI it‚Äôs still a good idea to see if our library name is unique. This will minimize confusion with other popular Python libraries and improve the odds our library name is distinctive, should we decide to someday host it on PyPI.

First, we should follow a few naming conventions for Python libraries.

Python Library Naming Conventions

Use all lowercase ;

; Make the name unique on PyPI (search for name on PyPI)

on PyPI (search for name on PyPI) No hyphens (you can use underscore to separate words)

Our library name is datapeek, so the first and third criteria are met; let‚Äôs check PyPI for uniqueness:

All good. üëç

We‚Äôre now ready to move through each step required to create a Python library.

STEP 1: Create Package Framework

JupyterLab should be up-and-running as per the instructions in the setup section of this article.

Use JupyterLab‚Äôs New Folder and Text File options to create the following directory structure and files:

datapeek

‚îú‚îÄ‚îÄ datapeek

‚îî‚îÄ‚îÄ __init__.py

‚îî‚îÄ‚îÄ utilities.py

‚îú‚îÄ‚îÄ setup.py

NOTE: Bold names are folders and light names are files. We will refer to the inner datapeek folder as the ‚Äúmodule directory‚Äù and the outer datapeek directory as the ‚Äúroot directory.‚Äù

The following video shows me creating our datapeek directory in JupyterLab:

There will be files we do not want to commit to source control. These are files that are created by the Python build system. As such, let‚Äôs also add the following .gitignore file to our package framework:

NOTE: At the time of this writing JupyterLab lacks a front-end setting to toggle hidden files in the browser. As such, we will simply name our file gitignore (no preceding dot); we will change it to a hidden file later prior to pushing to GitHub.

Add your gitignore file as a simple text file to the root directory:

datapeek

‚îú‚îÄ‚îÄ datapeek

‚îî‚îÄ‚îÄ __init__.py

‚îî‚îÄ‚îÄ utilities.py

‚îú‚îÄ‚îÄ setup.py

‚îú‚îÄ‚îÄ gitignore

STEP 2: Fill Out Description Details

Just as we did for R, we should add metadata about our new library. We do this using Setuptools. Setuptools is a Python library designed to facilitate packaging Python projects.

Open setup.py and add the following details for our library:

Of course you should change the authoring to your own. We will add more details to this file later. The keywords are fairly self-explanatory. url is the URL of our project on GitHub, which we will add later; unless you‚Äôve already created your python repo, in which case add the URL now. We talked about licensing in the R section. zip_safe simply means our package can be run safely as a zip file which will usually be the case. You can learn more about what can be added to the setup.py file here.

STEP 3: Add Functions

Our library obviously requires functions to be useful. For larger libraries we would organize our modules so as to balance cohesion/coupling, but since our library is small we will simply keep all functions inside a single file.

We will add the same functions we did for R, this time written in Python:

Add these functions to the utilities.py module, inside datapeek‚Äôs module directory.

STEP 4: List External Dependencies

Our library will often require other packages as dependencies. Our user‚Äôs Python environment will need to be aware of these when installing our library (so these other packages can also be installed). Setuptools provides the install_requires keyword to list any packages our library depends on.

Our datapeek library depends on the fuzzywuzzy package for fuzzy string matching, and the pandas package for high-performance manipulation of data structures. To specify our dependencies, add the following to your setup.py file:

install_requires=[

'fuzzywuzzy',

'pandas'

]

Your setup.py file should currently look as follows:

We can confirm all is in order by running the following in a JupyterLab terminal session:

python setup.py develop

NOTE: Run this in datapeek‚Äôs root directory.

After running the command you should see something like this:

‚Ä¶with an ending that reads:

Finished processing dependencies for datapeek==0.1

If one or more of our dependencies is not available on PyPI, but is available on GitHub (e.g. a bleeding-edge machine learning package is only available on Github‚Ä¶or it‚Äôs another one of our team‚Äôs libraries hosted only on GitHub), we can use dependency_links inside our setup call:

setup(

...

dependency_links=['http://github.com/user/repo/tarball/master#egg=package-1.0'],

...

)

If you want to add additional metadata, such as status, licensing, language version, etc. we can use classifiers like this:

setup(

...

classifiers=[

'Development Status :: 3 - Alpha',

'License :: OSI Approved :: MIT License',

'Programming Language :: Python :: 2.7',

'Topic :: Text Processing :: Linguistic',

],

...

)

To learn more about the different classifiers that can be added to our setup.py file see here.

STEP 5: Add Data

Just as we did above in R we can add data to our Python library. In Python these are called Non-Code Files and can include things like images, data, documentation, etc.

We add data to our library‚Äôs module directory, so that any code that requires those data can use a relative path from the consuming module‚Äôs __file__ variable.

Let‚Äôs add the Iris dataset to our library in order to provide users a quick way to test our functions. First, use the New Folder button in JupyterLab to create a new folder called data inside the module directory:

datapeek

‚îú‚îÄ‚îÄ datapeek

‚îî‚îÄ‚îÄ __init__.py

‚îî‚îÄ‚îÄ utilities.py

‚îî‚îÄ‚îÄ data

‚îú‚îÄ‚îÄ setup.py

‚îú‚îÄ‚îÄ gitignore

‚Ä¶then make a new Text File inside the data folder called iris.csv, and paste the data from here into the new file.

If you close and open the new csv file it will render inside JupyterLab as a proper table:

CSV file rendered in JupyterLab as formatted table.

We specify Non-Code Files using a MANIFEST.in file. Create another Text File called MANIFEST.in placing it inside your root folder:

datapeek

‚îú‚îÄ‚îÄ datapeek

‚îî‚îÄ‚îÄ __init__.py

‚îî‚îÄ‚îÄ utilities.py

‚îî‚îÄ‚îÄ data

‚îú‚îÄ‚îÄ MANIFEST.in

‚îú‚îÄ‚îÄ setup.py

‚îú‚îÄ‚îÄ gitignore

‚Ä¶and add this line to the file:

include datapeek /data/iris.csv

NOTE: The MANIFEST.in is often not needed, but included in this tutorial for completeness. See here for more discussion.

We also need to include the following line in setup.py:

include_package_data=True

Our setup.py file should now look like this:

STEP 6: Add Tests

As with our R library we should add tests so others can extend our library and ensure their own functions do not conflict with existing code. Add a test folder to our library‚Äôs module directory:

datapeek

‚îú‚îÄ‚îÄ datapeek

‚îî‚îÄ‚îÄ __init__.py

‚îî‚îÄ‚îÄ utilities.py

‚îî‚îÄ‚îÄ data

‚îî‚îÄ‚îÄ tests

‚îú‚îÄ‚îÄ MANIFEST.in

‚îú‚îÄ‚îÄ setup.py

‚îú‚îÄ‚îÄ gitignore

Our test folder should have its own __init__.py file as well as the test file itself. Create those now using JupyterLab‚Äôs Text File option:

datapeek

‚îú‚îÄ‚îÄ datapeek

‚îî‚îÄ‚îÄ __init__.py

‚îî‚îÄ‚îÄ utilities.py

‚îî‚îÄ‚îÄ data

‚îî‚îÄ‚îÄ tests

‚îî‚îÄ‚îÄ __init__.py

‚îî‚îÄ‚îÄ datapeek_tests.py

‚îú‚îÄ‚îÄ MANIFEST.in

‚îú‚îÄ‚îÄ setup.py

‚îú‚îÄ‚îÄ gitignore

Our datapeek directory structure is now set to house test functions, which we will write now.

Writing Tests

Writing tests in Python is similar to doing so in R. Assertions are used to check the expected outputs produced by our library‚Äôs functions. We can use these ‚Äúunit tests‚Äù to check a variety of expected outputs depending on what might be expected to fail. For example, we might want to ensure a data frame is returned, or perhaps the correct number of columns after some known transformation.

I will add a simple test for each of our 4 functions. Feel free to add your own tests. Think about what should be checked, and keep in mind Martin Fowler‚Äôs quote shown in the R section of this article.

We will use unittest, a popular unit testing framework in Python.

Add unit tests to the datapeek_tests.py file, ensuring the unittest and datapeek libraries are imported:

To run these tests we can use Nose, which extends unittest to make testing easier. Install nose using a terminal session in JupyterLab:

$ pip install nose

We also need to add the following lines to setup.py:

setup(

...

test_suite='nose.collector',

tests_require=['nose'],

)

Our setup.py should now look like this:

Run the following from the root directory to run our tests:

python setup.py test

Setuptools will take care of installing nose if required and running the test suite. After running the above, you should see the following:

All our tests have passed!

If any test should fail, the unittest framework will show which functions did not pass. At this point, check to ensure you are calling the function correctly and that the output is indeed what you expected. It can also be good practice to purposely write tests to fail first, then write your functions until they pass.

STEP 7: Create Documentation

As I mentioned in the R section, I use Julep to rapidly create sharable and searchable documentation. This avoids writing cryptic annotations and provides the ability to immediately host our documentation. Of course this doesn‚Äôt come with the IDE hooks that other documentation does, but for rapidly communicating it works.

You can find the documentation I create for this library here.

STEP 8: Share Your Python Library

The standard approach for sharing python libraries is through PyPI. Just as we didn‚Äôt cover CRAN with R, we will not cover hosting our library on PyPI. While the requirements are fewer than those associated with CRAN there are still a number of steps that must be taken to successfully host on PyPI. The steps required to host on sites other than GitHub can always be added later.

GitHub

We covered the steps for adding a project to GitHub in the R section. The same steps apply here.

I mentioned above the need to rename our gitignore file to make it a hidden file. You can do that by running the following in terminal:

mv gitignore .gitignore

You‚Äôll notice this file is no longer visible in our JupyterLab directory (it eventually disappears). Since JupyterLab still lacks a front-end setting to toggle hidden files simply run the following in terminal at anytime to see hidden files:

ls -a

We can make it visible again should we need to view/edit the file in JupyterLab, by running:

mv .gitignore gitignore

Here is a quick recap on pushing our library to GitHub (change git URL to your own):

Create a new repo on GitHub called datapeek_py

a new repo on GitHub called datapeek_py Initialize your library‚Äôs directory using git init

your library‚Äôs directory using Configure your local repo with your GitGub email and username (if using Docker) using:

git config --global user.email {emailaddress}

git config --global user.name {name}

Add your new remote origin using git remote add origin https://github.com/sean-mcclure/datapeek_py.git

your new remote origin using Stage your library using git add .

your library using Commit all files using git commit -m ‚Äòinitial commit‚Äô

all files using Push your library to the remote repo using git push origin master (authenticate when prompted)

Now, anyone can use our python library. üëç Let‚Äôs see how.

STEP 9: Install your Python Library

While we usually install Python libraries using the following command:

pip install <package_name>

‚Ä¶ this requires hosting our library on PyPI, which as explained above is beyond the scope of this article. Instead we will learn how to install our Python libraries from GitHub, as we did for R. This approach still requires the pip install command but uses the GitHub URL instead of the package name.

Installing our Python Library from GitHub

With our library hosted on GitHub, we simply use pip install git+ followed by the URL provided on our GitHub repo (available by clicking the Clone or Download button on the GitHub website):

pip install git+https://github.com/sean-mcclure/datapeek_py

Now, we can import our library into our Python environment. For a single function:

from datapeek.utilities import encode_and_bind

‚Ä¶and for all functions:

from datapeek.utilities import *

Let‚Äôs do a quick check in a new Python environment to ensure our functions are available. Spinning up a new Docker container, I run the following:

Fetch a dataset:

Check functions:

encode_and_bind(iris, 'species')

remove_features(iris, ['petal_length', 'petal_width'])

apply_function_to_column(iris, ['sepal_length'], 'times_4', 'x*4')

get_closest_string(['hey there','we we are','howdy doody'], 'doody')

Success!

SUMMARY

In this article we looked at how to create both R and Python libraries using JupyterLab running inside a Docker container. Docker allowed us to leverage Docker Stacks such that our environment was easily controlled and common packages available. This also made it easy to use the same high-level interface to create libraries through the browser for 2 different languages. All files were written to our local machine since we mounted a volume inside Docker.

Creating libraries is a critical skill for any machine learning practitioner, and something I encourage others to do regularly. Libraries help isolate our work inside useful abstractions, improves reproducibility, makes our work shareable, and is the first step towards designing better software. Using a lightweight approach ensures we can prototype and share quickly, with the option to add more detailed practices and publishing criteria later as needed.

As always, please ask questions in the comments section should you run into issues. Happy coding.

If you enjoyed this article you might also enjoy:

FURTHER READING AND RESOURCES
Predictive Maintenance: detect Faults from Sensors with CNN

Photo by Bruce Warrington on Unsplash

In Machine Learning the topic of Predictive Maintenance is becoming more popular with the passage of time. The challenges are not easy and very heterogenous: it‚Äôs useful to have a good knowledge of the domain or to be in touch with people who know how the underlying system works. For these reasons when a data scientist engages himself in this new field of battle has to follow a linear and rational approach, keeping in mind that the easiest solutions are always the better ones.

In this article, we will take a look at a classification problem. We will apply a simple but very powerful model made with CNN in Keras and we will try to give a visual explanation of our results.

THE DATASET

I decided to take a dataset from the evergreen UCI repository (Condition monitoring of hydraulic systems).

The data set was experimentally obtained with a hydraulic test rig. This test rig consists of a primary working and a secondary cooling-filtration circuit which are connected via the oil tank. The system cyclically repeats constant load cycles (duration 60 seconds) and measures process values such as pressures, volume flows and temperatures while the condition of four hydraulic components (cooler, valve, pump and accumulator) is quantitatively varied.

We can image to have a hydraulic pipe system which cyclically receives impulse due to e.g. the transition of particular type of liquid in the pipeline. This phenomenon lasts 60 seconds and was measured by different sensors (Sensor Physical quantity Unit Sampling rate, PS1 Pressure bar, PS2 Pressure bar, PS3 Pressure bar, PS4 Pressure bar, PS5 Pressure bar, PS6 Pressure bar, EPS1 Motor power, FS1 Volume flow, FS2 Volume flow, TS1 Temperature, TS2 Temperature, TS3 Temperature, TS4 Temperature, VS1 Vibration, CE Cooling efficiency, CP Cooling power, SE Efficiency factor) with different Hz frequencies.

Our purpose is to predict the condition of four hydraulic components which compose the pipeline. These target condition values are annotated in the form of integer values (easy to encode) and say us if a particular component is‚Ä¶
Ethics in Machine Learning

Most of the time, machine learning does not touch on particularly sensitive social, moral, or ethical issues. Someone gives us a data set and asks us to predict house prices based on given attributes, classifying pictures into different categories, or teaching a computer the best way to play PAC-MAN ‚Äî what do we do when we are asked to base predictions of protected attributes according to anti-discrimination laws?

How do we ensure that we do not embed racist, sexist, or other potential biases into our algorithms, be it explicitly or implicitly?

It may not surprise you that there have been several important lawsuits in the United States on this topic, possibly the most notably one involving Northpointe‚Äôs controversial COMPAS ‚Äî Correctional Offender Management Profiling for Alternative Sanctions ‚Äî software, which predicts the risk that a defendant will commit another crime. The proprietary algorithm considers some of the answers from a 137-item questionnaire to predict this risk.

In February 2013, Eric Loomis was found driving a car that had been used in a shooting. He was arrested and pleaded guilty to eluding an officer. In determining his sentence, a judge looked not just to his criminal record, but also to a score assigned by a tool called COMPAS.

COMPAS is one of several risk-assessment algorithms now used around the United States to predict hot spots of violent crime, determine the types of supervision that inmates might need, or ‚Äî as in Loomis‚Äôs case ‚Äî provide information that might be useful in sentencing. COMPAS classified him as high-risk of re-offending, and Loomis was sentenced to six years.

He appealed the ruling on the grounds that the judge, in considering the outcome of an algorithm whose inner workings were secretive and could not be examined, violated due process. The appeal went up to the Wisconsin Supreme Court, who ruled against Loomis, noting that the sentence would have been the same had COMPAS never been consulted. Their ruling, however, urged caution and skepticism in the algorithm‚Äôs use.

The case, understandably, caused quite a stir in the machine learning community ‚Äî I doubt anyone would want to be judged by an algorithm, after all, you cannot blame an algorithm for being unethical, can you?
While still in Beta, BigQuery ML has been available since mid last year; however, I didn‚Äôt get around to working with this Google cloud-based Machine Learning offering until recently. As a non-data scientist, my first impression ‚Äî what‚Äôs not to like? After all, the ability to run ML models from the comfort of web-based SQL editor is a dream come-true for any analyst out there. Not only this platform eliminates the need to learn a programming language, be it R , Python or SAS; it also streamlines data engineering process by leveraging existing BigQuery data sources, instead of having to bring external data into your model. Effectively, this product removes a number of barriers to entry into this coveted data science specialty and democratizes the field of ML by allowing any analyst with adequate knowledge of SQL to run linear and logistic regression models without having to invest in pricey hardware, such as multi-core GPUs usually needed to support a scalable ML project. Below image does a great job showcasing platform‚Äôs capabilities:

BigQuery ML demo from Google AI Blog

Many aspiring data science students turn to the trusted Titanic: Machine Learning from Disaster data set from one of the most popular Kaggle competitions to practice working with binary classification models. In fact, for a beginner, a binary classification model is a fairly easy concept to grasp: your task is to simply predict whether a certain event will occur or will not happen; or whether a certain condition will evaluate to be true or false. For this problem, anyone is able to wrap their head around the concept of predicting whether a particular Titanic ship passenger survives one of the most monumental ship wrecks of all times: there are only two possibilities here. To follow along you simply need to log in to existing Kaggle account or create a new one and download all three files provides. The irony of using Kaggle website (purchased by Google back in 2017) and BigQuery platform (another Google product) is not lost on me. Let‚Äôs dive into this problem using sample data set and a working BigQuery instance.
Review: DeepPose ‚Äî Cascade of CNN (Human Pose Estimation)

In this story, DeepPose, by Google, for Human Pose Estimation, is reviewed. It is formulated as a Deep Neural Network (DNN)-based regression problem towards body joints. With a cascade of DNN, high precision pose estimates are achieved. This is a 2014 CVPR paper with more than 900 citations. (Sik-Ho Tsang @ Medium)
Image used under licence from Getty Images.

Microsoft Introduction to AI ‚Äî Part 1

Are you a bit like me and have wanted to learn about Artificial Intelligence although felt a little intimidated by the maths involved? Maybe you thought the concepts were too difficult to understand and you would be out of your depth. I recently completed the Microsoft Introduction to AI course and wrote course notes to help me retain the knowledge that I have learned. I have tried to write these notes in a basic way to make them easy to consume. I‚Äôve recently become an aunt and have bought a few children‚Äôs books related to technology and space, I really love how the authors and illustrators have managed to simplify complicated topics. So, I‚Äôve been inspired to treat these topics in a similar way by simplifying them to make them a lot more accessible.

*If you would like to know a little background info behind the course notes and other notes related to tech and product design you can find out more here.*

Summary

The Microsoft Introduction to AI course provides an overview of AI and explores machine learning principles that provide the foundation for AI. From the course you can discover the fundamental techniques that you can use to integrate AI capabilities into your apps. Learn how software can be used to process, analyse and extract meaning from natural language. Find out how software processes images and video to understand the world the way humans do. Learn about how to build intelligent bots that enable conversations between humans and AI systems.

Image created by the author. Microsoft Introduction to Artificial Intelligence Course

The course takes approximately 1 month to complete so 1 medium article I write contains 1 week's worth of content. This means that it would only take you approximately 18 minutes to read 1 week worth of content which is a fast way of learning. The course is free without a certificate however, if you‚Äôd like a certificate as proof of completion there is a fee. There are labs associated with this course that I won‚Äôt include in the notes as I believe the best way to learn is to actually do the labs. However, these notes are useful if you‚Äôd like to know about the fundamental theory behind AI and would like to learn it in a way that might be a lot simpler than other resources. I‚Äôve tried to write it in layman terms and have included visuals to help illustrate the ideas. These notes are useful if you don‚Äôt have time to do the course, it‚Äôs a quick way to skim through the core concepts. Alternatively, if you have done the course like me you can use these notes to retain what you have learned.

Instructor: Graeme Malcolm ‚Äî Senior Content Developer at Microsoft Learning Experiences.

Syllabus

The course is broken into the four parts which include:

1. Machine Learning (*this medium article will focus on just this section)

Learn about the fundamentals about AI and machine learning.

Learn how software can be used to process, analyse and extract meaning from natural language.

Learn how software can be used to process images and video to understand the world the way that we do.

Find out how to build intelligent bots that enable conversational communication between humans and AI systems.

Image created by the author.

Machine Learning

The ‚ÄòMachine Learning‚Äô part of the course will tackle the following topics:

¬∑ What is Artificial Intelligence? ¬∑ What is Machine Learning? ¬∑ Supervised ¬∑ Unsupervised ¬∑ Regression ¬∑ Classification ¬∑ Clustering

Illustration by Michael Korfhage for HR Magazine SHRM.

What is Artificial Intelligence?

Artificial Intelligence (AI) is a way to enable people to accomplish more by collaborating with smart software. Think of it as putting a more human face on technology. AI is technology that can learn from vast amounts of data that is available in the modern world. Learning from this data it can understand our human kind of language and can respond in a similar kind of way. It‚Äôs technology that can see and interpret the world the way that we humans do.

Illustration by Justin Middendorp.

What is Machine Learning?

Machine learning (ML) provides the foundation for artificial intelligence.

So what is it?

Machine learning gives computers the ability to learn and make predictions or decisions based on data without explicitly programming that in. Well as the name suggests it‚Äôs a technique in which we train a software model using data. A model is a mathematical representation of a real-world process. The model learns from the training cases (training situations or examples) and then we can use the trained model to make predictions for new data cases. The key to this is to understand that fundamentally computers are very good at one thing and that is performing calculations. To have a computer make intelligent predictions from the data, we just need a way to train it to perform the correct calculations.

We start with a dataset that contains historical records which we often call ‚Äòcases‚Äô or ‚Äòobservations‚Äô. Each observation includes numeric features. Numeric features are basically characteristics of the item we‚Äôre working with and they have a numeric value attached to each characteristic.

Illustration by Vecteezy.

Let‚Äôs call the numeric feature X.

In general, we also have some value that we‚Äôre trying to predict which we‚Äôll call that Y. We use our training cases to train a machine learning model so that it can calculate a value for Y from the features in X. So in very simplistic terms, we‚Äôre creating a function that operates on a set of features ‚ÄòX‚Äô, to produce predictions ‚ÄòY‚Äô. Don‚Äôt worry if this is confusing it will make more sense in the next sections where we start to apply real world examples.

Now generally speaking, there are two broad types of machine learning and they are called supervised and unsupervised.

Supervised

In supervised learning scenarios, we start with the observations that include known values for the variable that we want to predict. We call these ‚Äòlabels‚Äô. Since we started with data that includes the label we‚Äôre trying to predict, we can train the model using only some of the data and withhold the rest of the data which we can use to evaluate the performance of the model. We then use a machine learning algorithm to train a model that fits the features to the known label.

Since we started with the known label value we can validate the model by comparing the value predicted by the function to the actual label value that we knew. Then when we‚Äôre happy that the model works well, we can use it with new observations for which the label is unknown and generate new predicted values.

In this example we know the value of both X (numeric feature) and Y (variable we want to predict). Since we know X and Y we can use this algorithm to train our model. Once the model has been trained and we are happy that it works well we can use this model to calculate Y for when X is unknown. Illustration by Vecteezy.

Unsupervised

Unsupervised learning is different from supervised learning, in that this time we don‚Äôt have the known label values in the training dataset. We train the model by finding similarities between the observations. After the model is trained, each new observation is assigned to the cluster of observations with the most similar characteristics.

In this example the value Y is unknown and so the way we train the model is through finding similarities between the observations. The observations are categorised in clusters that have similar characteristics. Once we train the model based on these clusters we can use it to predict the value of Y by assigning a new observation to a cluster. Illustration by Vecteezy.

Regression

Okay, let‚Äôs start with a supervised learning technique called ‚ÄòRegression‚Äô. Imagine we have some historic data about some health trials participants. We have information such as the exercise they have done, the number of calories they have spent and a lot more stats and info about them. In this case we could use machine learning to predict how many calories any new participants might be expected to burn while engaging in some exercises. When we need to predict a numeric value, like for example an amount of money or a temperature or the number of calories then what we use is a supervised learning technique called regression.

For example, let‚Äôs suppose Rosy here is a participant in our health study. Here she is doing some weight exercises. We gather some data about Rosy when she first signed up for the study. We also gather data while she‚Äôs exercising and capture data using a fitness monitor smart watch.

Now what we want do is model the calories burned using the features we have for Rosy‚Äôs exercise. These numeric features (X) are her age, weight, heart rate, duration, and so on. In this case we know all of the features and we know the label value (Y) of 231 calories. So we need an algorithm to learn the function that operates all of Rosy‚Äôs exercise features to give us a result of 231.

Illustration by Vecteezy.

Now of course a sample of only one person isn‚Äôt likely to give us a function that generalises well. So what we need to do is gather the same sort of data from lots of diverse participants and train our model based on this larger set of data.

Illustration by Vecteezy.

After we‚Äôve trained the model and we have a generalised function that can be used to calculate our label Y, we can then plot the values of Y calculated for specific features of X values on a chart like this.

Image created by the author.

We can then interpolate any new values of X to predict an unknown Y.

Image created by the author.

Now because we started with data that includes the label we are trying to predict, we can train the model using only some of the data and withhold the rest of the data for evaluating model performance.

Then we can use the model to predict (F(X)) for evaluation data and compare the predictions or scored labels to the actual labels that we know to be true. The difference between the predicted and actual levels are what we call the ‚Äòresiduals‚Äô. Residuals can tell us something about the level of error in the model.

Image created by the author.

Now there are a few ways we can measure the error in the model and these include root-mean-square error (RMSE) and mean absolute (MAE). Now both of these are absolute measures of error in the model.

Image created by the author.

For example an RMSE value of 5 would mean that the standard deviation of error from our test error is 5 calories. Of course absolute values can vary wildly depending on what you are predicting. An error of 5 calories would seem to indicate a reasonably good model. But if we were predicting how long an exercise session takes an error of 5 hours would indicate a very bad model.

So you might want to evaluate the model using relative metrics to indicate a more general level of error as a relative value between 0 and 1. Relative absolute error (RAE) and relative squared error (RSE) produce a metric where the closer to 0 the error, the better the model.

Image created by the author.

The coefficient of determination (CoD(R2)) which we sometimes call R squared is another relative metric but this time a value closer to 1 indicates a good fit for the model.

Image created by the author.

Classification

So we‚Äôve seen how to train a regression model to predict a numeric value. Now it‚Äôs time to look at another kind of supervised learning called classification. Classification is a technique that we can use to predict which class or category that something belongs to. The simplest variant of this is binary classification (ones and zeros) where we predict whether an entity belongs to one of two classes. It‚Äôs often used to determine if something is true or false about the entity.

For example, suppose we take a number of patients in our health clinic and we gather some personal details. We run some tests and we can identify which patients are diabetic and which are not. We can learn a function that can be applied to these patient features and give the result 1 for patients that are diabetic and 0 for patients that aren‚Äôt.

Illustration by Vecteezy.

More generally, a binary classifier is a function that can be applied to features X to produce a Y value of 1 or 0.

Illustration by Vecteezy.

Now the function won‚Äôt actually calculate an absolute value of 1 or 0, it will calculate a value between 1 and 0. We will use a threshold value (dotted line in diagram) to decide whether the result should be counted as a 1 or a 0.

The threshold is represented as the dotted line. Image created by the author.

When you use the model to predict values, the resulting value is classed as a 1 or a 0 depending on which side of the threshold line it falls.

Image created by the author.

Because classification is a supervised learning technique we withhold some of the test data to validate the model using known labels.

Image created by the author.

Cases where the model predicts a 1 for a test observation that actually has a label value of 1 these are considered true positives (TP).

Image created by the author.

Similarly cases where the model predicts 0 and the actual label is 0 these are true negatives (TN).

Image created by the author.

Now the choice of threshold determines how predictions are assigned to classes. In some cases a predicted value might be very close to the threshold but is still misclassified. You can move the threshold to control how the predicted values are classified. In the case of the diabetes model it might be better to have more false positives (FP) but reduce the number of false negatives (FN) so that more people who are at risk of diabetes get identified.

Image created by the author.

The number of true positives (TP), false positives (FP), true negatives (TN), and false negative (FN) produced by your model is crucial in evaluating its effectiveness.

Image created by the author.

The grouping of these are often shown in what‚Äôs called a confusion matrix shown below. This provides the basis for calculating performance metrics for the classifier. The simplest metric is accuracy which is just the number of correctly classified cases divided by the total number of cases.

Image created by the author.

In this case there are 5 true positives (TP) and 4 true negatives (TN). There are also 2 false positives (FP) and no false negatives (FN). That gives us 9 correct predictions out of a total of 11 which is an accuracy of 0.82 or 82%.

Image created by the author.

Now that might seem like a really good result but perhaps surprisingly accuracy actually isn‚Äôt all that useful as a measure of a model‚Äôs performance. Suppose that only 3% of the population is diabetic. I can create a model that simply always predicts zero and it would be 97% accurate but it‚Äôs completely useless for identifying potential diabetics.

A more useful metric might be the fraction of cases classified as positive that are actually positive. This metric‚Äôs known as precision. In other words out of all the cases classified as positives which ones are true and not false alarms.

Image created by the author.

In this case there are 5 true positives, and 2 false positives. So our precision is 5 / (7) which is 0.71 or 71% of our cases identified as positive are really diabetic and 29% are false alarms.

Image created by the author.

In some situations we might want a metric that‚Äôs sensitive to the fraction of positive cases we correctly identify. We call this metric recall. Recall is calculated as the number of true positives divided by the combined true positives and false negatives. In other words, what fraction of positive cases are correctly identified?

Image created by the author.

In this case, there are 5true positives and no false negatives. So our recall is 5 out of 5 which is of course is 1 or 100%. So in this case we‚Äôre correctly identifying all patients with diabetes. Now recall actually has another name sometimes it‚Äôs known as the True Positive Rate.

Image created by the author.

There‚Äôs an equivalent rate for false positives compared to the actual number of negatives. In this case we have 2 false positives and 4 true negatives. So our false positive rate is 2/(6) which is 0.33.

Image created by the author.

Now you may remember that the metrics we got were based on a threshold (blue dotted line) of around 0.3 and we can plot the true positive rate against the false positive rate for that threshold like this.

Image created by the author.

If we were to move the threshold back to 0.5 our true positive rate becomes 4 out of 5 or 0.8. Our false positive rate is 1 out of 6 or 0.16 which we can plot here.

Image created by the author.

Moving the threshold further to say 0.7 gives us a true positive rate of 2 out of 5 or a 0.4 and a false positive rate of 0 out of 6 or 0.

Image created by the author.

If we plotted this for every possible threshold rate we would end up with a curved line as shown in the diagram below. Now this is known as a receiver operator characteristic, a ROC chart. Now the area under the curve (AUC) is an indication of how well the model predicts. Generally, you want to see a large AUC with a curve staying as close as possible to the top left corner of the chart. A perfect classifier would go straight up the left and then along the top giving an AUC of one. Now, you can always compare with a diagonal line and that represents how well the model would perform if you simply made a 50‚Äì50 guess. It‚Äôs an AUC of 0.5. So you‚Äôre just simply random guessing 50% of the time true, 50% false. In this case, our model has an AUC of 0.9 which means that our model is definitely outperforming guessing.

The area under the curve (AUC) is an indication of how well the model predicts. Generally, you want to see a large AUC with a curve staying as close as possible to the top left corner of the chart. What is shown in the blue graph is a good example of a model that is outperforming a 50‚Äì50 guess. Image created by the author.

Clustering

Well, we‚Äôve seen some examples of supervised learning specifically regression and classification but what about unsupervised learning? Now with unsupervised learning techniques you don‚Äôt have a known label with which to train the model. But you can still use an algorithm that finds similarities in data observations in order to group them into clusters.

Suppose for example our health clinic has a website that contains links to articles and medical and healthy lifestyle publications. Now I might want to automatically group similar articles together.

Illustration by Vecteezy.

Or maybe I want to segment our study participants and we can categorise them based on similar characteristics.

Illustration by Vecteezy.

There are a number of ways we can create a clustering model and we‚Äôre going to look at one of the most popular clustering techniques which is called k-means clustering.

Image created by the author.

Now the key to understanding k-means is to remember that our data consists of rows of data and each row has multiple features. Now if we assume that each feature is a numeric value then we can plot them as coordinates. Now here we‚Äôre plotting two features on a two dimensional grid. But in reality, multiple features would be plotted in n-dimensional space.

We then decide how many clusters we want to create which we call k. We plot k points at random locations that represent the center points of our clusters.

k points are represented as the stars in the diagram. Image created by the author.

In this case, k is 3 so we‚Äôre creating 3 clusters. Next, we identify which of the three centroids each point is closest to and assign the points to clusters accordingly.

Image created by the author.

Then we move each centroid to the true center of the points and its cluster.

Image created by the author.

We then reallocate the points in the cluster based on their nearest centroid.

Image created by the author.

We just repeat that process until we have nicely separated clusters.

Image created by the author.

So what do I mean by nicely separated? Well, we want a set of clusters that separate data by the greatest extent possible. To measure this we can compare the average distance between the cluster centers.

Image created by the author.

In addition, the average distance between the points in the cluster and their centers.

Image created by the author.

Clusters that maximise this ratio have the greatest separation. We can also use the ratio of the average distance between clusters and the maximum distance between the points and the centroid of the cluster.

Image created by the author.

Now another way we could evaluate the results of a clustering algorithm is to use a method called principal component analysis (PCA). In this method we decompose the points in a cluster into directions. We represent the first two components of the PCA decomposition as an ellipse.

Image created by the author.

The first principal component is along the direction of the maximum variance or major axis of the ellipse and the second PCA is along the minor axis of the ellipse. A cluster that is perfectly separate from the first cluster shows up as an ellipse with the major axis of the ellipse perpendicular to the ellipse of the first cluster.

Image created by the author.

Every second cluster is reasonably well separated but not perfectly separated.

Image created by the author.

Then it will have a major axis that is not quite perpendicular to the first ellipse. If the second cluster is quite poorly separated from the first then the major axis of both ellipses will be nearly collinear.

Image created by the author.

So the ellipse may be more like a circle because the second cluster is less well defined.

Final Word

Thanks for reading this article, Part 1 of the Microsoft Introduction to Artificial Intelligence course. If you found this helpful then check out all 4 parts on my Medium account or in Towards Data Science. If you had some trouble with some of the concepts in this article (don‚Äôt worry it took me awhile for the information to sink in) and you need a bit more info, then enrol for free in the Microsoft Introduction to AI course. It‚Äôs helpful to watch the course videos alongside with these notes.

*If you would like to know a little background info behind the course notes and other notes related to tech and product design you can find out more through here.*

A little background

Hi, I‚Äôm Christine :) I‚Äôm a product designer who‚Äôs been in the digital field for quite some time and have worked at many different companies; from large companies (as large as 84,000 employees), to mid size and to very small startups still making a name for themselves. Despite having a lot of experience I‚Äôm a product designer who has a fear of suffering from the dunning-kruger effect and so I‚Äôm continuously trying to educate myself and I‚Äôm always searching for more light. I believe to be a great designer you need to constantly hone your skills especially if you are working in the digital space which is constantly in motion.
When we face computer vision project, first of all we need to load the images before any preprocessing.

There are various libraries out there to perform imread() . Here I want to consolidate the popular libraries for loading image and their difference. This article will go through:

Libraries for loading image Colour channel Efficiency Cheatsheet!

Library for loading image

There are four libraries that are usually used for loading images.

Matplotlib ‚Äî plt.imread()

OpenCV ‚Äî cv2.imread()

Pillow ‚Äî Image.open()

scikit-image ‚Äî io.imread()

import matplotlib.pyplot as plt img = plt.imread(img_dir) import cv2 img = cv2.imread(img_dir)

from PIL import Image img = Image.open(img_dir)

from skimage import io img = io.imread(img_dir)



Colour channel

After loading the image, usually plt.imshow(img) will be used to plot the images. Let‚Äôs plot some doge!

You may spot that the OpenCV image above looks odd. It is because matplotlib, PIL and skimage represent image in RGB (Red, Green, Blue) order, while OpenCV is in reverse order! (BGR ‚Äî Blue, Green, Red)

Easy Fix
Repetition in Songs: A Python Tutorial

Credit: Unsplash

Everyone has heard a song or knows what a song sounds like. I can carelessly say everyone can define a song ‚Ä¶in their own words. Just for the benefit of the doubt, a song (according to Wikipedia) is a single work of music that is typically intended to be sung by the human voice with distinct and fixed pitches and patterns using sound and silence and a variety of forms that often include the repetition of sections.

In his journal article called ‚ÄúThe complexity of Songs‚Äù, computer scientist Donald Knuth capitalized on the tendency of popular songs to devolve from long and content-rich ballads to highly repetitive texts. As some may waste no time agreeing with his notion, it does raise some questions like: Does repetitiveness really help songs become a hit? Is music really becoming more repetitive over time?

In an attempt to teach some basic python code in the form of a case study, I am going to test this hypothesis (Are popular songs really repetitive?)with one of my favorite songs. One way to test this hypothesis is to figure out the unique words and calculate the fraction of those words to the total number of words in a song.

In this tutorial, we‚Äôll cover:

Variables and data types

Lists and Dictionaries

Basic Arithmetic operations

Built-in Functions and Loops

Prerequisite Knowledge

To get the most out of this tutorial, you can follow along by running the codes yourself.

The music we will be using is entitled ‚ÄòPerfect‚Äô by Ed Sheeran. You can copy the lyrics here. However, the lyrics I am using in this analysis was cleaned out to get a conclusive result. For example, I changed words like ‚Äúwe‚Äôll‚Äù to ‚Äúwe will‚Äù etc. You can get my version of the lyrics here The editor used was Jupiter NoteBook. Here is a quick tutorial on how to install and use it.

For the purpose of this case study, we will streamline our hypothesis by asking two major questions:

How many unique words were used compared to the whole lyrics of our case study song ‚Äî Perfect by Ed Sheeran?

What are the most repetitive words used and how many times were they used throughout the song?

Let's get started analyzing already

The Basic

1. A String is a list of characters. A character is anything you can type on the keyboard in one keystroke, like a letter, a number, or a backslash. However, Python recognizes strings as anything that is delimited by quotation marks either a double quote (‚Äú ‚Äú) or a single quote (‚Äò ‚Äò) at the beginning and end of a character or text. For example: ‚ÄòHello world‚Äô

For this case study, a string is our lyrics as seen below

2. Variables are typically descriptive names, words or symbols used to assign or store values. In other words, they are storage placeholders for any datatype. It is quite handy in order to refer to a value at any time. A variable is always assigned with an equal sign, followed by the value of the

variable. (A way to view your code output is to use a print function. As you may already know with Jupyter notebook, an output can be viewed without a print function)

To store the lyrics, we will assign it a variable named perfect_lyrics .

3. Lists can be created simply by putting different comma-separated values between square brackets. It can have any number of items and they may be of different types (integer, float, string etc.). It can even have another list as an item. For example:

list1 = [1,'mouse', 3.5, [2,'is',5.0]]

#3.5 is a float

Now that we have gotten a sense of what a list looks like. Let go back to our data.

Since one of our aims is to figure out the number of unique words used, it means we will need to do a bit of counting i.e to count each word. In order to achieve these, we will not only have to put our string into a list but will have to separate each word using a .split() method. Therefore our dataset will look like this

Input

Output
Transfer Learning Intuition for Text Classification

This post is the fourth post of the NLP Text classification series. To give you a recap, I started up with an NLP text classification competition on Kaggle called Quora Question insincerity challenge. So I thought to share the knowledge via a series of blog posts on text classification. The first post talked about the different preprocessing techniques that work with Deep learning models and increasing embeddings coverage. In the second post, I talked through some basic conventional models like TFIDF, Count Vectorizer, Hashing, etc. that have been used in text classification and tried to access their performance to create a baseline. In the third post, I delved deeper into Deep learning models and the various architectures we could use to solve the text Classification problem. In this post, I will try to use ULMFit model which is a transfer learning approach for NLP.

As a side note: if you want to know more about NLP, I would like to recommend this excellent course on Natural Language Processing in the Advanced machine learning specialization. You can start for free with the 7-day Free Trial. This course covers a wide range of tasks in Natural Language Processing from basic to advanced: sentiment analysis, summarization, dialogue state tracking, to name a few. You can start for free with the 7-day Free Trial.

Before introducing the notion of transfer learning to NLP applications, we will first need to understand a little bit about Language models.

Language Models And NLP Transfer Learning Intuition:

In very basic terms, the objective of the language model is to predict the next word given a stream of input words. In the past, many different approaches have been used to solve this particular problem. Probabilistic models using Markov assumption is one example of this sort of models.

In the recent era, people have been using RNNs/LSTMs to create such language models. They take as input a word embedding and at each time state return the probability distribution of next word probability over the dictionary words. An example of this is shown below in which the‚Ä¶
Do machine learning models always need to be interpretable? Given a choice between an interpretable model that is inaccurate and a non-interpretable model that is accurate, wouldn‚Äôt you rather choose the non-interpretable but accurate model? In other words, is there any reason for sacrificing accuracy at the altar of interpretability?

Before going any further we should try to clarify what makes a model interpretable. Often, interpretability is equated with simplicity. This definition is obviously ambiguous; what is simple for one person may not be so for another. More importantly, advanced machine learning is most useful for modeling complex systems. These systems are called complex for a reason ‚Äî they are not simple! Demanding that useful models for such systems should be simple, by any measure, makes very little sense.

Perhaps, we can go further if we consider how we think about interpretability in a traditional modeling setting. Consider the mathematical model of an internal combustion engine in a car. I doubt if anyone would consider that to be simple. But, on the other hand, I also doubt if anyone would consider the model of an internal combustion engine to be non-interpretable, either. This is primarily because we can derive this model in a deductive manner from well established physical theories such as thermodynamics and fluid dynamics.

Why not use this as our definition of interpretability? A model should be considered to be interpretable if it can be derived (or at least motivated) from a trustworthy theory. This definition of interpretability serves the dual purpose of understanding and trust. It helps us understand the model because we tend to understand things in a deductive manner ‚Äî by going from the known to the unknown. Also, with such a definition, the trust in the model is derived from the trust that we place in the underlying theory.

Indeed, there are situations where both understanding and trust are necessary ‚Äî scenarios where we are interested in determining the causal factors behind the behavior of a system. In such scenarios, we must insist that the corresponding models must be interpretable according to the above definition. Most models in the realm of physical sciences belong to this category. One can argue that purely inductive blackbox models are not suitable for such scenarios.

However, there are many other situations where understanding might be nice to have, but by no means is it a must have. In these situations what really matters is the ability to make trustworthy predictions. In these situations, if we could provide an alternate source of trust, then our models need not be bound by the definition of interpretability given above. This is a common argument, and there is merit to it. Remember, machine learning is a way of systematically building models from (preferably) large amounts of data using inductive reasoning. Constraining these models to be interpretable in a deductive manner can seriously limit their accuracy.

So then the question becomes how can we generate trust in a blackbox model where we have little to no insight into its inner workings. A credible basis for trust could be testing. After all, testing forms the basis of our trust in regular software. But to test a model we need be able to formalize our expectations about it. If we could formalize our expectations completely then that would correspond to a complete specification of the model itself. In that case, we would not really need machine learning or any other modeling methodology. What we really need to be able to do is to formalize our expectations about the aspects of the model that we consider important. This is not easy either, because many of the concepts that we care about, such as fairness, do not lend themselves to a convenient mathematical treatment.

It is worth pointing out that significant progress has been made in developing testing methodologies for testing machine learning models. I personally find the idea of using metamorphic relations for formalizing expectations to be particularly promising. But, we are still a long way from having concrete methodologies that will allow us to perform comprehensive testing of blackbox models, and this inability of ours contributes to a trust deficit in blackbox models.

One could question the efficacy of such expectation-based comprehensive testing. After all, the goal of machine learning is to find undiscovered patterns in data. By insisting that the models meet our expectations amounts to pre-defining the model, which defeats the whole purpose. Following this line of reasoning, one would argue that as long as the data is representative and our algorithms are powerful enough to capture the patterns, there is little reason not to trust the model ‚Äî we should expect the model results to generalize to the overall population, and the extent to which we should expect them to generalize is encapsulated in the model‚Äôs performance (accuracy) scores.

Thus, in essence we are asked to delegate our trust to the trifecta of data, algorithms and performance scores. We first need to dissuade ourselves from the notion that a single performance (accuracy) score can form sufficient basis for trusting the model. A performance score is usually a point estimate of how a model is expected to generalize on an average over a population given the current data. Trust, on the other hand, is a nuanced multidimensional concept that cannot be encapsulated in such a single coarse grained score. One can imagine defining more granular performance scores‚Äî e.g. by population segments. But, that would require a certain level of understanding of the population and determining what we consider important ‚Äî this is not very different from forming expectations.

Let us examine the data aspect of this argument. It is, indeed, quite easy to convince oneself that if the data is representative of the population we are interested in, then it should contain all the relevant patterns and no spurious ones. Unfortunately, that is rarely the case. The degree to which the data can be non-representative depends quite acutely on the situation. Nonetheless, we can identify certain high level scenarios.

In the first scenario, we would have a good understanding of the population and complete control over the data collection mechanism. In this scenario, we can choose our data to be representative, and with a high degree of confidence we can expect our resulting model‚Äôs predictions to be applicable to the overall population. However, note that having a good enough understanding of the population to be able to draw a representative sample for the task at hand means that we already have some understanding of which features are important for the prediction. Hence, in this case it is debatable if blackbox models are terribly useful. Opinion polling for predicting election results is a good example of this scenario.

In the second scenario, we do not have complete control over the data collection, but our predictions do not affect the data collected. In this scenario, if we assume that the data collection mechanism is unbiased then were we to wait long enough, we would have a representative sample of the population. Of course, there are a lot of ifs and buts that go with this assumption. Firstly, one does not know how long is long enough. Thus one needs to assume that the time scale over which the data is collected is long enough to produce a representative sample. Furthermore, the population itself might change in the meantime. Thus, an additional assumption is that the time scale over which population changes is much longer than the time scale over which a representative sample is generated. As long as we can justify those assumptions, then the estimated performance will be reliable. A model for predicting the stock prices is an example of such a scenario ‚Äî as long as we are not making investments that are large enough to tip the market as a whole, the decisions that we make as the result of the predictions should not affect the stock prices.

The third scenario is one where the data collection is impacted by the predictions, but we have a moderate to high risk appetite for wrong predictions. An example of this is a product recommender system. The model for a recommender systems will be trained on data consisting of ordered lists of products that different users have bought/clicked on. Based on this data the model will predict what a user is most likely to buy/click on and based on the model‚Äôs predictions the system will decide what the user gets to see, which limits what (s)he can buy/click-on. Thus the prediction biases the data collection. In product recommender systems, one can circumvent this problem, somewhat, by keeping an exploration budget ‚Äî for a fraction of the cases the system shows the user a random set of products regardless of the prediction of the model. The observations resulting from these randomized predictions can then be used to estimate the performance of the model. One still has to address the concerns of the aforementioned second scenario in order to access the reliability of these estimates.

In the fourth and final scenario, the data collection is impacted by the predictions, but we have little to no risk appetite for wrong predictions. For example, suppose we have to build a model to predict whether someone will default on their mortgage loan payments. The mortgage loan will be approved or not based on the prediction. If the prediction is that the person will default, then the loan will not be approved, and in that case there is no way of knowing whether this person would have actually defaulted or not. It is difficult to imagine a situation where an institution would randomly approve (or otherwise) a loan for the sake of data exploration. In these situations, it is very difficult to gauge the reliability of the estimated performance of the resulting model without additional information.

Thus, it is not such a great idea to blindly expect the data to be representative of the population. In most scenarios, given the constraints of the problem at hand, it simply might not be possible to get an unbiased representative sample. Understanding the limitations of one‚Äôs data collection mechanism, being able to deduce the implications of those limitations, and having the honesty to report those as a part of the model‚Äôs results goes a long way in building trust.

Let us now consider the algorithm aspect of the argument. It is a widespread belief that the more flexible an algorithm is the better it is, because flexibility equips an algorithm to capture more complex patterns. But if the history of the actual successful applications of machine learning are anything to go by, then this belief would appear to be utterly misplaced. In computer vision, success came when we were able to encode the symmetries in pictures into models in the form of convolutional neural networks. In natural language processing we are now able to build extremely accurate cross-purpose language models because we could encode our knowledge about languages, including structure and word context, into these models. In recommender systems ‚Äî most collaborative filtering algorithms including matrix factorization methods, make strong assumptions about the affinity of a user towards an item.

Whether we would like to slap the label of interpretability on these models or not, it is an objective fact that we build better models when we understand the domain and the context in which the model needs to operate. The best models do not come from the most flexible algorithms, they come from algorithms that are well constrained by domain knowledge and have just the right amount of flexibility to capture the relevant patterns in the data.

We have seen the word understanding being used quite a few times in the above discussion. What we should have realized by now is that it is difficult to build trust without understanding. In the end, it boils down to how one perceives machine learning. Yes, machine learning is an incredibly powerful inductive modeling technique. When combined with big data and big compute, it allows us to model systems and solve problems that were previously out of our reach. But the entry of machine learning should not imply the exit of everything else, including common sense. Machine learning is one element in the wider modeling family that includes deductive modeling as well as domain knowledge. The better we understand and leverage the interconnections between these elements, the further we will go towards robust complex system modeling.

Trust is contextual and trust can have multiple sources, but eventually it flows from knowledge and integrity; specifically in our trust in the knowledge and integrity of the individuals who are building the models. Trust as well as adoption of models will come, in my opinion, only when the wider audience is convinced that the modelers have the knowledge to understand the limitations of their models (machine learning or otherwise), and the integrity to report them.
This paper by Quanshi Zhang, Ying Nian Wu, and Song-Chun Zhu from University of California, Los Angeles proposes a method to modify traditional convolutional neural networks (CNNs) into interpretable CNNs, in order to clarify knowledge representations in high conv-layers of CNNs.

Figure 1: Comparison of a filter‚Äôs feature maps in an interpretable CNN and those in a traditional CNN

Problem: without any additional human supervision, can we modify a CNN to obtain interpretable knowledge representations in its conv-layers?

Bau et al. [1] defined six kinds of semantics in CNNs, i.e. objects, parts, scenes, textures, materials, and colors.

In fact, we can roughly consider the first two semantics as object-part patterns with specific shapes, and summarize the last four semantics as texture patterns without clear contours. Filters in low conv-layers usually describe simple textures, whereas filters in high conv-layers are more likely to represent object parts.

Their approach is to train each filter in a high conv-layer to represent an object part. In a traditional CNN, a high-layer filter may describe a mixture of patterns, i.e. the filter may be activated by both the head part and the leg part of a cat (Figure 1). Such complex representations in high conv-layers significantly decrease the network interpretability. Therefore, their approach forces the filter in an interpretable CNN is activated by a certain part.

Learning a better representation

This paper invented a generic loss to regularize the representation of a filter to improve its interpretability.

The loss encourages a low entropy of inter-category activations and a low entropy of spatial distributions of neural activations which means forcing feature map of a layer in a CNN not to be randomly activated by different region of an object and to have consistent distribution of activations.

The filter must be activated by a single part of the object, rather than repetitively‚Ä¶
An Executive‚Äôs Guide to Implementing AI and Machine Learning

As a Chief Analytics Officer, I‚Äôve had to bridge the gap between business needs and data scientists. How that gap is bridged is, in my experience, the difference between how well the value and promise of artificial intelligence (AI) and machine learning is realized. Here are a few things I‚Äôve learned.

AI = machine learning (at least in 2019)

Machine learning is a path to get to AI. At least as of 2019, it is the only known viable path that I‚Äôm aware of. In the coming years, there may be other approaches. The two terms are not interchangeable, but for our purposes I will focus on machine learning.

Machine learning is a category of tools and approaches where a computer is given a large training set of data that includes an ‚Äúanswer key‚Äù. The machine then learns how to derive the answer key from combinations of the inputs. The model is then tested against a different testing data set to determine its accuracy.

Machine learning as a category can include basic statistical tools (e.g. linear regression) that fit this approach. It also includes neural networks, decision trees, and several other tools.

Is machine learning the right tool for the problem you‚Äôre trying to solve?

This one has tripped me up in the past.

For example, recently I had a data set with a lot of data collected from hospitals which had, for each employee, fifty measurements (for example, whether they showed up for work on time or whether they were consistently the only experienced person on their shift) and an indicator of whether they resigned in the weeks and months following. The question was: given this data set, could we create a model to predict employees who would resign before they did so, allowing hospitals to intervene early?

We spent months reviewing the data set and had used basic data visualization approaches to determine a set of rules. For example, employees who were just hired were twice as likely to resign than employees who had already worked at the hospital for ten years. Employees in certain‚Ä¶
Ocearch tracks whales, seals, sharks, and many other types of sea life as they make their way back and forth across the ocean. The tracking is available on their site and you can even sign up to follow certain sharks as they send in their telemetry data to the center.

Oh, does it bother you to be tagged?

All of this comes together as a ton of data including, location, speed, depth, heart rate and other metrics in the hope that we can better understand these animals in their own environment. By tracking and storing all of this data, environmentalists and biologists hope to analyze and improve the understanding of these magnificent creatures, without disturbing them.

The process of tagging an animal certainly has some momentary pain for the animal, and then they pretty much go about their lives and begin to transmit tons of data.

Fitness Trackers Are Telemetry Tags for Humans

So adorably simple.

In her book, The Age of Surveillance Capitalism: The Fight for a Human Future at the New Frontier of Power, Shoshana Zuboff does an excellent write up on telemetry and its origins and similarities to human tracking. Telemetry in the wild requires us to capture and forcibly attach a device to the animal. In the human realm, this happens in retail stores and on Amazon when we buy our fitness trackers. Sure, it‚Äôs a bit painful to pay the $600 for a top-notch tracker, but heck, I want to improve my fitness, so let‚Äôs go for it.

About ten years ago, this was my Timex watch. It was before fitness tracking really took off. There were a few Garmin and Polar trackers back then, but they were like strapping a microwave to your wrist, so only the most die-hard triathletes used them. Most of us just used this type of stopwatch to keep track of how long we worked out. This Timex was a higher-end model because it actually stored something like the last 20 workouts, which amounted to exactly 40 data points total. Start. Stop. Start. Stop. Start. Stop.

Truly high-tech.

SWOLF you.

Today, I have this incredible Garmin Fenix watch. This watch tracks literally everything. Time, location, waypoints, direction, speed, cadence, pace, heart rate, elevation, VO2 Max (oxygen velocity), steps, weight, swim speed, and even your SWOLF. Never heard of SWOLF? It‚Äôs what you get if swimming and golf have a baby.

To think how far these devices have come in such a brief period is amazing. I imagine they learned many of their algorithmic tricks from the classic use of telemetry in other fields of biology.

Tracking all of this data is at first amazing to a data geek such as myself. However, after a few months of inspecting and analyzing the data, I came to understand two painful realizations.

First, I‚Äôm No Healthier

Gathering data is not the same as having an answer or an action plan. The reality is that data is helpful but it must be interpreted and turned into action by individuals. As it turns out, my findings personally are not that dissimilar to others, in that fitness tracking doesn‚Äôt have a drastic difference in the outcome.

The reams and reams of data available from Garmin are impressive but what I realized is that I would analyze that data for a pretty decent amount of time each week. At the end of each workout and at least a few times a week, I‚Äôd review the data for anywhere from 5 minutes to 10 minutes. Add that up, and I could have actually used that time to work out, resulting in roughly one additional 45-minute workout a week. Doesn‚Äôt sound like much, but to give credit to the Timex stopwatch, finding the most important data points and focusing on those is sometimes more important than tracking everything.

Second, I‚Äôm Producing a Ton of Personal Data

Being the geek that I am, I took the output from Garmin from one 8-mile run while visiting the Jersey Shore and began to analyze it. Garmin provides these beautiful dashboard views of the data it gathers workout by workout. You can view some combination of screens on your phone or desktop through their App.
How a hackathon can help founders find their purpose

The inner path to becoming SensAI Arjan Haring ¬∑ Follow 5 min read ¬∑ Mar 30, 2019 -- Listen Share

Recently I did a proof of concept hackathon with Yama Saraj for his startup SensAI. Such hackathon forces founders to express their vision on their business model and their technology stack. There is a big difference running a social entreprise on DIY technology or a startup with a stack in the cloud and a clear exit strategy. And in Yama‚Äôs case he might be going for both‚Ä¶

Yama characterizes himself as a ‚Äúcrazy development economist‚Äù. When he was young, his family had to leave Afghanistan and they ended up in the Netherlands where he became a succesful student. Yama first studied electrical engineering then economics, but there was something missing. After his studies, in search of his ikigai, he drove all the way from the Netherlands to Afghanistan to give kids boxing lessons.

‚ÄúTechnology is just a medium to get the message across.‚Äù

He then realized that through sports you can empower this generation to become more resilient and you can even inspire them to be true changemakers. With a touch of irony, martial arts could lead to a peaceful society with less violence and competition. Technology is just a medium to get the message across. But what is the message?

Yama is one of a kind, with an energy that matches mine (almost), he is someone you can not not like. He sees connections everywhere and gets almost everyone excited about his ideas. And his ideas are great, if you ask me. Normally people would say someone like Yama needs to focus to be more effective. That could be true, but that most probably doesn‚Äôt make him happy.

Team ‚ÄúSustainable SensAI‚Äù working on their prototype. Hardware by MadLab Eindhoven.

Team Sellout SensAI versus team Sustainable SensAI

In the process of our hackathon Yama‚Äôs ideas were made tangible. One team wanted to work on his idea of a circular boxing bag that would help both the problem of used car tires and that of obesity in working class neighborhoods. And another team start working on the idea to gamify boxing, both for professional and private use.

Part of the slidedeck of team ‚ÄúSellout SensAI‚Äù

To run a sustainable, as in long term viable, company you need a robust busines model. That much is certain. So the tendency of millenials to work on something that is ‚Äúgood for the world‚Äù still needs to be matched by a revenue model. There are too many nice initiatives that don‚Äôt last because there is no clear revenue model. Volunteering or bootstrapping is often not a sustainable model.

To run a sustainable, as in long term viable, company you need a robust business model.

Soul-searching; 1 proof of concept at a time

Thinking by doing is a wellknown strategy in a lot of disciplines, for example in electrical engineering. But this is not yet a very common approach to the art of living. And in some cases, like finding a life partner, you want to take a more conservative approach and think more before you do.

‚Äúa failed startup is even considered a positive addition to your resume‚Äù

But in your working life I would argue you have quite a lot of flexibility to test things out. Is it something you like doing? Is it something you see yourself doing in 10 years still? Job hopping is more and more accepted, and a failed startup is even considered a positive addition to your resume (lucky me üòú).

A working prototype of a gamified boxing bag (Rogier Brussee demonstrating his makey makey contraption and a pretty mean Jab-Jab-cross combo)

So why not test with life a little?

So why not test with life a little? Enjoy the different things it has to offer and better prepare yourself for choices that have great impact on your life.

During the hackathon in the proof of concept lab Yama could better imagine what it would be like to run a company that sold the products the teams came up with. But Yama could also better understand what people he needed on his team for all the different projects.

Find your SensAI

Yama has a special relationship with 5 time World Champion Thai boxing Yucel Fidan. Yucel is one of the persons that sees the great things that SensAI could be part of. And Yama would like to learn how to be a champion like Yucel, albeit in a different arena.

For me it was an incredible honor to be part of the follow-up day of the SensAI hackathon as well that was hosted at Fidan Gym. Yucel is an incredibly balanced champion. Having worked hard myself, I am always in awe of people that managed to metaphorically move mountains, made a small dent in the universe and stayed true to themselves no matter what.

5 time World Champion Thai boxing Yucel Fidan & CEO of SensAI Yama Saraj

I am not sure what choices Yama will take in the nearby future. But I hope, and I am pretty sure, he stays true to himself. I have a feeling that the proof of concepts have given him more grip on what the effects are of the choices he makes as an entrepreneur for his own life.

I am very grateful to have worked with Yama for my first proof of concept session at JADS. And it might not surprise you we are already planning a new proof of concept session soon. Stay tuned, stay SensAI.
Opening your mind to different ways of applying machine learning to the real world. By Abraham Kang with special thanks to Kunal Patel and Jae Duk Seo for being a sounding board and providing input for this article.

Photo by Franck V. on Unsplash

Executive Summary

Candidate Problem

Many people are interested in automating redundant processes within the organization using AI. Let‚Äôs start with a concrete problem, what I noticed is that lawyers typically gather facts from clients when something bad happens. These facts form the basis of causes of action (negligence, battery, assault, intentional infliction of emotional distress) that an individual can sue on. Once the causes of action have been determined based on legal justification and the facts, a complaint is written up and submitted to the court for commencement of the legal action. The complaint is a legal document which sets out the facts giving rise to a legal basis for taking action against another party. Manually creating this document can be time consuming and similar facts result in similar causes of action. For example, if someone hits another person there is usually a ‚Äúbattery‚Äù. If someone accidentally hurts someone else or someone slips and falls within a store there could be an action for negligence. Based in this problem we have a customer who would like to use AI to learn how to write a complaint from a fact paragraph describing what happened.

Understanding the Problem

Trying to get AI/ML to read facts and figure out a way for AI/ML to write a whole complaint might be biting off more than the model can chew and may be an effort that would take years to solve. However, if you take the time to understand and think about the underlying problem, you can find existing techniques (with some slight modifications) that could be used to solve different pieces of the puzzle. For example, when you look at a complaint it starts with a description of the parties and their positions (plaintiff vs defendant) as well as counsel representing them. There may be a class action section, a justification of jurisdiction (does court have power over parties), description of the parties, a justification of venue (are we in the proper court location), a listing of the causes of action, and description of the facts. When you look at the sections you have to think about where the data that is going to build the individual sections is going to come from. In certain cases you will not have an answer but if you look carefully you will see patterns and correlations between different sections of the complaint. This will allow you to think about what your inputs to the neural network will be and the candidate outputs.

Getting Inputs for the Neural Network

We don‚Äôt have any data per se but there may be a way to parse the facts out of all existing complaints and use them as the input for our neural network. Every complaint that is submitted to the court becomes public information so there will be plenty of data. This solution will require attorneys to write their facts as if they were inserting them directly into the complaint, but this is a minor inconvenience to be able to have machine learning provide generated complaints. Generating a complete complaint may be difficult. So let‚Äôs break the problem down.

Breaking the Problem Down

Logically how would you break the generation of a document down into smaller pieces? Well you need to look at one so here is an example: https://www.heise.de/downloads/18/1/8/9/1/3/4/6/NP-v-Standard-Innovation-Complaint.pdf. To make it interesting I picked a maker of adult toys so it might peak your curiosity. Basically, we want to eventually generate a complaint (above pdf) from the facts provided by a lawyer. So if you look at the document and at other complaints you will find similar patterns as to structure.

So what do you think would be the best way to break things down‚Ä¶ don‚Äôt scroll down until you have had time to think about it.

‚Ä¶.Really think about it‚Ä¶..

Well if you said to break things down by section using templating, then this would be the route that would probably be best.

When you break down a complaint there are causes of action listed in the complaint. Each cause of action (violation of Federal Wiretap Act, Illinois Eavesdropping Statute, Intrusion upon Seclusion, Unjust Enrichment, Fraud and Deceptive Business Practice Act, etc.) has supporting rules and justification based on the facts. So now there are two problems. How do you come up with the causes of action from the facts text and how do you generate the supporting text under each cause of action?

Finding the Causes of Action

When we look at the facts of the case we need to find all of the causes of action (laws that were broken) that we could sue on. There are no direct solutions for finding causes of action from text so we will have to think more fundamentally.

What existing techniques do you think we can use to look at text and infer meaning or a description of the text. If you said multi-label text classification or multi-label sentiment analysis, then you are ahead of the game (https://paperswithcode.com/task/text-classification, https://paperswithcode.com/task/sentiment-analysis). Analyzing text to determine its associated causes of action is a similar process to classifying text or finding the sentiment of related text. There are associated problems like the fact that causes of action will need to be updated as laws are introduced. There may be an alternate way to create an embedding for the facts and then tie the causes of action to the facts based on triplet (https://arxiv.org/pdf/1503.03832.pdf) or quadruplet loss (https://arxiv.org/pdf/1704.01719.pdf) to push causes of action sharing similar words together in the embedding space and unrelated causes of action further apart. Then use a clustering technique to find causes of action close to determinative word embeddings used in the supporting argument associated with the words in the individual cause of action sections of the complaint.

Generating the Text in the Supporting Arguments Section of Individual Causes of Action

Now that you have figured out how to get the high level causes of action from the text, how can you generate the supporting argument text for each of the individual cause of action sections (violation of Federal Wiretap Act, Illinois Eavesdropping Statute, Intrusion upon Seclusion, Unjust Enrichment, Fraud and Deceptive Business Practice Act, etc.)?

This one is not so straight forward. Think about a what neural network architectures which generate text (Don‚Äôt scroll down until you have some ideas)‚Ä¶.

‚Ä¶.Open your mind‚Ä¶.Use the Force‚Ä¶.

Text generation algorithms (https://paperswithcode.com/task/data-to-text-generation, https://paperswithcode.com/area/nlp/text-generation) might be an option but even the best ones create gibberish often. The better alternative might be to use an architecture like neural networks involved in translation (https://paperswithcode.com/task/machine-translation, https://paperswithcode.com/task/unsupervised-machine-translation, https://paperswithcode.com/paper/unsupervised-clinical-language-translation). In addition, it might be a good idea to have a separate ‚Äútranslation‚Äù neural network for each cause of action to help each neural network focus on identifying the key facts used in generating a supporting argument for each cause of action.

Clean Up

It is probably going to be a good idea to run the candidate text for the supporting argument text for each cause of action through a grammar checker/fixer (https://paperswithcode.com/task/grammatical-error-correction). This way any blatant mess ups are fixed.

Conclusion

I hope you learned how to apply the machine learning solutions more broadly. Let me know if you get stuck as I would definitely be interested in hearing about problems that people are trying to solve with machine learning.
Wild Wide AI: responsible data science

Data Science can do good things for us: it improves life, it makes things more efficient, more effective and leads to a better experience. There are however some miss-steps that data-driven analysis has already exhibited. Here are few examples where data science tools were intentionally or unintentionally misused:

In 2012 a team of investigative journalists from The Wall Street Journal found out that Staples - a multinational supply retailing corporation ‚Äî offered lower prices to buyers who live in more affluent neighborhoods. Staples‚Äô intention was to offer discounts to customers who lived closer to their competitors‚Äô stores. However their competitors tended to build stores in richer neighborhoods. Based on the correlation between location and social status, this resulted in price discrimination based on race. Neither Staples nor customers did not know about this side effect until a team of investigative journalists brought it to light (source).

In 2015, the AdFischer project demonstrated via simulation that synthetic men online profiles were being shown ads for high paying jobs significantly more frequently than female profiles. The result was a clear employment discrimination based on gender (source). The study started surfacing the problem due to lack of responsibility intentionally or not in data-driven algorithmic pipelines.

In 2016, investigators from ProPublica discovered that the software used by judges in court to predict future crimes was often incorrect, and it was racist: blacks were almost twice as likely as whites to be labeled a higher risk but less likely to re-offend. The tool made the opposite mistake among whites: they were much more likely than blacks to be labeled lower risk but went on to commit other crimes. ProPublica‚Äôs study was very influential. They published the dataset, the data methodology, as well as the data processing code in the form of a Jupyter Notebook on GitHub. This striking result really speaks about the opacity and the lack of fairness in these types of tools, especially when they were used in the public sector, in governments, in the juridical system (source).

Is Data Science impartial?

It is often claimed that data science is algorithmic and therefore cannot be biased. And yet, we saw examples above where all traditional evils of discrimination exhibit themselves in the data science ecosystem. Bias is inherited both in the data and in the process, is propelled and amplified.

Transparency is an idea, a mindset, a set of mechanisms that can help prevent discrimination, enable public debate and establish trust. When we make data science, we interact with society. The way we do decisions has to be in an environment where we have trust from the participants, from the public. Technology alone won‚Äôt solve the issue. User engagement, policy efforts are important.

Data responsibility

Aspects of responsibility in the data science ecosystem include: fairness, transparency, diversity and data protection. The area of responsible data science is very new but is already at the edge of all the top machine learning conferences because these are difficult but interesting and relevant problems.

Moritz Hardt

What is Fairness?

Philosophers, lawyers, sociologists have been asking this question for many years. In the data science context we usually solve the task of predictive analytics, predicting future performance or behavior based on some past or present observation (dataset). Statistical bias occurs when models used to solve such tasks do not fit the data very well. A biased model is somehow imprecise and does not summarize the data correctly. Societal bias happens when the data or the model does not represent the world correctly. An example occurs when the data is not representative. This is the case if we only used the data for police going to the SAME neighborhood over and over, and we use this information only about crime from those particular neighborhoods. Societal bias can also be caused by how we define world. Is it the world as it is that we are trying to impact with predictive analytics or the world as it should be? Who should determine what the world should be like?

What is discrimination?

In most legal systems, there are two concepts defining discrimination:

Disparate treatment is the illegal practice of treating an entity, such as a creditor or employee, differently based on a protected characteristic such as race, gender, age, religion, sexual orientation, or national origin. Disparate treatment comes in a context where there is some benefit to begin or some harm to be brought to the individual being treated, for example sentencing them or admitting them to college or granting them credit. It is something where there is actually tangible positive or negative impact.

Disparate impact is the result of systematic disparate treatment, where disproportionate adverse impact is observed on members of

a protected class. Different countries protect different classes or sub-populations.

When we talk about discrimination, we are using terms which could be uncomfortable, such as racism, gender, sexual orientation. Political correctness in the extreme sense has no place in these debates about responsible data science. We have to be able to name concepts to be able to talk about them. Once we can talk about those concepts, we can take corrective action.

Technical definition of fairness

Let‚Äôs consider vendors who are assigning outcomes to members of a population. This is the most basic case, a binary classification. Positive outcomes may be: offered employment, accepted to school, offered a loan, offered a discount. Negative outcomes may be: denied employment, rejected from school, denied a loan, not offered a discount. What we worry about in fairness is how outcome is assigned to members of a population. Let‚Äôs assume that 40% got the positive outcome. Some sub-population however may be treated differently by this process. Let‚Äôs assume that we know ahead of time what the sub-population is, for example red haired people. Thus we can divide our population into two groups: people with red hair, and people without red hair. In our example we observe that while 40% of the population got the positive outcome, only 20% of red haired received the positive outcome. 60% of the other received the positive outcome. Here, according to some definition, we observe disparate impact on the group of red haired individuals. Another way to denote this situation is that statistical parity fails. This is a baseline definition of fairness without conditioning. There is quite some sophistication about using such assessment in real-life, for example in courts. This basic definition of fairness, written into many laws around the globe, dictates that demographics of the individuals receiving any outcome are the same as demographics of the underlying population.

Assessing disparate impact

The vendor could say that he actually did not intend or did not look at all at hair color, which happens to be the sensitive attribute in the dataset. Instead the vendor would say that he decided to give the positive outcome to people whose hair is long. The vendor is denying the accusation and saying that he is not discriminating based on hair color. The thing is that the vendor has adversely impacted red haired people. It is not the intention that we care about, but the effect on the sub-population. In other words, blinding is not a legal or ethical excuse. Removing hair color from vendor‚Äôs process on outcome assignment does not prevent discrimination from occurring. Disparate impact is legally assessed on the impact, not on the intention.

Mitigating disparate impact

If we detect a violation of statistical parity, we may want to mitigate. In an environment in which we have a number of positive outcomes which we can assign, we have to swap some outcomes. We have to take a positive outcome from somebody in the not-red haired group and give it to someone else in the red haired group. Not everyone will agree with swapping outcomes. An individual who used to get the positive outcome would stop getting it any more. This would lead to individual fairness. It stipulates that any two individuals who are similar within a particular task should receive similar outcomes. There is a tension between group and individual fairness that is not easy to resolve.

Individual vs group fairness

An example in which individual fairness and group fairness was taken to the supreme court appears in the Ricci v. DeStefano case in 2009. Firefighters took a test for promotion, and the department threw out the test results because none of the black firefighters scored high enough to be promoted. The fire department was afraid that they could be sued for discrimination and disparate impact if they were to admit results and not promote any black firefighter. But then the lawsuit was brought by the firefighters who would have been eligible for promotion but who weren‚Äôt promoted as a result of this. There was an individual fairness argument, a disparate treatment argument. They argued that race was used to negatively impact them. This case was ruled in favor of white firefighters, in favor of individual fairness.

Individual fairness is equality, everybody gets the same box to reach the tree. Group fairness is the equity view, everybody gets as many boxes as they need to be able to reach the tree. Equity costs more because society has to invest more. These are two intrinsically different world views that we cannot logically decide which one is better. These are just two different points of view, there isn‚Äôt a better one. They go back to what we believe a world as it is, is a world as it should be. The truth is going to be somewhere in the middle. It is important to understand which kinds of mitigation are consistent with which kinds of belief systems.

Formal definition of fairness

Friedler et. al. tease out the difference between beliefs about fairness and

mechanisms that logically follow from those beliefs in their paper from 2016. The construct space is intrinsically the state of the world. It is made of things we cannot directly measure such as intelligence, grit, propensity to commit crime and risk-adverseness. We however want to measure intelligence and grit when we decide who to admit to college. We want to know the propensity of a person to recommit crime and his risk-adverseness in justice. These are raw properties which are exhibited and not directly accessible. Instead we look at the observed space where there are proxies, which are to a greater or lesser degree aligned with the properties that we want to measure. For intelligence the proxy would be SAT score, grit would be measured by high-school GPA, propensity to commit crime by family history and risk-adverseness by age. The decision space is then made of what we would like to decide: performance in college and recidivism.

Fairness is defined here as a mapping from the construct space to the decision space, via the observe space. Individual fairness (equality) believes that the observed space faithfully represents the construct space. For example high-school GPA is a good measure of grit. Therefore the mapping from construct to decision space has low distortion. Group fairness (equity) however says that there is a systematic distortion caused by structural bias, society bias when going from the construct space to observed space. Furthermore this distortion aligns with groups structure, with membership in protected groups in our society. In other words the society systematically discriminates.

to be continued ‚Ä¶

References: lecture on responsible data science at Harvard University by Prof. Julia Stoyanovich (New York University) ‚Äî selected chapters from ‚Äú The Age of Surveillance Capitalism‚Äù book by Shoshana Zuboff ‚Äî thoughts from ‚ÄúWhat worries me about AI‚Äù post by Fran√ßois Chollet.
#InsideRL

In a typical Reinforcement Learning (RL) problem, there is a learner and a decision maker called agent and the surrounding with which it interacts is called environment. The environment, in return, provides rewards and a new state based on the actions of the agent. So, in reinforcement learning, we do not teach an agent how it should do something but presents it with rewards whether positive or negative based on its actions. So our root question for this blog is how we formulate any problem in RL mathematically. This is where the Markov Decision Process(MDP) comes in.

Typical Reinforcement Learning cycle

Before we answer our root question i.e. How we formulate RL problems mathematically (using MDP), we need to develop our intuition about :

The Agent-Environment relationship

Markov Property

Markov Process and Markov chains

Markov Reward Process (MRP)

Bellman Equation

Markov Reward Process

Grab your coffee and don‚Äôt stop until you are proud!üßê

The Agent-Environment Relationship

First let‚Äôs look at some formal definitions :

Agent : Software programs that make intelligent decisions and they are the learners in RL. These agents interact with the environment by actions and receive rewards based on there actions. Environment :It is the demonstration of the problem to be solved.Now, we can have a real-world environment or a simulated environment with which our agent will interact.

Demonstrating an environment with which agents are interacting.

State : This is the position of the agents at a specific time-step in the environment.So,whenever an agent performs a action the environment gives the agent reward and a new state where the agent reached by performing the action.

Anything that the agent cannot change arbitrarily is considered to be part of the environment. In simple terms, actions can be any decision we want the agent to learn and state can be anything which can be useful in choosing actions. We do not assume that everything in the environment is unknown to the agent, for example, reward calculation is considered to be the part of the environment even though the agent knows a bit on how it‚Äôs reward is calculated as a function of its actions and states in which they are taken. This is because rewards cannot be arbitrarily changed by the agent. Sometimes, the agent might be fully aware of its environment but still finds it difficult to maximize the reward as like we might know how to play Rubik‚Äôs cube but still cannot solve it. So, we can safely say that the agent-environment relationship represents the limit of the agent control and not it‚Äôs knowledge.

The Markov Property

Transition : Moving from one state to another is called Transition. Transition Probability: The probability that the agent will move from one state to another is called transition probability.

The Markov Property state that :

‚ÄúFuture is Independent of the past given the present‚Äù

Mathematically we can express this statement as :

Markov Property

S[t] denotes the current state of the agent and s[t+1] denotes the next state. What this equation means is that the transition from state S[t] to S[t+1] is entirely independent of the past. So, the RHS of the Equation means the same as LHS if the system has a Markov Property. Intuitively meaning that our current state already captures the information of the past states.

State Transition Probability :

As we now know about transition probability we can define state Transition Probability as follows :

For Markov State from S[t] to S[t+1] i.e. any other successor state , the state transition probability is given by

State Transition Probability

We can formulate the State Transition probability into a State Transition probability matrix by :

State Transition Probability Matrix

Each row in the matrix represents the probability from moving from our original or starting state to any successor state.Sum of each row is equal to 1.

Markov Process or Markov Chains

Markov Process is the memory less random process i.e. a sequence of a random state S[1],S[2],‚Ä¶.S[n] with a Markov Property.So, it‚Äôs basically a sequence of states with the Markov Property.It can be defined using a set of states(S) and transition probability matrix (P).The dynamics of the environment can be fully defined using the States(S) and Transition Probability matrix(P).

But what random process means ?

To answer this question let‚Äôs look at a example:

Markov chain

The edges of the tree denote transition probability. From this chain let‚Äôs take some sample. Now, suppose that we were sleeping and the according to the probability distribution there is a 0.6 chance that we will Run and 0.2 chance we sleep more and again 0.2 that we will eat ice-cream. Similarly, we can think of other sequences that we can sample from this chain.

Some samples from the chain :

Sleep ‚Äî Run ‚Äî Ice-cream ‚Äî Sleep

Sleep ‚Äî Ice-cream ‚Äî Ice-cream ‚Äî Run

In the above two sequences what we see is we get random set of States(S) (i.e. Sleep,Ice-cream,Sleep ) every time we run the chain.Hope, it‚Äôs now clear why Markov process is called random set of sequences.

Before going to Markov Reward process let‚Äôs look at some important concepts that will help us in understand MRPs.

Reward and Returns

Rewards are the numerical values that the agent receives on performing some action at some state(s) in the environment. The numerical value can be positive or negative based on the actions of the agent. In Reinforcement learning, we care about maximizing the cumulative reward (all the rewards agent receives from the environment) instead of, the reward agent receives from the current state(also called immediate reward). This total sum of reward the agent receives from the environment is called returns.

We can define Returns as :

Returns (Total rewards from the environment)

r[t+1] is the reward received by the agent at time step t[0] while performing an action(a) to move from one state to another. Similarly, r[t+2] is the reward received by the agent at time step t[1] by performing an action to move to another state. And, r[T] is the reward received by the agent by at the final time step by performing an action to move to another state.

Episodic and Continuous Tasks

Episodic Tasks: These are the tasks that have a terminal state (end state).We can say they have finite states. For example, in racing games, we start the game (start the race) and play it until the game is over (race ends!). This is called an episode. Once we restart the game it will start from an initial state and hence, every episode is independent. Continuous Tasks : These are the tasks that have no ends i.e. they don‚Äôt have any terminal state.These types of tasks will never end.For example, Learning how to code!

Now, it‚Äôs easy to calculate the returns from the episodic tasks as they will eventually end but what about continuous tasks, as it will go on and on forever. The returns from sum up to infinity! So, how we define returns for continuous tasks?

This is where we need Discount factor(…§).

Discount Factor (…§): It determines how much importance is to be given to the immediate reward and future rewards. This basically helps us to avoid infinity as a reward in continuous tasks. It has a value between 0 and 1. A value of 0 means that more importance is given to the immediate reward and a value of 1 means that more importance is given to future rewards. In practice, a discount factor of 0 will never learn as it only considers immediate reward and a discount factor of 1 will go on for future rewards which may lead to infinity. Therefore, the optimal value for the discount factor lies between 0.2 to 0.8.

So, we can define returns using discount factor as follows :(Let‚Äôs say this is equation 1 ,as we are going to use this equation in later for deriving Bellman Equation)

Returns using discount factor

Let‚Äôs understand it with an example,suppose you live at a place where you face water scarcity so if someone comes to you and say that he will give you 100 liters of water!(assume please!) for the next 15 hours as a function of some parameter (…§).Let‚Äôs look at two possibilities : (Let‚Äôs say this is equation 1 ,as we are going to use this equation in later for deriving Bellman Equation)

One with discount factor (…§) 0.8 :

Discount Factor (0.8)

This means that we should wait till 15th hour because the decrease is not very significant , so it‚Äôs still worth to go till the end.This means that we are also interested in future rewards.So, if the discount factor is close to 1 then we will make a effort to go to end as the reward are of significant importance.

Second, with discount factor (…§) 0.2 :

Discount Factor (0.2)

This means that we are more interested in early rewards as the rewards are getting significantly low at hour.So, we might not want to wait till the end (till 15th hour) as it will be worthless.So, if the discount factor is close to zero then immediate rewards are more important that the future.

So which value of discount factor to use ?

It depends on the task that we want to train an agent for. Suppose, in a chess game, the goal is to defeat the opponent‚Äôs king. If we give importance to the immediate rewards like a reward on pawn defeat any opponent player then the agent will learn to perform these sub-goals no matter if his players are also defeated. So, in this task future rewards are more important. In some, we might prefer to use immediate rewards like the water example we saw earlier.

Markov Reward Process

Till now we have seen how Markov chain defined the dynamics of a environment using set of states(S) and Transition Probability Matrix(P).But, we know that Reinforcement Learning is all about goal to maximize the reward.So, let‚Äôs add reward to our Markov Chain.This gives us Markov Reward Process.

Markov Reward Process : As the name suggests, MDPs are the Markov chains with values judgement.Basically, we get a value from every state our agent is in.

Mathematically, we define Markov Reward Process as :

Markov Reward Process

What this equation means is how much reward (Rs) we get from a particular state S[t]. This tells us the immediate reward from that particular state our agent is in. As we will see in the next story how we maximize these rewards from each state our agent is in. In simple terms, maximizing the cumulative reward we get from each state.

We define MRP as (S,P, R,…§) , where :

S is a set of states,

P is the Transition Probability Matrix,

R is the Reward function, we saw earlier,

…§ is the discount factor

Markov Decision Process

Now, let‚Äôs develop our intuition for Bellman Equation and Markov Decision Process.

Policy Function and Value Function

Value Function determines how good it is for the agent to be in a particular state. Of course, to determine how good it will be to be in a particular state it must depend on some actions that it will take. This is where policy comes in. A policy defines what actions to perform in a particular state s.

A policy is a simple function, that defines a probability distribution over Actions (a‚àà A) for each state (s ‚àà S). If an agent at time t follows a policy œÄ then œÄ(a|s) is the probability that the agent with taking action (a ) at a particular time step (t).In Reinforcement Learning the experience of the agent determines the change in policy. Mathematically, a policy is defined as follows :

Policy Function

Now, how do we find a value of a state. The value of state s, when the agent is following a policy œÄ which is denoted by vœÄ(s) is the expected return starting from s and following a policy œÄ for the next states until we reach the terminal state. We can formulate this as :(This function is also called State-value Function)

Value Function

This equation gives us the expected returns starting from the state(s) and going to successor states thereafter, with the policy œÄ. One thing to note is the returns we get is stochastic whereas the value of a state is not stochastic. It is the expectation of returns from start state s and thereafter, to any other state. And also note that the value of the terminal state (if there is any) is zero. Let‚Äôs look at an example :

Example

Suppose our start state is Class 2, and we move to Class 3 then Pass then Sleep.In short, Class 2 > Class 3 > Pass > Sleep.

Our expected return is with a discount factor of 0.5:

Calculating the Value of Class 2

Note:It‚Äôs -2 + (-2 * 0.5) + 10 * 0.25 + 0 instead of -2 * -2 * 0.5 + 10 * 0.25 + 0.Then the value of Class 2 is -0.5 .

Bellman Equation for Value Function

Bellman Equation helps us to find optimal policies and value functions. We know that our policy changes with experience so we will have different value functions according to different policies. The optimal value function is one that gives maximum value compared to all other value functions.

Bellman Equation states that value function can be decomposed into two parts:

Immediate Reward, R[t+1]

Discounted value of successor states,

Mathematically, we can define Bellman Equation as :

Bellman Equation for Value Function

Let‚Äôs understand what this equation says with a help of an example :

Suppose, there is a robot in some state (s) and then he moves from this state to some other state (s‚Äô). Now, the question is how good it was for the robot to be in the state(s). Using the Bellman equation, we can that it is the expectation of reward it got on leaving the state(s) plus the value of the state (s‚Äô) he moved to.

Let‚Äôs look at another example :

Backup Diagram

We want to know the value of state s. The value of state(s) is the reward we got upon leaving that state, plus the discounted value of the state we landed upon multiplied by the transition probability that we will move into it.

Value Calculation

The above equation can be expressed in matrix form as follows :

Bellman Linear Equation

Where v is the value of state we were in, which is equal to the immediate reward plus the discounted value of the next state multiplied by the probability of moving into that state.

The running time complexity for this computation is O(n¬≥). Therefore, this is clearly not a practical solution for solving larger MRPs (same for MDPs).In later Blogs, we will look at more efficient methods like Dynamic Programming (Value iteration and Policy iteration), Monte-Claro methods, and TD-Learning.

We are going to talk about the Bellman Equation in much more detail in the next story.

What is Markov Decision Process ? Markov Decision Process : It is Markov Reward Process with a decisions.Everything is same like MRP but now we have actual agency that makes decisions or take actions.

It is a tuple of (S, A, P, R, ùõæ) where:

S is a set of states,

A is the set of actions agent can choose to take,

P is the transition Probability Matrix,

R is the Reward accumulated by the actions of the agent,

ùõæ is the discount factor.

P and R will have slight change w.r.t actions as follows :

Transition Probability Matrix

Transition Probability Matrix w.r.t action

Reward Function

Reward Function w.r.t action

Now, our reward function is dependent on the action.

Till now we have talked about getting a reward (r) when our agent goes through a set of states (s) following a policy œÄ. Actually, in Markov Decision Process(MDP) the policy is the mechanism to take decisions. So now we have a mechanism that will choose to take an action.

Policies in an MDP depend on the current state. They do not depend on history. That‚Äôs the Markov Property. So, the current state we are in characterizes history.

We have already seen how good it is for the agent to be in a particular state(State-value function). Now, let‚Äôs see how good it is to take a particular action following a policy œÄ from state s (Action-Value Function).

State-action value function or Q-Function

This function specifies how good it is for the agent to take action (a) in a state (s) with a policy œÄ.

Mathematically, we can define the State-action value function as :

State-action value function

Basically, it tells us the value of performing a certain action(a) in a state(s) with a policy œÄ.

Let‚Äôs look at an example of the Markov Decision Process :

Example of MDP

Now, we can see that there are no more probabilities. In fact, now our agent has choices to make like after waking up, we can choose to watch Netflix or code and debug. Of course, the actions of the agent are defined w.r.t some policy œÄ and will get the reward accordingly.
When I first took science class in elementary school, we learned about the five senses. Maybe not far into the future, that information might be as outdated as the idea of nine planets in our solar system (sorry, Pluto). This might be thanks to Neuralink, a company that‚Äôs been making headlines for its controversial brain-chip interface. On July 17, 2019, the company finally unveiled its hard work in a YouTube live stream.

When you first hear it, Neuralink‚Äôs pursuit sounds like it comes straight out of a mad genius‚Äôs diary; certainly, with Elon Musk at the head of the company, that image might not be a far stretch. But if you look beyond what seems to be a Sci-Fi horror movie, you can get a glimpse of quite a different future for humanity ‚Äî one that has a new, sixth sense: Neuralink.
Amount of information available today on the web is astounding and it is ever-expanding. For example, there are more than 1.94 billion websites that are linked with the World Wide Web and search engines (e.g., Google, Bing, etc.) can go through those links and serve useful information with great precision and speed. In most of those successful search engines, the most important denominator is the use of Knowledge Graphs. Not only search engines, social network sites (e.g., Facebook, etc.), e-commerce sites (e.g., Amazon, etc.) are also using Knowledge Graphs to store and retrieve useful information.

A Brief History

In 1960, Semantic Networks were invented to address the growing need for a knowledge representation framework that can capture a wide range of entities ‚Äî real-world objects, events, situations or abstract concepts and relations and in the end can be applied to extended English Dialogue tasks. The main idea behind Semantic Networks was to capture a wide range of issues which includes the representation of plans, actions, time, individuals‚Äô beliefs and intentions, and be general enough to accommodate each issue.

According to Wikipedia, in late 1980, two Netherlands universities started a project called Knowledge Graph which was kind of a semantic network, but with some added restrictions to facilitate algebraic operations on the graph.

In 2001, Tim Berners-Lee coined the term Semantic Web which is an application of Semantic Network combined with the Web.

Tim Berners-Lee stated that ‚ÄúThe Semantic Web is an extension of the current web in which information is given well-defined meaning, better enabling computers and people to work in cooperation.‚Äù

In 2012, Google named its Knowledge Graph as Knowledge Graph.

An Undefined Definition

Every Company/Group/Individual creates their own version of the Knowledge Graph to limit complexity and organize information into data and knowledge. For example, Google‚Äôs Knowledge Graph, Knowledge Vault, Microsoft‚Äôs Satori, Facebook‚Äôs Entities Graph, etc.

So, there is no formal definition of Knowledge Graph. In a broader perspective, a Knowledge Graph is a variant of semantic network with added constraints whose scope, structure, characteristics and even uses are not fully realized and in the process of development.

An Example of Knowledge Graph

Source: Maximilian Nickel et al. A Review of Relational Machine Learning for Knowledge Graphs: From Multi-Relational Link Prediction to Automated Knowledge Graph Construction

Why Should You Get Excited?

With every passing year, Machine Learning and Knowledge Representation Learning on Knowledge Graphs are advancing rapidly, both in scale and depth, but in different directions. On one hand, Machine Learning techniques are getting better at performing various tasks (e.g., Classification, Generation, etc.) on a variety of datasets with great precision and recall. On the other hand, Knowledge Representation brings the ability to represent entities and relations with high reliability, explainability, and reusability. Recent advances in Knowledge Representation Learning include mining logical rules from the graph.

Source: Bishan Yang et al. Embedding Entities and Relations for Learning and Inference in Knowledge Bases.

However, bringing knowledge graphs and machine learning together will systematically improve the accuracy of the systems and extend the range of machine learning capabilities. For example, results inferred from Machine Learning models will have better explainability and trustworthiness.

Below are some of the opportunities that can be availed by bringing Knowledge Graph to Machine Learning:

Data Insufficiency

Having a sufficient amount of data to train a machine learning model is very important. In the case of sparse data, Knowledge Graph can be used to augment the training data, e.g., replacing the entity name from original training data with an entity name of a similar type. This way a huge number of both positive and negative examples can be created using Knowledge Graph.

Zero-Shot Learning

Today, the main challenge with a Machine Learning model is that without a properly trained data it can not distinguish between two data points. In Machine Learning, this is considered as Zero-Shot Learning problem. This is where knowledge graphs can play a very big role. The induction from the Machine Learning model can be complemented with a deduction from the Knowledge Graph, e.g., with pictures where the type of situation did not appear in the training data.

Explainability

One of the major problems in machine learning industry is explaining the predictions made by machine learning systems. One issue is the implicit representations causing the predictions from the machine learning models. Knowledge Graph can alleviate this problem by mapping the explanations to some proper nodes in the graph and summarizing the decision-taking process.

Note: The above opportunities are explained in more detail in the seminar report of Knowledge Graphs: New Directions for Knowledge Representation on the Semantic Web (Dagstuhl Seminar 18371)

Some Use Cases

Question ‚Äî Answering is one of the most used applications of Knowledge Graph. Knowledge Graphs contain a wealth of information and question answering is a good way to help end-users to more effectively and also more efficiently retrieve information from Knowledge Graphs.

is one of the most used applications of Knowledge Graph. Knowledge Graphs contain a wealth of information and question answering is a good way to help end-users to more effectively and also more efficiently retrieve information from Knowledge Graphs. Storing Information of Research is another useful application Knowledge Graph. Recently, a lot of companies are using Knowledge Graph to store information generated from various stages of research which can be used for building accessible models, risk management, process monitoring, etc.

is another useful application Knowledge Graph. Recently, a lot of companies are using Knowledge Graph to store information generated from various stages of research which can be used for building accessible models, risk management, process monitoring, etc. Netflix uses a Knowledge Graph to store a vast amount of varied information for its Recommendation System which helps in finding relationships between movies, TV shows, persons, etc. Later, these relationships can be used to predict what customers might like to watch next.

which helps in finding relationships between movies, TV shows, persons, etc. Later, these relationships can be used to predict what customers might like to watch next. Supply Chain Management is also being benefited from the use of Knowledge Graph. Companies can easily keep track of inventories of different components, personnel involved, time, etc which allows them to move items more swiftly and cost-effectively.

and many more‚Ä¶

Open Challenges

A coherent set of best practices, that can be applied during the creation of knowledge graphs, will be helpful in understanding and reuse of Knowledge Graphs amongst engineers, developer, and researchers. Given a set of unstructured data and Knowledge Graph, the problem of knowledge integration is to identify whether the entities mentioned in the data match with the real world entities present in Knowledge Graph. Although this problem can be solved using machine learning algorithms, the outcome of those algorithms directly depends on the quality of the training data. Given a wide variety of dataset, knowledge integration becomes quite difficult. Knowledge is not static, but constantly evolving. For example, if a Knowledge Graph keeps track of patients‚Äô health, the data stored at a particular moment could be false for some later moment. So, how do we capture this evolving nature of knowledge? How to evaluate a Knowledge Graph? Which quality improvement (e.g., completeness, correctness, linkage, etc.) is more important?

Acknowledgments
Should you buy the book?

Yes. But you don‚Äôt have to. You can read it first. But you should buy a copy, hold it, read it, sit it on your coffee table. Then when your friends ask, ‚ÄòWhat is machine learning?‚Äô, you‚Äôll be able to tell them.

Who is the book for?

Maybe you‚Äôre studying data science. Or you‚Äôve heard of machine learning being everywhere and you want to understand what it can do. Or you‚Äôre familiar with applying the tools of machine learning but you want to make sure you‚Äôre not missing any.

I‚Äôve been studying and practising machine learning for the past two-years. I built my own AI Masters Degree, it led to being a machine learning engineer. This book is part of my curriculum now but if it was out when I started, it would‚Äôve been on there from the beginning.

What previous knowledge do I need for the Hundred-Page Machine Learning book?

Having a little knowledge about math, probability and statistics would be helpful but The Hundred-Page Machine Learning Book has been written a way that you‚Äôll get most of these as you go.

So the answer to this question remains open. I read it from the perspective of a machine learning engineer, I knew some things but learned many more.
How to tune hyperparameters of tSNE

This is the second post of the column Mathematical Statistics and Machine Learning for Life Sciences. In the first post we discussed whether and where in Life Sciences we have Big Data suitable for Machine / Deep Learning, and emphasized that Single Cell is one of the most promising Big Data resources. t-distributed stochastic neighbor embedding (tSNE) is a Machine Learning non-linear dimensionality reduction technique which is absolutely central for Single Cell data analysis. However, the choice of hyperparameters for the tSNE might be confusing for beginners.

In this post, I will share my recommendations on selecting optimal values of hyperparameters such as perplexity, number of principal components to keep, and number of iterations for running tSNE.

How to Use tSNE Effectively

When teaching single cell RNA sequencing (scRNAseq) course I keep getting questions about sensitivity of tSNE with respect to hyperparameters such as perplexity. The questions are usually inspired by this fantastic post about challenges with interpreting tSNE plots.

A popular tutorial on developing intuition behind tSNE

Despite my great respect for the main message of the post, I think scRNAseq community should not worry too much about perplexity and other tSNE hyperparameters based on what they learn from that post because: a) many examples in the post come from abstract mathematical topologies which do not really resemble scRNAseq data, b) the post concentrates on extreme tSNE hyperparameters which are rarely used in the real world scRNAseq analysis.

If you do scRNAseq analysis you will not avoid the popular Rtsne function and R package which is based on Barnes-Hut C++ implementation of the original tSNE algorithm. The Rtsne function has three main hyperparameters:
Photo credit: Annie Spratt, UK

I‚Äôve become something of a MOOC connoisseur; at this point in my journey to employment, I‚Äôve tried every major MOOC and I still dabble to keep things fresh on the brain.

The great thing is, I learned a ton without spending much money. So far Udacity was the most expensive, and I got it on an interest-free loan. Often they offer discounts and many of them run on a monthly subscription for $30‚Äì50. For brevity, I don‚Äôt want to focus on the syllabus of each program here, but explain their strengths and weaknesses.

Part of my motivation for doing so many courses was my interest in education; I think the MOOC format‚Äôs made a lot of bright people rethink learning, and Coursera and SharpestMinds offer an important format that could compete with some aspects of academia. If you remember nothing else, though: MOOCs are a great way to build an industry skill and augment your education; they supply you with the skill-building academia can not. They are not a substitute for academic education and you will still need to

build your own projects, and answer your own questions

to become a data scientist. The closest one to academia would be Coursera. Ultimately I became a data scientist by using a combination of MOOCs, self-studying books, and leveraging my formal education.

Codecademy:

I highly recommend Codecademy Pro. Price is worth it and exercises really give the right amount of brainhurt, along with good explanations. I recommend clearing your progress on certain lessons and redoing them if you had trouble (reworking missed problems is good practice for anything technical).

I did their essential Python 3 course to make me a better Python user, but they have a Computer Science career track which is more involved and covers topics like recursion, algorithms and data structures.

The strength of Codecademy lies in its combination of code-alongs, guided projects, quizzes and videos: they are really pushing for a holistic understanding of the material. You need all those formats to truly understand something technical.

Dataquest:

Imagine Codecademy, but harder. These lessons are incredibly detailed. It‚Äôs a great place to learn, but it will take a lot of time to do even a normal mission. I love their ethos of ‚Äúquality education at a low price‚Äù ($30/month currently, $50 for the Premium plan, much cheaper if you buy a year) which makes it accessible to anyone willing to roll up their sleeves and work. For me, since I already covered a lot of these concepts, I didn‚Äôt find it worthwhile to complete any of the major certificates, but struggling through missions made me a better coder and practitioner of data science.

On the Premium plan you get a monthly call with a mentor and he‚Äôll review your resume or give technical advice; take advantage of this opportunity. The most valuable thing they can give you is criticism.

Dataquest offers a nice curriculum selection that will take you on up to advanced Python topics and, as somewhat of a statistics pro, I found their explanation of stats very good. I recommend going from Codecademy to Dataquest to move beyond foundational skills and get strong.

My one complaint is they could turn down the difficulty a hair; there‚Äôs something to be said for repetition and plug-and-chug when learning an applied skill. However, they cover an enormous amount of material and seem to have the best SQL courses of any MOOC. They go into Redshift and really advanced data engineering stuff. I‚Äôd consider a few DQ missions essential for any aspiring data scientist and it almost makes other MOOCs obsolete.

DataCamp:

My opinion is: DataCamp‚Äôs an earnest shot at education. It‚Äôs a great place to get your feet wet and the video format is noobie-friendly. I enjoyed it at the time, but it‚Äòs only step 1 of a long process. The code-along format is like Codecademy, but you don‚Äôt write code, just fill in blanks. There is some nice brainhurt here, but it‚Äôs not the best bang for your buck.

Videos (and lectures in general) aren‚Äôt a great way to teach a technical concept. If you‚Äôre totally new to data science, this is a nice place to start and get motivated, but I‚Äôd still recommend DataQuest.

Many DataCamp learners (myself included) report doing a DC course then going over to DataQuest to strengthen their skills, which is something I recommend (studying the same thing from different sources is always good practice).

I will say DataCamp has the best selection of R courses and if you are looking to familiarize yourself with a new R package, you can‚Äôt go wrong with DataCamp; their lecturers are world-class. It also exposes you to what good code looks like. At $30/month I‚Äôd say a few months is a good way to augment your education.

Udacity (Machine Learning Engineer):

This program covers the major ML algorithms and I thought the videos were helpful for things I‚Äôd already studied; they expanded my understanding of things like SVMs, but I don‚Äôt think they‚Äôd teach it to you alone.

The projects here are also good and the mini-labs (not required) are nice skill-builders. I thought the staff grading my projects were great and very thorough. They really pushed me to write good explanations of every tool I used, and justify my decisions.

$2000 is the current full list price for this program, and I think they‚Äôve actually dismantled it and changed it fundamentally. The Machine Learning Engineer program I see on the site now seems smaller and focused on deployment and data engineering, whereas what I took discussed all the major ML and Deep Learning algorithms (with no coverage of deployment).

Ultimately I‚Äôd say $2000 is way too much for what I got out of this, and the ‚ÄúCareer Services‚Äù from Udacity are a joke (I got a good resume review and that was it). It will, however, look good on your resume and has probably helped me land interviews.

Springboard (Intermediate Data Science w/ Python):

Springboard does not create much content of their own, instead linking you to free or low-cost education resources around the web. For machine learning fundamentals I think this is a good idea; there‚Äôs plenty of quality content out there so why reinvent the wheel at students‚Äô expense? It‚Äôs oriented for students making data science projects in Jupyter for the first time, and does its job well (previous MOOCs I listed are all in-browser code-alongs without pulling up your own Jupyter notebook).

The really valuable asset of SpringBoard is their mentors. This was my first time seeing a lot of Machine Learning concepts and my mentor really grilled me to make sure I know my stuff. Brilliant guy.

At $500/month I‚Äôd consider this program a great way to build your chops. Get ready to be challenged, earn a nice certificate, and interact with an experienced mentor. Student support from the staff and community is quite cool and staff quickly responds to emails. I wouldn‚Äôt be anywhere in data science without them.

SharpestMinds:

SharpestMinds offered what I‚Äôd been looking for a long time, being a guy in a smallish city with a smaller tech scene: a chance to interact with real data scientists. You need to apply to get in, and to acquire a mentor you should be like 80% of the way there on education/skillset, but once you‚Äôre in you‚Äôre golden.

The other mentees are bright and about as far along in data science as I, so the community Slack has interesting projects and posts. Often in the Slack, folks will say ‚ÄúMy company‚Äôs hiring, shoot me a message if you‚Äôre interested in X topic.‚Äù

After walking across the graduation stage with my MS in Pure Math, I felt like someone pushed me out a plane and said ‚ÄúQuick, get a job before you hit the ground.‚Äù SharpestMinds was like the 101st Airborne Division appearing out of the sky and handing me a parachute.

This is probably the best ‚Äòcareer services‚Äô of any MOOC or Bootcamp, ever, since they don‚Äôt get paid until you find a job. SM now averages one new hire every 3 days (an impressive feat considering how difficult landing an entry-level data science role is in 2019).

Part of its success stems from its specificity and selectivity: this NOT a program for people wondering ‚ÄúIs data science for me?‚Äù It‚Äôs for people with significant knowledge under their belt trying to land the best entry-level DS role. You will need to work hard to with SM and build an ambitious project with your mentor, and once you‚Äôre hired you owe them a small portion of your first year‚Äôs salary (they‚Äôre upfront about this arrangement). They also put a lot of focus on your resume and online presence, and the mentors coach you with mock interviews.

Essentially, this is not a MOOC or a bootcamp, but an education and career service for those trying to turn pro.

Coursera:

A great format for learning. It‚Äôs fundamentally different from code-along MOOCs but instead focuses on conceptual understanding. The quizzes here genuinely make you think and I never felt the videos were wasting my time.

I used Coursera for an introductory R course and introductory SQL, which helped me snag a lot of interviews because employers are always looking for someone who knows SQL. I probably don‚Äôt need to tell you about Coursera, but if you‚Äôre applying for jobs, make sure you have a few of their certificates on your resume and LinkedIn; the old guard don‚Äôt know all the new MOOCs but everyone knows (and respects) Coursera.

Be the Real McCoy:

Don‚Äôt forget that, to build coding skills, you‚Äôll need to stop by Edabit or Hackerrank and see if you can level up. Start easy and don‚Äôt get ahead of yourself, make those fundamentals really strong (you‚Äôll need them when you‚Äôre slicing strings backwards and building time series graphs). People become programmers by coding hours a day for months. You will inevitably need to find interesting datasets and test your knowledge yourself on them; no MOOC can recreate this. If you want to become a data scientist, don‚Äôt just knock out MOOCs to make your resume look good; actually build the skills employers are looking for.
Photo by Sebastian Pichler on Unsplash

Web crawling, also known as web scraping, data scraping or spider, is a computer program technique used to scrape a huge amount of data from websites where regular-format data can be extracted and processed into easy-to-read structured formats.

Web crawling is commonly used:

Web crawling basically is how the internet functions. For example, SEO needs to create sitemaps and gives their permissions to let Google crawl their sites in order to make higher ranks in the search results. Many consultant companies would hire companies to specialize in web scraping to enrich their database so as to provide professional service to their clients.

It is really hard to determine the legality of web scraping in the era of the digitized era.

Why does web crawling have a negative connotation:

Web crawling can be used in the malicious purpose for example:

Scraping private or classified information. Disregard of the website‚Äôs terms and service, scrape without owners‚Äô permission. An abusive manner of data requests would lead web server crashes under additionally heavy load.

It is important to note that a responsible data service provider would refuse your request if:

The data is private which would need a username and passcodes The TOS (Terms of Service) explicitly prohibits the action of web scraping The data is copyrighted

What reasons can be used to sue people?

Your ‚Äújust scraped a website‚Äù may cause unexpected consequences if you used it inappropriately.

HiQ vs LinkedIn

You probably heard of the HiQ vs Linkedin case in 2017. HiQ is a data science company that provides scraped data to corporate HR departments. Linkedin then sent desist letter to stop HiQ scraping behavior. HiQ then filed‚Ä¶
Neural Style Transfer

NST was first published in the paper ‚ÄúA Neural Algorithm of Artistic Style‚Äù by Gatys et al, originally released to ArXiv 2015 [7].

Several mobile apps use NST techniques, including DeepArt and Prisma.

Here are some more examples of stylizations being used to transform the same image of the riverbank town that we used earlier.

Neural style transfer combines content and style reconstruction. We need to do several things to get NST to work:

choose a layer (or set of layers) to represent content ‚Äî the middle layers are recommended (not too shall, not too deep) for best results.

Minimize the total cost by using backpropagation.

Initialize the input with random noise (necessary for generating gradients).

Replacing max-pooling layers with average pooling to improve the gradient flow and to produce more appealing pictures.

Code Implementation

Now for the moment you‚Äôve all been waiting for, the code to be able to make these images yourself. For clearer relationship between the code and the mathematical notation, please see the Jupyter notebook located in the GitHub repository.

Part 1: Import Necessary Functions

Part 2: Content Loss

We can generate an image that combines the content and style of a pair with a loss function that incorporates this information. This is achieved with two terms, one that mimics the specific activations of a certain layer for the content image, and a second term that mimics the style. The variable to optimize in the loss function will be a generated image that aims to minimize the proposed cost. Note that to optimize this function, we will perform gradient descent on the pixel values, rather than on the neural network weights.

We will load a trained neural network called VGG-16 proposed in 1, who secured the first and second place in the localization and classification tracks of ImageNet Challenge in 2014, respectively. This network has been trained to discriminate over 1000 classes over more than a million images. We will use the activation values obtained for an image of interest to represent the content and styles. In order to do so, we will feed-forward the image of interest and observe it‚Äôs activation values at the indicated layer.

The content loss function measures how much the feature map of the generated image differs from the feature map of the source image. We will only consider a single layer to represent the contents of an image.

Part 3: Style Loss

The style measures the similarity among filters in a set of layers. In order to compute that similarity, we will compute the Gram matrix of the activation values for the style layers. The Gram matrix is related to the empirical covariance matrix, and therefore, reflects the statistics of the activation values.

The output is a 2-D matrix which approximately measures the cross-correlation among different filters for a given layer. This, in essence, constitutes the style of a layer.

Part 4: Style Loss ‚Äî Layer‚Äôs Loss

In practice we compute the style loss at a set of layers rather than just a single layer; then the total style loss is the sum of style losses at each layer:

Part 5: Total-Variation Regularizer

We will also encourage smoothness in the image using a total-variation regularizer. This penalty term will reduce variation among the neighboring pixel values.

Part 6: Style Transfer

We now put it all together and generate some images! The style_transfer function below combines all the losses you coded up above and optimizes for an image that minimizes the total loss. Read the code and comments to understand the procedure.

Part 6: Generate Pictures

Now we are ready to make some images, run your own compositions and test out variations of hyperparameters and see what you can come up with, I will give you an example below. The list of hyperparameters to vary is as follows:

The base_img_path is the filename of content image.

is the filename of content image. The style_img_path is the filename of style image.

is the filename of style image. The output_img_path is the filename of the generated image.

is the filename of the generated image. The convnet is for the neural network weights, VGG-16 or VGG-19.

is for the neural network weights, VGG-16 or VGG-19. The content_layer specifies which layer to use for content loss.

specifies which layer to use for content loss. The content_weight weights the content loss in the overall composite loss function. Increasing the value of this parameter will make the final image look more realistic (closer to the original content).

weights the content loss in the overall composite loss function. Increasing the value of this parameter will make the final image look more realistic (closer to the original content). style_layers specifies a list of which layers to use for the style loss.

specifies a list of which layers to use for the style loss. style_weights specifies a list of weights to use for each layer in style_layers (each of which will contribute a term to the overall style loss). We generally use higher weights for the earlier style layers because they describe more local/smaller-scale features, which are more important to texture than features over larger receptive fields. In general, increasing these weights will make the resulting image look less like the original content and more distorted towards the appearance of the style image.

specifies a list of weights to use for each layer in style_layers (each of which will contribute a term to the overall style loss). We generally use higher weights for the earlier style layers because they describe more local/smaller-scale features, which are more important to texture than features over larger receptive fields. In general, increasing these weights will make the resulting image look less like the original content and more distorted towards the appearance of the style image. tv_weight specifies the weighting of total variation regularization in the overall loss function. Increasing this value makes the resulting image look smoother and less jagged, at the cost of lower fidelity to style and content.

The following code will generate the front image of this article if run for 50 iterations.

Here are a couple of rough examples from my own implementation after 50 iterations:

Style of ‚ÄòEscher Sphere‚Äô used to transform an image of the Goldengate Bridge.

Style of ‚ÄòSeated Nude‚Äô used to transform an image of the riverbank town image.

I recommend taking some of the images in the GitHub repository (or your own) and playing around with the hyperparameters and seeing what images you can make. However, to warn you, the training times are quite high unless you have access to a GPU, possibly taking several hours for one image.
The Easiest Python Numpy Tutorial Ever

Pie

Want to be inspired? Come join my Super Quotes newsletter. üòé

Python is by far one of the easiest programming languages to use. Writing programs is intuitive and so is reading the code itself ‚Äî it‚Äôs almost like plain English!

One of Python‚Äôs greatest strengths is its endless supply of powerful libraries. Many of these libraries are written at least partially in C / C++ for speed and a Python wrapper on top for easy usage!

Numpy is one such Python library.

Numpy is mainly used for data manipulation and processing in the form of arrays. It‚Äôs high speed coupled with easy to use functions make it a favourite among Data Science and Machine Learning practitioners.

This article will be a code tutorial ‚Äî the easiest one ever ‚Äî for learning how to use Numpy!

Creating arrays
Introduction: Fast R-CNN (Object Detection)

A beginners guide to one of the most fundamental concepts in object detection. shafu.eth ¬∑ Follow 3 min read ¬∑ Jul 18, 2019 -- 2 Listen Share

This is the second part of a three-part series, covering systems that combine region proposals with Convolutional Neural Networks (CNN). In the first part, we covered the R-CNN system. You will need to read that first to fully understand this article. You can find it here.

In this part, we will talk about the fast R-CNN system. This paper was published one year after the original R-CNN paper and directly builds on top of it. The R-CNN paper was a major breakthrough in 2014, combining region proposals with a CNN. But it had some problems:

It was slow : You had to calculate a feature map (one CNN forward pass) for each region proposal.

: You had to calculate a feature map (one CNN forward pass) for each region proposal. Hard to train : Remember that in the R-CNN System we had 3 different parts (CNN, SVM, Bounding Box Regressor) that we had to train separately. This makes training very difficult.

: Remember that in the R-CNN System we had 3 different parts (CNN, SVM, Bounding Box Regressor) that we had to train separately. This makes training very difficult. Large memory requirement: You had to save every feature map of each region proposal. This needs a lot of memory.

Fast R-CNN System

So, how did the author try to solve those problems. The major thing introduced in this paper is the following:

We only have one system, that we can train end-to-end.

If you only take one thing with you from this article it‚Äôs this: We combine the three different parts that we had in the R-CNN system (CNN, SVM, Bounding Box Regressor) into one architecture.

Architecture

Fast R-CNN architecture. First image in the paper.

This architecture looks more complicated than it actually is. It works like this:

1. Process the whole image with the CNN. The result is a feature map of the image.

The CNN input and output

2. For each region proposal extract the corresponding part from the feature map. We will call this the region proposal feature map. We take the region proposal feature map from the feature map and resize it to a fixed size with the help of a pooling layer.

This pooling layer is called the Region of interest (RoI) pooling layer. If you want to find out more about it, here you can find a greate explanation.

RoI pooling layer

3. Then we flatten this fixed sized region proposal feature map. This is now a feature vector, that always has the same size.

4. This feature vector is now the input to the last part. These are fully connected layers that have 2 outputs. The first is the softmax classification layer, where we decide which object class we found. The second it the Bounding Box Regressor, where we output the bounding box coordinates for each object class.

Results

The fast R-CNN trains the VGG16 network 9 times faster than R-CNN. But the amazing thing about this system is this:

The inference is 213 times faster and achieves a higher mAP. Wow!

Conclustion

In the second part of this series, we talked about the fast R-CNN system and how it tried to improve the R-CNN system. Rember that only one year passed between the two papers and the system improved dramatically, especially in inference time. This illustrates how fast the whole deep learning space moves.

And again, the most important thing summarized in one sentence:

We now have one end-to-end system that we can train with back-propagation.

Thank you for reading and keep up the learning!

If you want more and stay up to date you can find me here:
Getting an internship in Machine Learning as an undergraduate student is tough. Really tough‚Ä¶ Most of the well-known companies are looking for Ph.D. students with publications in prestigious journals. How to increase the chances of getting an internship in ML if you can‚Äôt satisfy the above?

Photo by Hunters Race on Unsplash

Last year, I have spent hours sending applications to apply for Machine Learning/AI internships. As an Electronic Engineering student, I‚Äôve found it particularly difficult to get even an interview, even though I had some relevant experience in Machine Learning. During my search for internships, I have noticed a few common things which companies are looking for. Based on these facts, I have decided to change my strategy for sending CVs, which finally resulted in me getting a job as a Machine Learning Research Intern. If you are planning to apply for Machine Learning internships, or you are struggling to get one, I hope that this article will help you land your dream job!

I also wrote another article that helps you prepare for the interview. Link is here.

Big companies = Big competition

Photo by Pawe≈Ç Czerwi≈Ñski on Unsplash

The first mistake I made, was to apply to big, well-known companies. Companies such as Google, Amazon or Apple are getting hundreds of applications per day and it is very difficult to get even through the first recruitment stage for an internship. If you feel that you have goods skills in Machine Learning, with substantial experience in this field then go for it. Otherwise, it might be better to focus on targeting smaller, less known companies to maximize your chances of getting hired. For example, have a look at the required basic qualifications for Machine Learning Intern at Amazon:

Internship requirements for Machine Learning Internship at Amazon, found on Linkedin

Woah! You see, that is what I meant by saying tough requirements. Preferably, you should be a P.h.D. student with several publications in Machine Learning. If you are undergraduate, like me, you are clearly at the disadvantage here.
Introduction

Starbucks is a global coffee company selling coffee, tea, espresso drinks, bakery, and grab-and-go offerings in 75 countries. One of the company‚Äôs values is ‚ÄúCreating a culture of warmth and belonging, where everyone is welcome.‚Äù Therefore, it utilizes many channels to market its products from social media to TV spots and ads. Starbucks executes its extraordinary marketing strategy by deploying a combination of marketing media channels, where it creates brand recognition. Starbucks does not only understand its products and customers, but also keeps up with how its customers use technology. Starbucks App enables customers to keep track of the available offers and happy hour deals at participating stores. It allows customers to earn and collect stars (collect two stars per $1) that can be redeemed in-store or via the app.

‚ÄúWith nearly 100 million customers in our stores every week, we‚Äôre looking for more opportunities to engage directly and personally, providing them with special benefits and offers that are meaningful‚Äù ‚Äî Matt Ryan, executive vice president and chief strategy officer for Starbucks.

Here, we are going to investigate and analysis three files that simulate how people make purchasing decisions and how promotional offers influence those decisions. A sneak of the final data after being cleaned and analyzed: the data contains information about 8 offers sent to 14,825 customers who made 26,226 transactions while completing at least one offer. Below are two examples of the types of offers Starbucks sends to its customers through the app to encourage them to purchase products and collect stars.
Fun fact: Scikit-Learn doesn‚Äôt have any distance metrics which can handle both categorical and continuous data! How can we then use clustering algorithms, e.g. k-NN, if we have a dataset with mixed-type variables?

Photo by Fredy Jacob on Unsplash

Update (27/07/19) ‚Äî The package has been released at PyPI as Distython. I have published an article to explain how it works.

The big problem that I have faced during my summer internship in the IT Innovation Centre was the lack of existing implementations of distance metrics which could handle both mixed-type data and missing values. It has started my long search for algorithms which can satisfy those requirements. Several research papers later, I have discovered quite interesting distance metrics which can help to improve the accuracy of your machine learning model when dealing with mixed-type data, missing values, or both. I have implemented them in my spare time and published their code implementation on the Github so you can use it easily with Scikit-Learn. But how? I will explain that in this tutorial!

What I love about Towards Data Science, is that it attracts many like-minded people who are passionate about AI and Data Science. That‚Äôs why I would like to connect with you on Linkedin! You can also leave any feedback and questions via my personal website.

Overview of Heterogenous Distance Metrics

Photo by Annie Spratt on Unsplash

Before we start, I would like to recommend to look at this paper if you want to get a more in-depth understanding of the algorithms I will talk about. My main goal here is to provide you with an intuitive understanding of those algorithms so you can use my article as a quick reference sheet. You can find the practical part with a code at the end of the article. Let‚Äôs get started!

Distance Metrics

But wait‚Ä¶ What are actually the distance metrics? The distance metrics measure the distance between two instances in the dataset. They measure the similarity between instances based on their features. For example, imagine patients of a‚Ä¶
Understanding the types of data in a business/organization

Try to google it, and you will guarantee to find various sources each with their own versions (some said 3 types of data, some said 5 types, some even said 13 types). We make it easy for you by summarizing them and take your comprehending into the next level Rendy Dalimunthe ¬∑ Follow Published in Towards Data Science ¬∑ 4 min read ¬∑ Jul 18, 2019 -- Listen Share

Before you start to rolling out your data management initiatives, be it Master Data Management, Enterprise Data Warehouse, Big Data Analytics or whatever it is, you need to start by understanding the very basic ingredient: the data. Only by thoroughly recognizing their characteristics, you will know the right way on how to treat each of them.

‚ÄúData is a precious thing and will last longer than the systems themselves‚Äù Tim Berners-Lee

So let‚Äôs get started!

Transactional Data

This type of data describes your core business activities. If you are a trading company, this may includes the data of your purchasing and selling activities. If you are a manufacturing company, this will be your production activities data. If you are a ride-hailing or cab company, this will the trip data. In a very basic organizational operations, the data related to the activities of hiring and firing employees can also be classified as transactional data. As a result, this kind of data has a very huge volume in comparison with the other types and usually created, stored, and maintained within the operational application such as ERP system.

Master Data

It consists of key information that make up the transactional data. For example, the trip data in a cab company may contain driver, passenger, route, and fare data. The driver, passenger, locations, and basic fare data are the master data. The driver data may consists the name of the driver and all of the associated information. So does the passenger data. Together, they make up the transactional data.

Master data usually contains places (addresses, postal-coded, cities, countries), parties (customers, suppliers, employees) and things (products, assets, items, etc.). It is application-specific, meaning that its uses are specific for the application with business process related to it, e.g: the employees master data is created, stored, and maintained within the HR application.

By now, you should get some grasp of understandings that master data is relatively constant. While the transaction data is created at a lightning speed, the master data is somehow constant. The trip data is created in any second but the list of the driver will remain the same unless there‚Äôs a new driver on-board or get kicked out.

Nowadays, processes within the organization are usually so inter-dependable, which means that one process conducted in one system is related to the process conducted in other system. They may use the same master data. If each system manage their own master data, potential duplication and inconsistencies may arise. For instance, a customer may be stored as Rendy in system A, but listed as Randy in system B, although Rendy and Randy is actually the same entity. But no need to worries, there‚Äôs a discipline to manage this kind of situation. It‚Äôs called Master Data Management.

Reference Data

Reference data is a subset of master data. It is usually a standardized data that governed by certain codification (e.g. list of Countries is governed by ISO 3166‚Äì1. There‚Äôs an easy way to differentiate reference data from master data. Always remember that reference data is way less volatile than master data. Let‚Äôs back again to our cab company. Tomorrow, the day after tomorrow, or next week, the list of driver may change whenever there‚Äôs a new person onboard or kicked out. But I can guarantee you that the list of countries will remain the same even 2 decades from now, unless there‚Äôs a little land that declare its independence.

Reporting Data

It‚Äôs an aggregated data compile for the purpose of analytic and reporting. This data consist of transactional, master, and reference data. For example: Trip data (transaction + master) on the 13th day of July in Greater London region (reference). Reporting data is very strategic and usually being produced as ingredient of decision making process.

Metadata

It‚Äôs a data about data. Sounds confusing? Indeed. It‚Äôs the type of data that got me dizzy in the first time I enter the data management field. Thankfully, this beautiful picture make it easy for me to comprehend what metadata actually is.

Data & its metadata

If I ask you a question: what is the color of the cat? Immediately by just looking at the data you can confidently answer my question. It‚Äôs grey. But what if I come up with another questions: when and where this picture be taken? Chances are high that you will not be able to give me the right answer by only looking at the data. And here is where the metadata come to rescue. It gives you the complete information about the data including when and where it was taken.

So metadata is giving you the answer to any question that you cannot answer by just looking at the data. That‚Äôs why it said: data about data.
Data Scientist‚Äôs toolkit ‚Äî How to gather data from different sources

Photo by Jakob Owens on Unsplash

Not so long ago!

Do you remember the time when data was sent to you in an external hard drive for your analysis or model building.

Now ‚Äî as a data scientist, you are not limited to those means. There are several ways of storing data, sharing data as well as different sources to acquire data, augment data.

Below, I‚Äôm listing down several ways of gathering data for your analysis

Table of contents:

CSV file Flat File (tab, space, or any other separator) Text File (In a single file ‚Äî reading data all at once) ZIP file Multiple Text Files (Data is split over multiple text files) Download File from Internet (File hosted on a server) Webpage (scraping) APIs (JSON) Text File (Reading data line by line) RDBMS (SQL Tables)

In Python, a file is characterized as either text or binary, and the difference between the two is important

Text files are structured as sequence of lines. Each line is terminated with a special character called EOL or End of line character. There are several types but most common are

or ,

A Binary file type is basically any type of file that‚Äôs not a text file. Because of their nature, binary file can only be processed by an application that know or understand the file‚Äôs structure

1. CSV File

Most common format for storing and sharing dataset is comma separated format or a csv file. pandas.read_csv() is THE most useful and powerful method and I strongly recommend you to read its documentation . By using appropriate kind of sep you can load several types of data in dataframe

import pandas df = pd.read_csv('data.csv', sep =',')

2. Flat File

but at times you might receive file that‚Äôs tab separated or a fixed width format or‚Ä¶
Introduction

Why?

There are many articles and courses dedicated to the latest ML/AI research aimed at training bigger models and achieving higher classification accuracy. This is great for research and academia and pushing the limits of what AI can do. However, these are not really tailored for poor student practitioners starting off with their first major AI projects or penny conscious entrepreneurs looking to build an MVP of their cool revolutionary idea.

What?

In this work I take a budgeted approach to model training and try to answer the question:

What is the minimum, practical cost to complete a real world AI project?

The problem that I chose for this was an Image Classification problem.This article captures the process I followed and key budgeting lessons learned from each step.

Summary

The answer is roughly $300 ‚Üí This is the amount it takes to train a well performing Computer Vision model using cloud computing. Incidentally (or not) this is also the amount of credit that Google gives as incentive to get started on the Google Cloud Platform (GCP) [1].

The breakdown of the budget is given below. The two rightmost columns list the instances from AWS and GCP that are most amenable to this task. The cost is an average of the instances listed in these columns. At present there is not a lot separating the two cloud providers in terms of cost.
Generating Startup names with Markov Chains

The most interesting applications of Machine Learning are, without a doubt, the generative models.

The idea of finding patterns in data and generating new content that at the same time is similar to your data, but unique in its own way, has always fascinated me.

So I have decided to develop a simple text generator to create Startup names using Markov Chains.

But first of all, a short introduction to Markov Chains üîó

Markov Chains

A Markov chain is a model of some random process that happens over time.

Markov chains are called this way because they follow a rule called the Markov property. The Markov property says that whatever happens next in a process only depends on how it is right now (the state).

For instance, consider the example of predicting the weather for the next day, using only the information about the current weather. By analysing some real data, we may find these conditions:

Given that today is sunny, tomorrow will also be sunny 90% of the time

Given that today is sunny, tomorrow will be rainy 10% of the time

Given that today is rainy, tomorrow will also be rainy 50% of the time

Given that today is rainy, tomorrow will be sunny 50% of the time

This is modelled via the Markov Chain below, where each circle is a state and the numbers in the arrows represent the probabilities of changing from the current state.

So if we want to make a prediction for tomorrow, we just need to verify the current state we are in (sunny or rainy) and use the transition probabilities to calculate which next state is more likely.

An in-depth explanation of Markov Chains, with some cool animations, can be found here: http://setosa.io/ev/markov-chains/

Applying Markov Chains to text generation

So, how can we apply this idea to generate text? Well, it's quite simple actually.

Names or sentences are basically a sequence of characters and those sequences follow some patterns.

For instance, if I asked you to give words that started with whe__, you would quickly come up with the words when, where, whenever, etc. While wheeziness is a perfectly valid word, it's less frequent considering the initial whe_. In other words, given the state whe, we will most likely change to the states when, where or whenever than to the state wheeziness.

So how can we build a model that capture these probabilities given our data?

For this, I'll show you how to build a simple Markov Chain using the words startup, statistic and artist. First of all, we will list all the states transitions for each tuple of 3 characters:

Startup

sta -> tar

tar -> art

art -> rtu

rtu -> tup Statistic

sta -> tat

tat -> ati

ati -> tis

tis -> ist

ist -> sti

sti -> tic Artist

art -> rti

rti -> tis

ist -> ist

Now, if you pay close attention to the states, you will notice that some of them are shared among different tuples. To better visualize that, let's create a dictionary where each entry is a state, and the values are the next states and its weights.

{

"sta": {

"tar": 1,

"tat": 1

},

"tar": {

"art": 1

},

"art": {

"rtu": 1,

"rti": 1

},

"rtu": {

"tup": 1

},

"tat": {

"ati": 1

},

"ati": {

"tis": 1

},

"tis": {

"ist": 2

},

"ist": {

"sti": 1

},

"sti": {

"tic": 1

},

"rti": {

"tis": 1

}

}

Ta-da! That's our Markov Chain. Simple, isn't it? (PS: Technically, Markov Chains are defined with a transition matrix with probabilities, not weights. We could easily transform into a transition matrix, but for the problem we have in mind, this is a better way of visualizing it).

Now, how can we generate new data with this?

There are basically four steps, let's go through each one of them:

Step 1: Start with some initial random state

You could select any of the states as a starting position, however, you will most likely generate text that doesn't make any sense. For instance, rtu is a valid initial state, but you won't find a word in real life that starts with those letters (none that I can't think of it, at least)

A better approach is to keep track of the starting states in another dictionary, and selecting the first state from there. In our case, the possible initial states are sta(as both startup and statistic start with sta) and art. For our example, let's select sta.

Step 2: Select randomly one of its transition states, considering its weights

For the tuple sta, you can go to tar or tat, both of them with the same probability (same weight). In a real case scenario, they would have different weights considering the distribution found in your dataset, but as we have just used three words, they have equal weights. Let's "randomly" select the tuple tar.

Step 3: Append the new state to your generated text

So far, we have started with the state sta and transitioned to the state tar. So our current generated word is star.

Step 4: Repeat step one using the new state, until a stop character is found, or until you are happy with your result

Now, for our current state tar, the only possible state is art, so our generated word become start.

Now let's continue the algorithm in a faster way. From art, we can go to rti or rtu. Let's select rti. If you continue to apply the algorithm, you will quickly generate our new word: Startist, which is a mix of startup and artist.

Even though the example is quite simple, it demonstrates the potential of Markov Chains.

Now that we have "implemented" by hand a Markov Chain, let's do in Python using real data where you can get actual useful results.

Let's code!

Let's start by importing some modules. We will only need two: pandas, to read CSV data, and random to (unsurprisingly) generate random numbers.

import pandas as pd

import random

As a dataset for our Startup name generator, we are going to use a 2015 dump from CrunchBase with around 18k companies.

The dataset is not that big, but you will see that Markov Chains work pretty well even with a database much smaller than this.

Reading our companies data is pretty straightforward: pandas read_csv function accepts a URL as a parameter and returns a data frame. We have also removed symbols and transformed the names to lower case.

As we have discussed previously, the simplest way to model the data structure for a Markov Chain is a dictionary containing the states and transitions weights.

chain = build_markov_chain(companies['name'].tolist(), 3)

print(chain['sta'])

If you run the above code, you will get this result:

{

'tar':290,

'tat':151,

'ta.':52,

'ta ':35,

'tac':55,

'tag':43,

'tal':46,

'tay':34,

'tau':22,

'tad':14,

'tam':19,

'tas':19,

'taq':5,

'tan':92,

'tab':23,

'tap':6,

'tak':8,

'tai':22,

'taf':16,

'tax':5,

'ta‚Ñ¢':1,

'tah':2,

'tav':5,

'tae':1,

'taj':1,

'taw':1,

'taa':2,

'taz':1

}

What does this mean? These are the list of next states and weights on the Markov Chain, considering that the current state is the tuple sta. The higher the next state weight, more likely its transition to it.

For instance, if you take the first state tar, it has the largest weight on this state list. Intuitively, it makes sense, as it is probably capturing the occurrence of the word startup.

Now we need to build a function that returns a random tuple from the chain considering its weights.

Finally, here is where the magic happens: let's generate some new words.

Let's go step by step in our generate function.

tuple = select_random_item(chain['_initial'])

result = [tuple]

Remember that we mentioned that is better to keep track of the initial tuples and selecting one of those as the initial state? That's exactly what we are doing here.

while True:

tuple = select_random_item(chain[tuple])

last_character = tuple[-1]

if last_character == '.':

break

result.append(last_character)

This is where we are navigating through our Markov Chain, considering its probabilities. We are selecting a random weighted next state and appending the last character of this state to our result string. However, if the last character is a period, we stop our generation, as this is the ending of our chain.

We could add additional rules such as generating words given a minimum or maximum length, but let's keep it simple for now.

generated = ''.join(result)

if generated not in chain['_names']:

return generated

else:

return generate(chain)

Finally, we join all of the generated characters together, and we do the last verification. As nothing prevents the Markov Chain from generating an already existing name, and as we are interested in creating new names, we will simply generate a new one if the generated name is already in our database.

Results

Here is a couple of examples from our Startup Name Generator‚Ñ¢¬Æ.

Domos

Hup Online

Vubanky

Acara

Ignaly

iFly

Pretty cool, huh? üòé

More ideas

One final idea, what if we generated Startup names specific for each industry? It would be awesome, don't you think?

Let's do it, it will be extremely easy üôÉ

The only thing we have to do is to build our Markov Chain considering only examples from the industry we are interested in.

And here are our results:

Travel Startups

print(generate_amount_by_category('Travel',5))

Pango

Movology

Nextrive

Triptel

Stingi

Technology Startups

print(generate_amount_by_category('Technology',5))

Naco Innovation

Kicksense

NetWatch

Chony

Datars

Try yourself

You can try yourself using this Google Colab link or downloading the source code directly from my GitHub.

What are your thoughts? Any suggestion for new content? Feedbacks? Let me know in the comments.

Hope you have enjoyed it :)
A Recipe for using Open Source Machine Learning models

Photo by Luca Bravo on Unsplash

Machine learning continues to produce state of the art (SOTA) results for an increasing variety of tasks and more companies are looking to ML to solve their problems. With the incredibly rapid pace of machine learning research, many of these SOTA models come from academic and research institutions which open source these models. Often, using one of these open source models to bootstrap your machine learning efforts within your company can be much more effective than building a model from scratch.

However, these models are often released by researchers whose focus isn‚Äôt necessarily to enable easy use and modification of their models (though there are many exceptions). Using these open source models for your tasks can be quite difficult.

In this post, my goal is to provide a recipe you can follow to evaluate and use open source ML models to solve your own tasks. These are the steps I‚Äôve used over and over again in my own work (as of this writing I have anaconda environments set up over 15 open source models). As my work is mostly using deep learning for vision and NLP, my focus here is specifically on using neural network-based models.

Whether you‚Äôre trying to use machine learning to solve real problems within your company or experiment with some fun SOTA results at home, my hope is that after this post, you‚Äôll have a path to take an open source model and modify and use it to address your own task with your own dataset.

Step 1: Naming your task

The first step is figuring out what your particular task is called in the research literature so you can successfully search for it. This can initially be quite frustrating. For example, finding all the instances of a dog in a picture would be an ‚Äúobject detection‚Äù task. But if you want to know exactly which pixels in the picture correspond to dogs that‚Äôs called ‚Äúimage segmentation.‚Äù

There are a few ways you can try to figure this out. First, if you happen to know any ML researchers or practitioners, definitely start there. Another option is to ask in r/machinelearning or r/learnmachinelearning. If none of these pan out, the next step is to google to the best of your ability. As you land on research papers you‚Äôll often see the name commonly associated with the task in the literature.

Step 2: Finding papers and code

Once you know what to search for, the next step is to find those open source models that best suit your task. There are a few resources that are helpful here:

paperswithcode: A repository of papers and associated code, organized by task. This is a really good starting point, especially if it‚Äôs a well-known task.

arxiv-sanity: Many open source models are associated with research papers. Most papers in machine learning are (fortunately!) openly published on arxiv. Searching arxiv for recent papers that solve for your task is another good place to start. Not all published papers here have code associated with them. If you find a paper you like, try searching for ‚Äú<paper name> github‚Äù to see if the code has been released.

Kaggle: If there happens to be a Kaggle competition with a task similar to yours, this can be a great way to get high quality, state of the art models. Pay particularly close attention to winner blogs for past competitions, these often have great explanations and code. The little tricks that were used to win the competition can often be really valuable for your task as well.

Dataset benchmarks: If there‚Äôs a benchmark dataset that‚Äôs similar to the task you‚Äôre working on, the leaderboard for that benchmark is a quick way to find papers with demonstrably SOTA results.

Google: For standard/common tasks like image segmentation, searching for ‚Äúimage segmentation github‚Äù, ‚Äúimage segmentation pytorch‚Äù or ‚Äúimage segmentation tensorflow‚Äù will give you a lot of results.

Step 3: Read the papers

This can be intimidating because academic papers can be pretty inaccessible, even to experienced software engineers. But if you focus on the abstract, introduction, related work, results, and delay a lot of the deep details/math for later readings, you‚Äôll find you can get a lot out of the paper and a deeper understanding of the problem.

Pay particularly close attention to the dataset(s) they use and the constraints of those datasets or their model. Often you‚Äôll find that the constraints may not be applicable to you and are fundamental to the model design. For example, classification models for imagenet expect there to be one and only one salient object in an image. If your images have zero, one or more objects to identify, those models are probably not directly applicable. This isn‚Äôt something you want to find out after investing the time needed to bring up the model.

Also, follow some of the references, especially those you see in multiple papers! You‚Äôll regularly find that at least one of the references provides a very clear description of the problem and dramatically increases your understanding. Referenced papers may also end up being more useful and may have nicer code associated with them, so it‚Äôs worth doing a little digging here.

Step 4: Make sure the code is usable

Once you‚Äôve found a paper with open source code, make sure it‚Äôs usable. Specifically:

Check the license: While a lot of code is released under liberal open source licenses (MIT, BSD, Apache etc), some of it isn‚Äôt. You may find the model has a non-commercial use only license, or no license at all. Depending on your use case and company, the code may or may not be usable for you.

Check the framework: If you‚Äôre working with a particular framework (eg. Tensorflow, Pytorch) check the framework the model is built in. Most of the time you‚Äôre stuck with what you get, but sometimes there‚Äôs a reimplementation of the model in your preferred framework. A quick Google to check for this (eg. ‚Äú<paper name> pytorch‚Äù) can save you a lot of trouble.

Check the language: Similarly, if the model is in Lua and you‚Äôre not a Lua developer, this can be really painful. See if there‚Äôs a reimplementation in the language of your choice (often Python, since in deep learning Python should be part of your repertoire), and if not you might be better off finding another model.

Check the coding style: Researchers aren‚Äôt all software engineers so you can‚Äôt have as high a bar as for other open source projects, but if the code is a total mess you may want to look for another model.

Step 5: Get the model running

Results from NVIDIA‚Äôs StyleGAN trained on a custom furniture dataset

Once you‚Äôve found a model you think is a good fit, try to get the model running. The goal here is to run the training and inference loop for the model as-is, not to get it running on your specific dataset or to make any significant modifications. All you want to do is make sure that you have the right dependencies and that the model trains and runs as advertised. To that end:

Create a conda environment for the model: You may be trying out multiple models, so create a conda environment (assuming Python) for each model (nvidia-docker is another option here, but personally I find it to be overkill).

I‚Äôll often set up my environments like so: conda create -n <name of the github repo> python=<same version of python used by the repo>

A quick way to figure out which version of python the repo is using is to look at the print statements. If there are no parens, it‚Äôs python 2.7, otherwise 3.6 should work.

Install the libraries: I highly recommend starting off by installing the exact same version of the framework that the original code used to start. If the model says it works with pytorch>0.4.0 , don‚Äôt assume it‚Äôll work with pytorch 1.0. At this stage, you don‚Äôt want to be fixing those kinds of bugs, so start with pytorch=0.4.0 . You can install a particular version of a framework (eg. pytorch) with the command conda install pytorch=0.4.0 -c pytorch . A lot of code won‚Äôt have a requirements.txt file, so it may take some sleuthing and iterating to figure out all the libraries you need to install.

Get the original dataset and run the scripts: At this point, you should be able to download the original dataset and run the testing and training script. You‚Äôll probably have to fix some paths here and there and use the README and source to figure out the correct parameters. If a pre-trained model is available, start with the testing script and see if you‚Äôre getting similar results to the paper.

Once you have the testing script running, try to get the training script up. You‚Äôll probably have to work through various exceptions and make slight modifications to get it to work. Ultimately your goal with the training script is to see the loss decreasing with each epoch.

If it‚Äôs straightforward (ie. only requires changing some command-line flags), at this point you might try running training script on your own dataset. Otherwise, we‚Äôll do this in step 7.

Step 6: Create your own testing notebook

At this point, you‚Äôve confirmed that the model works and you have the right environment set up to be able to use it. Now you can dig in and start really playing with it. At this point, I recommend creating a Jupyter notebook, copy-pasting in the testing script, and then modifying till you can use it with a single item of data. For example, if you‚Äôre using an object detection model that finds dogs in an image, you want a notebook where you can pass it a picture and have it output the bounding boxes of the dogs.

The goal here is to get a feel for the inputs and outputs, how they must be formatted and how exactly the model works, without having to deal with the additional complexity of training or munging your own data into the right formats. I recommend doing this in a Jupyter notebook because I find that being able to see the outputs along each step is really helpful in figuring it out.

Step 7: Create your own training notebook with your dataset

Now that you have some familiarity with the model and data, it‚Äôs time to try to create a training notebook. Similar to step 6, I start by copying and pasting in the training script, separating it into multiple cells, and then modifying it to fit my needs.

If you‚Äôre already feeling comfortable with the model, you may want to go directly to modifying the training notebook so it works with your dataset. This may involve writing dataloaders that output the same format as the existing dataloaders in the model (or simply modifying those dataloaders). If you‚Äôre not yet comfortable enough to do that, start by just getting the training script to work as-is in the notebook and removing code that you don‚Äôt think is useful. Then work on getting it to work with your dataset.

Keep in mind the goal here isn‚Äôt to modify the model, even if it‚Äôs not quite solving the exact task you want yet. It‚Äôs just to get the model working with your dataset.

Step 8: Start modifying the model to suit your task!

By this point, you should have a notebook that can train the model (including outputting appropriate metrics/visualizations) and a notebook where you can test new models you create. Now is a good time to start to dig in and make modifications to the model (adding features, additional outputs, variations etc) that make it work for your task and/or dataset. Hopefully having the starting point of an existing state of the art model saved you a lot of time and provides better results than what you might get starting from scratch.

There‚Äôs obviously a lot happening in this step and you‚Äôd use all your existing model building strategies. However, below are some pointers that may be helpful specifically when building off of an existing model.

Modify the dataset before modifying the model: It‚Äôs often easier to munge your data into the format the model expects rather than modifying the model. It‚Äôs easier to isolate problems and you‚Äôre likely to introduce fewer bugs. It‚Äôs surprising how far you can sometimes push a model just by changing the data.

Reuse the pre-trained model as much as possible: If your model changes aren‚Äôt drastic, try to reuse the pre-trained model parameters. You may get faster results and the benefits of transfer learning. Even if you expand the model, you can often load the pre-trained parameters into the rest of the model (eg. use strict=False when loading the model in pytorch).

Make incremental changes and regularly check the performance: One benefit of using an existing model is you have an idea of the performance you started with. By making incremental changes and checking the performance after each one, you‚Äôll immediately identify when you‚Äôve made a mistake or are going down a bad path.

Ask for help: If you‚Äôre totally stuck, try reaching out to the author and asking for some pointers. I‚Äôve found they‚Äôre often willing to help, but remember they‚Äôre doing you a favor and please act accordingly.

Automatically texturing a 3D model using neural renderer

Step 9: Attribute and Contribute

Depending on the license and how you‚Äôre distributing your model, you may be required to provide attribution to the developer of the original code. Even if it‚Äôs not required, it‚Äôs nice to do it anyway.

And please contribute back if you can! If you come across bugs and fix them during your own development, submit a pull request. I‚Äôm sure well-written bug reports are welcome. Finally, if nothing else, sending a quick thank you to the author for their hard work is always appreciated.
How to Choose Between Multiple Models

In a previous article we discussed the concepts of underfitting and overfitting, how they can lead to models that don‚Äôt match the available data, how to identify each issue, and how to identify models that do fit the data well. These concepts can help you avoid major blunders and generate models that fit the data reasonably accurately; however, there are an incredible number of models that meet that description. This means that the next step, beyond generating a model that fits decently, is identifying which of the possible models fits best.

When determining how well a model fits the data set it‚Äôs important to calculate statistical values comparing the model predictions to the data set. This is beyond the scope of this conceptual article, but more information can be found in Data Science from Scratch or in Practical Statistics for Data Scientists. In this article we‚Äôll discuss the process of developing, validating, and testing models.

What are the model development, validation, and testing phases and why are they necessary?

The fundamental issue to be aware of here is that you cannot trust a model that you‚Äôve developed simply because it fits the training data well. This is for a simple reason: You forced the model to fit the training data well. If after creating a model the statistical calculations show that it matches the data well, this means that it‚Äôs possible to use mathematical methods to force a model to match the data well. What it doesn‚Äôt mean is that the model is capturing the trends that are really occurring, or that the model is able to predict other circumstances. The example of the overfit model in my previous article is a great way to highlight this.

The solution to this is model validation. Validation is the practice of using the model to predict the output in other situations for which you have data, and calculating those same statistical measures of fit on those results. Note that this means you need to divide your dataset into two different data files. The first is a training data set, which you use to generate your models. The second is a validation data set, which you use to check the accuracy of your‚Ä¶
How 90% of Drivers Can be ‚ÄúAbove Average‚Äù or Why You Need to be Careful When Using Statistics

Most people see the headline ‚Äú90% of Drivers Consider Themselves Above Average‚Äù and think ‚Äúwow, other people are terrible at evaluating themselves objectively.‚Äù What you should think is ‚Äúthat doesn‚Äôt sound so implausible if we‚Äôre using the mean for average in a heavily negative-skewed distribution.‚Äù

Although a headline like this is often used to illustrate the illusion of superiority, (where people overestimate their competence) it also provides a useful lesson in clarifying your assertions when you talk statistics about data. In this particular case, we need to differentiate between the mean and median of a set of values. Depending on the question we ask, it is possible for 9/10 drivers to be above average. Here‚Äôs the data to prove it:
Fables of Data Science ‚Äî Anscombe‚Äôs quartet

A fable is a short story that teaches a lesson or conveys a moral. Here we explore Anscombe‚Äôs quartet and see what horror it warns us against. dearC ¬∑ Follow Published in Towards Data Science ¬∑ 5 min read ¬∑ Jul 18, 2019 -- Share

Once upon a time..

..in a land far far away there used to live a man called Francis John ‚ÄúFrank‚Äù Anscombe. He was a statistician of great repute.

He wandered far and wide meeting many fellow practitioners and sat in the Citadel training young apprentices in the sacred art of statistics. The more he talked and trained people he saw a dangerous trend. People tended to ignore visualizations in favor of summary statistics. Everyone said it was too much effort to plot the data.

A worried Frank is a Red Frank

This was a dangerous, Frank was very worried. This plague was spreading was spreading fast, he did not have the time counter it. He had to do something, something that would help stop it.

So he rushed to talk to the council of olders but every one just laughed at him.

‚ÄúWhen you have the data‚Äôs mean,

What else do you want to examine?

Standard deviation is just sublime,

Plotting is a big waste of time!‚Äù ‚Äî Master PoorPoetrix

Frank had never anticipated that the Council too was poisoned, he had to open their eyes.

So he traveled, traveled northwards into the Lands of Always Winter, to the peak of the Frostfang mountains. There he sat, deep in meditation. And after 5 minutes he realized that it is shit cold out in the mountains and he should have packed more woolen clothes.

Shivering in cold he went around looking for a cave to settle in and as luck would have it he found one.

There Frank meditated and he mediated deep,

though some claimed he just fell asleep.

In his dreams he saw numbers, numbers and more numbers and it was then that he realized that the numbers were the key to his solution. Day after day he pondered on those numbers and finally he solved their mystery.
Photo by Kaleidico on Unsplash

Influencer marketing is not a foreign concept to us anymore. In the US retail business, there are only 33% of the retailers haven‚Äôt used or do not intend to use influencer marketing of any kind. However, it is a great way to lift your brand awareness through word-of-mouth advertising. The tricky part is how to find the right person and/or channel to promote your brand.

If you Google for the biggest influencers on social media, you are running against a wall. Instead, you should start with your audience pool and then extend the connections. For example, obtaining your commenters‚Äô information and contacting them about news is a good way to find your influencers. The best way to get those candidates is through web scraping amongst your audience. A lot of businesses don‚Äôt have the budget for expensive tools like data integration. However, there are plenty of cheap alternatives out there for you to gather valuable information, and one of them is web scraping software.

What is Web Scraping

Web scraping is a technique to automate the process of data extraction. It involves the process of parsing a website, and collect the snippet of data for your needs. I am going to show you how to use a web scraping tool and make these data available for digital marketing. There is no programming skill required to conduct web scraping. The tool that I used is Octoparse, and I will explain why it yields incredible value for marketing professionals.

Find your ‚ÄúEvangelists‚Äù.

According to Joshua, comments are the place where we can leverage to extend our marketing strategy. I am not saying we leave comments and clap hands for your commenters. When the user name of the commenters is clickable, it is possible to connect with them by extracting the profile information. Whenever we have a new piece of content, we reach out to these people. This way we turn commenters into evangelists. Besides, you can go a little more creative by snowballing this method and creating your evangelist‚Äôs pool to further your marketing process.

Twitter Evangelists:

Blindly sending messages to beg for tweets won‚Äôt work. A great start off is to use your audience pool. The idea is:
This three-part series about photography and imaging examines many of the recent technical and social developments. In part 1, we look at the 190-year history since photography‚Äôs invention, noting the rapid pace of change in the medium and sudden transition from film to digital and rise of The Smartphone.

In this installment, part 2, we‚Äôll survey a number of recent technical developments in an effort to build a larger context for understanding new capabilities and consider what‚Äôs next.

The terms Computer Vision and Computational Photography are often used interchangeably. Computer vision, however, is a broader discipline covering a range of digital capture and processing techniques. It refers to the ability of computers to understand imagery, often in a similar way as people. Commonly this is achieved by characterizing the content, color, tonal values, shape or edge data. But it can also apply to other metadata such as embedded location or time data. When very large sets of images are analyzed, patterns and insights are gained and subsequently applied to organizing, verifying or even modifying imagery.

Computational photography is more specifically a discipline involving calculations, analysis, manipulation of imagery using algorithms rather than optical methods. We won‚Äôt fret over the distinction between the two disciplines here but rather consider the larger genre of computer vision.

Eyes Robot

This isn‚Äôt necessarily a new area. Initial examples of computer vision have been with us for awhile in the likes of:

Optical Character Recognition (OCR) enabling bar- and QR-code scanning and conversion of text-based printouts to machine-readable documents.

High-Dynamic-Range (HDR) imaging where multiple images are combined to depict a high-contrast setting, normally one that exceeds the range of a camera sensor or even the human eye. Recent updates have dramatically improved the quality, helping HDR shed the moody and surreal ‚ÄúHarry Potter‚Äù look of its early days.

Panoramic imagery where multiple images are aligned and stitched together with the seams between images automatically blended.

Contextual image replacement, coined Content-Aware Fill in Adobe‚Äôs software, where portions of an image are replaced using surrounding data. A common usage would be to remove power lines from a photo. Adobe released this in 2010 but more recently, the feature has seen dramatic improvements in quality and capability with the application of more sophisticated algorithms based on their AI platform.

We‚Äôve seen a raft of new developments in computer vision covering a range of use in photography and video. Recent examples include:

Mechanical Brains

The news has been rife with stories of fake imagery and videos, often involving some of the techniques mentioned above ‚Äî and it‚Äôs only beginning to mature. A basic understanding of the methodology is helpful to understand its capabilities and where it may lead. While it can be used to generate new, synthesized or fake imagery, it can also be used to recognize, categorize, track or creatively modify imagery as in the case of the popular Prisma app which uses a technique known as style transfer achieved through use of a Convolutional Neural Network (CNN). Additionally, these approaches, which are highly adaptive, are a big focus in the effort to create self-driving vehicles.

Generally, achieving good results makes use of neural networks patterned after biological systems where stimuli roll up to higher levels creating more meaningful impulses. At a very elemental level neural networks are optimization methods that train a computer model by finding associations between data. Strong associations are given more importance, weak associations have less value. It‚Äôs a bit of a brute force method but computers being fast and tireless can crunch enormous amounts of data to reach surprisingly good results.

One approach pits two neural networks against each other in an optimization scheme known as a Generative Adversarial Network (GAN). One network generates an image based on learnings from a dataset, the other assesses the image to determine if it‚Äôs realistic. Rejected images are refined until the discriminator can no longer determine if the image is fake.

Convolutional Neural Networks (CNNs) are commonly used to categorize images or otherwise find patterns. As data is analyzed, convolutional layers transform the data and pass the info to the next layer for further analysis. A number of filters are specified for each layer such as edges, shapes and corners, representing more complex information or objects with each layer. As the data moves further into the network, the more sophisticated layers are able to identify more complex objects like eyes, faces, or cars as data from prior layers is combined.

Perceptual Loss Functions are also used for their speed in training a CNN. This method recognizes that 2 images can look the same to humans but be mathematically different to a computer ‚Äî such as shifting the same image by a pixel or more. The more data analyzed, the better the results.

These explanations represent the very tip of the iceberg with these technologies. Implementations are still rough around the edges but they are improving rapidly. Yet even with this limited understanding, it‚Äôs not hard to see how neural networks can be used to generate impressive, animated models of real people, especially celebrities as we‚Äôve heard many times in the news. For example, high definition video with 24 frames per second can be pulled from YouTube to train a network on how a specific person speaks and moves. These learnings can then be used to generate new or altered imagery such as this example where Jon Snow apologizes for GoT season 8.

These methods are computationally very intensive. Faster processors and the availability of huge amounts of digital imagery that can be used for patterning have allowed the more sophisticated and open-sourced algorithms to proliferate at this time. Interestingly, despite the complexity of image data, ML/AI methodologies have progressed much further than they have with text, due largely to the objective nature of imagery. Words and text, on the other hand, can have varying interpretations based on context, personality, culture and other factors like irony which pose bigger challenges for machines to understand.

The examples we covered above are far from comprehensive. Software and hardware companies continue their aggressive progress while many universities have added the subject to their curriculum and formed computer vision departments. It‚Äôs clear we‚Äôll continue to see an increase in the volume and quality of manipulated imagery. Further characterization of large image datasets will naturally bring insights and learnings along with some abuses.

In the final installment of this series, we‚Äôll consider some of the social and ethical challenges with these technologies along with some thoughts on mitigation. We‚Äôll also look at what‚Äôs on the horizon.
Tips and tricks to promote brands and gain insights from data

Suppose, some day you think of starting a company. In this age and time, one of the most important things is the reach of your company. If the company is formed on certain products which are used by people in day-to-day life, then it is likely that you already have a lot of competitors in the market. Now, what matters is to beat the heat of the competition. So, the places where your company becomes accessible to people is not only the advertisement hoardings these days. It‚Äôs the social media where people keep scrolling through their news feed around three times a day. On average, an American spends 705 hours on social media every year. The chances of failing are negligible, and even if you do fail, you have nothing to lose.

So, let‚Äôs assume your company is officially participating in brand marketing on Social Media. You‚Äôve set up a Facebook or Instagram page as the brand that is supposed to be marketed is a cosmetic brand ( Not a big fan of cosmetics but the dataset used is that of a cosmetic brand). You respond to customer questions, follow fans, post important news, and thank your advocates for their support. Beyond that, are you taking enough actions to monitor and analyze the results of your work? If you‚Äôre engaging in social media, then you should certainly measure those activities. Is there any better way of knowing it? It‚Äôs easier done than said, sounds ironical, huh ?

Social media metrics are data and statistics that give you insights into your social media marketing performance. Some of these metrics are explained in the blog below.

To suffice the purpose of this blog, we‚Äôll segment the metrics into four different categories:

Awareness : These metrics illuminate your current and potential audience.

: These metrics illuminate your current and potential audience. Engagement : These metrics show how audiences are interacting with your content.

: These metrics show how audiences are interacting with your content. Conversion : These metrics demonstrate the effectiveness of your social engagement.

: These metrics demonstrate the effectiveness of your social engagement. Consumer: These metrics reflect how active customers think and feel about your brand.

I‚Äôll be covering all the categories in general without going into the specific details for each. So, following are some of the metrics that you need to know to get a better understanding of the whole thing:

Reach: The total number of distinct people or the users the post has reached.

Impressions: The number of times a post was seen by the users. A post reached to n number of people can be seen by them m times individually. So, the total impressions become m*n. The number of times a post is displayed in your news feed, irrespective of the post being clicked. People may see multiple impressions of the same post. For example, someone might see a Page update in News Feed once, and then a second time if their friend shares it.

Total number of page likes: Likes show your audience size on Facebook. Over time, that number should be growing. If you‚Äôre stuck with around the same number of likes for months, it means one of two things:

1) You‚Äôre losing the same number of Likes as you‚Äôre gaining.

2) New people aren‚Äôt following you

Engagement: It means the number of times the users have performed an action on the post. Engagement is one of the most important Facebook metrics you can track. Subjectively, engagement is a sign that people actually like the content you‚Äôre sharing. But another reason engagement is so valuable is it may give your posts more exposure to your audience. This includes liking, commenting, sharing and people who‚Äôve viewed your video or clicked on your links and photos. And it also includes people who‚Äôve clicked on a commenter‚Äôs name, liked a comment, clicked on your Page name and even gave negative feedback by reporting your post.

Consumptions: This metric is similar to Engagement but Consumption does not necessarily produce a story.

Consumptions = Link Clicks + Photo Views + Video Plays + Other Clicks (some of which do generate stories)

Total interactions: As we know, the main motive is to increase the number of people viewing the post and to increase the number of interactions (like, comment, share) with the post so that a story is created and it automatically appears to the viewer‚Äôs friends in their News Feed. The total interactions are calculated from the actions performed by Lifetime Post Engaged Users (Story is created) and Lifetime Post Consumers (Story not created). Do note these metrics, these will be used later.

Total interactions= Likes + Comments + Shares

Type of Posts: There are four types of post on Facebook: Video, Photo, Status, Links. A general human tendency is to look at images, read and share them. Statuses are generally longer and people are reluctant to read it. So, the natural biasing is towards looking and reacting to Videos, Pictures followed by statuses.

Last year (2018), Facebook admitted that it prioritizes video in its algorithm, with extra emphasis on live video. If you can create video content, you have a much better chance of getting to the top of news feeds. So, you‚Äôll see the later visualizations are a little biased towards Video Posts.

Paid Likes: Number of people who have Liked your Page as a result of a Facebook Ad campaign.

The Facebook page analytics provides data and statistics related to these metrics. So, what I have done is I have a dataset with the name Morto et al, which gave the metrics for advertisement of particular cosmetic anonymized brand on Facebook. The dataset has a total of 500 records which has 19 columns with such metric values defined. Now, coming back to next part of this blog is getting started with R programming.

R is a statistical programming language. R possesses an extensive catalog of statistical and graphical methods. It includes machine learning algorithms, linear regression, time series, statistical inferences. There are various packages in R which make it easy to get the tasks in the data science domain done easily. For detailed information theoretically on R, visit https://en.wikipedia.org/wiki/R_(programming_language) and https://www.tutorialspoint.com/r/index.htm to get help for syntax of R programming language.

Going to the project wherein I plotted various data visualizations which will help any layman to make decisions as to what actually helps in bringing the advertisement close to the end user.

Note: The following observations and data analysis was done by intuition and then some changes were done to get the best results out of it.

Chiefly, The total page likes depend on following Facebook Post metrics:

Total Reach

Total Impressions

Type of Post (Photo, Status, Video, Link)

Weekday

Hour of the day

With the help of multiple regression analysis, we get the coefficients for all the above metrics which will help us determine the importance of each metric, which helps in increasing our page likes.

Why Multiple Regression? Multiple regression is an extension of linear regression into relationship between more than two variables. In simple linear relation we have one predictor and one response variable, but in multiple regression we have more than one predictor variable and one response variable.

We create the regression model using the lm() function in R. The model determines the value of the coefficients using the input data. Next we can predict the value of the response variable for a given set of predictor variables using these coefficients.

data<-read.csv("Morto_Et_Al.csv") input<-read.csv(file = "facebook.csv", sep = ",")[ ,c('PTL','LPTReach','LPTImpressions','Type','Weekday','Hour')] model<-lm(PTL~LPTReach+LPTImpressions+Type+Weekday+Hour,data=input) a <- coef(model)[1]

XLPTReach <- coef(model)[2]

XLPTImpressions <- coef(model)[3]

XTypePhoto <- coef(model)[4]

XTypeStatus <- coef(model)[5]

XTypeVideo <- coef(model)[6]

XWeekday <- coef(model)[7]

Xhour <- coef(model)[8]

2. Prediction analysis:

Here, using the coefficients obtained by multiple regression analysis are used to generate an equation wherein putting the values for the variables the equation predicts the number of likes depending on the type of post.

x1 <- readline("What is the total reach?")

x1 <- as.numeric(x1)

x2 <- readline("What is the value for total impressions?")

x2 <- as.numeric(x2)

x6 <- readline("What is weekday?")

x6 <- as.numeric(x6)

x7 <- readline("Which is the hour of the day?")

x7 <- as.numeric(x7) x<-c("Photo","Status","Video")

type<-readline("What is the type of post?") if("Photo" %in% type) {

Y = a + XLPTReach*x1+XLPTImpressions*x2+XTypePhoto*2.5+XWeekday*x6+Xhour*x7Z = a + XLPTReach*x1+XLPTImpressions*x2+XTypePhoto*3+XWeekday*x6+Xhour*x7 }

else if ("Status" %in% x) {

Y = a + XLPTReach*x1+XLPTImpressions*x2+XTypeStatus*1.4+XWeekday*x6+Xhour*x7

Z = a + XLPTReach*x1+XLPTImpressions*x2+XTypeStatus*2+XWeekday*x6+Xhour*x7 }

else if ("Video" %in% x) {

Y = a + XLPTReach*x1+XLPTImpressions*x2+XTypeVideo*4+XWeekday*x6+Xhour*x7

Z = a + XLPTReach*x1+XLPTImpressions*x2+XTypeVideo*5+XWeekday*x6+Xhour*x7

}

3. Post weekday v/s total reach, Post weekday v/s Total Impressions:

Here, I have plotted a line graph to show the relation, how a post weekday affects the total reach and impressions. Results show a post posted on weekday 3 gives the best results for reach, and the same post has the maximum number of impressions the following day. Makes sense, doesn‚Äôt it?
How to Adjust DetectNet

Photo by Christian Wiediger on Unsplash

DetectNet is an object detection architecture created by NVIDIA. It can be ran from NVIDIA‚Äôs Deep Learning graphical user interface, DIGITS, which allows you to quickly setup and start training classification, object detection, segmentation, and other types of models.

There are two basic DetectNet prototxt files provided by NVIDIA:

one for single class (which is the original) which can be found here, and one for two classes which can be found here.

DetectNet‚Äôs original architecture is written in Caffe. I could not find much documentation on the architecture besides 2 blog posts present in NVIDIA‚Äôs website and few tutorials which (mostly) reiterate the blogs‚Äô content. I did find that a lot of information has been accumulated under one particular GitHub issue, issue #980 under the NVIDIA/DIGITS repository.

Here are the highlights I collected from the GitHub issue:

The images in your training set should not be of different sizes. If they are, you should pad them or resize them to be of equal dimensions. The resizing or padding can be done in the DIGITS dataset creation step.

DetectNet is sensitive to bounding boxes in the size range of 50x50 pixels to 400x400 pixels. It has difficulty identifying bounding boxes which are outside of this range.

If you want to detect objects smaller than the size DetectNet is sensitive to, you may either resize the images to be larger so most bounding boxes will fit DetectNet‚Äôs preferred range, or you may change the model‚Äôs stride to be smaller.

The image dimensions must be divisible by the stride. For example, 1248 and 384 (DetectNet‚Äôs default image size) are divisible by 16.

If you are training a model using an image resolution different from the original architecture (which expects images of width 1248 and height 384), you need to change the specified image dimensions within the architecture on the lines 57, 58, 79, 80, 118, 119, 2504, 2519, and 2545 (these lines refer to the single class DetectNet prototxt).

To change the model stride, you must change the default stride value (16) to your desired value in the lines 73, 112, 2504, 2519, and 2545 (these lines refer to the single class DetectNet prototxt).

If you specify a smaller stride, you will need to reduce the layers in the network in order to adjust the dimensionality. One way to reduce the dimensionality is by changing the kernel and stride parameters of the pool3/3x3_s2 layer to be 1. This layer is present from line 826 to 836 (these lines refer to the single class DetectNet prototxt).

For multiclass object detection in which you want to detect more than 2 classes, you may change the 2-class DetectNet prototxt [5]. Lines dependent on the number of classes are:
As Metis Data Science Bootcamp students, we were tasked to build a linear regression model for our first individual project. Simple enough right? Just draw a line through data.

Not so fast!

As a healthcare professional, I was excited to use this algorithm to solve a problem in the medical field. But what I quickly found out was, before any modeling, a successful machine learning project starts with picking the right kind of data. Linear regression works best with continuous numerical data which excludes several sources of publicly available healthcare data. However, it turns out that Medicare payment data is a perfect fit.

But why care about Medicare? Medicare is a government-funded health insurance program currently covering 44 million people, or 1 in 8 individuals in the U.S. (1). That carries with it some huge public health and financial implications for the government. As soon as you turn 65, you‚Äôre eligible for it with a few exceptions such as younger people with disabilities or those suffering from end stage renal disease. And this population is only getting bigger. In fact, the Census Bureau estimates that by 2030, the elderly population alone will nearly double to 78 million people or 1 in 5 Americans (2).

On top of this, it seems likely that Medicare is going to be increasingly more important in the coming years due to its prevalence in political discussions around the country. If the United States were to adopt a single-payer system, such as the proposed Medicare-for-All, it would be essential for us to lower costs wherever possible, and the effective usage of data could help to achieve this.

One way to do this would be to look at healthcare provider costs. You can follow along with my code on my GitHub repository; I‚Äôve organized it chronologically with this article for your convenience.

After searching the Center for Medicare and Medicaid Services website, I was able to acquire the most recent provider payment data which had over 1 million rows, each corresponding to a healthcare organization or individual provider, and 70 features.

Data Cleaning

But as expected, this data was quite a mess. So in order to glean the most reliable insight I could, I decided to narrow down what I was measuring to only individual providers in the U.S, excluding territories and military zones.

I then replaced all missing values in columns with count or percentage data with zeroes. It is likely that if a provider doesn‚Äôt have patients with a certain disease, the field is just left blank; this means it should have been relatively safe to impute nulls without losing much fidelity.

There was also some documentation provided for the meaning of each column name. I decided the target variable should be total_medicare_payment_amt, which is the total amount the government paid for all the provider‚Äôs services per patient after deductible and coinsurance amounts have been subtracted. Additionally, I removed all columns that were either unnecessary (like provider names) or that would lead to possible data leakage (like other price-based columns).

Finally I was left with a little over 990,000 clean rows with 38 features. Let‚Äôs begin!

Initial Model

Using the Statsmodels python library, I just threw all my data into the ordinary least squares (OLS) linear regression to see how it would perform initially with no modifications.

Univariate Linear Regression Example

As previously mentioned, linear regression tries to find a linear relationship between independent variables and a dependent variable. Above, you can see the simplest univariate form with only one independent variable or feature. It uses the equation y = mx + b to find the best fit with the data; m is the slope coefficient and b is the y intercept.

But obviously with 38 features, this linear regression problem is a lot more complex. In this case, there will be 38 ‚Äúmx‚Äù terms added together, with each m term corresponding to the size and direction of the effect that specific variable is having on the dependent variable. In geometric terms, we will be fitting a 38th dimensional hyperplane to 39th dimensional space (instead of a line). If you find a way to visualize this, let me know!

Ok, so now we have some intuition for the model, but how do we determine how well the model is doing?

The metric that is commonly used here is called the coefficient of determination or R-squared. Essentially, it is the percentage of variance of the target variable that is predicted by the features. We want an R-squared close to 1 which indicates that the model is very predictive.

The Inner Workings of R-squared

But let‚Äôs dive a little deeper into the actual formula of R-squared because it helps us to understand how we are evaluating the model. The most naive method we could use to predict Medicare costs would be to just guess the average cost. That is the green »≥ (called y-bar) in the the diagram above. This will be our baseline.

But we can do a lot better than that by using linear regression or the red ≈∑ (called y-hat). Now we just find how off these two predictions are from the actual value and divide them by each other (SSE/SST). This will tell us the percentage of variance the model cannot explain. But what we really want to know is what percentage of variance this model does explain. Subtracting that value from 1 will get us there.

1 - (Error Sum of Squares/Total Sum of Squares) or 1 - (SSE/SST)

And after running the initial model, the R-squared was 0.619. That means that our model only accounts for about 62% of the variation in the data. That‚Äôs not too good.

Checking Our Assumptions

But wait! Linear regression has many assumptions and it is important to check if our data is actually working for this model.

Assumption #1: Is there a linear relationship between the target variable and the features?

Number of Services vs Total Medicare Cost Before Feature Engineering

For illustration purposes, if we use the feature of total medicare cost, it‚Äôs not entirely clear. To rectify this situation, we could do some feature engineering. One option is to do a logarithmic transformation of both the feature and the target.

Number of Services vs Total Medicare Cost After Feature Engineering

Wow! That‚Äôs a dramatic improvement. Anyone can draw a line through that! As you can see, it is often the case that we need to transform the data in specific ways to make it conform to the assumptions of the model we are using.

Note: Always remember to undo this transformation afterwards in order to return your value back to the original context. Because after all, what does the logarithm of Medicare costs even mean?

Assumption #2: Are the target and the features normally distributed?

Total Medicare Cost Before and After Feature Engineering

In the above figure, the left plot shows the target variable before using a logarithmic transformation; as you can see, it is terribly right skewed. The right plot, on the other hand, shows how applying this transformation results in a remarkably normal distribution.

Assumption #3: Is there little to no multicollinearity among the features?

Correlation Coefficient Heat Map of All Variables

Multicollinearity is when the features are highly correlated with each other. Above, we see a heat map where the darker colors indicate strong positive correlations. Ideally, we would see only light colors everywhere else except the diagonal line across the middle, as obviously a variable will be perfectly correlated with itself.

But in reality, we see darker colors popping up all over the place, which indicates we are violating this assumption. This can lead to imprecise regression coefficients or worse, changes in sign for the same features in different samples, which makes it difficult to reliably extract meaning out of those coefficients.

The way to go about solving this is to remove features until there is no longer any collinearity. As will be discussed later, regularization techniques do this for you by zeroing out coefficients of some of the features that are collinear with each other.

Assumption #4: Are the residuals correlated with themselves?

An autocorrelation happens when the residuals for a specific feature are not independent from each other. This is considered bad because it indicates the model is not extracting all the information possible from the data, and thus, we see it in the residuals.

This can be measured through the Durbin-Watson test. Values near 2 indicate no autocorrelation, while values near 0 or 4 indicate strong autocorrelations. Our initial model has a value of 1.998, indicating that the model is extracting as much information as possible and the assumption has been met.

Assumption #5: Is the data homoskedastic?

What we want to avoid here is heteroskedasticity, a big word with a simple explanation. This is when the variance of the residuals change across the range of values in a feature.

An Example of Heteroskedasticity (Source)

As you can see in this hypothetical example, it is very clear that the variance gets wider as age increase. This is not good as it means that our model will get worse at making predictions the older someone gets. What we really want is a consistent predictability and variance across the entire range of values, known as homoskedasticity. In other words, the two dotted red lines would be parallel to each other.

Predicted vs Residuals Plot Before Feature Engineering

Here we see the predicted values versus the residuals for our model on the Medicare data. This doesn‚Äôt look good at all. There is a harsh cut off in the negative residuals (due to government cost always being greater than or equal to 0) and the variance is completely inconsistent across the range of values.

Predicted vs Residuals Plot After Feature Engineering

But after applying the logarithmic transformation that we did before, the plot looks relatively homoskedastic now and we have met this assumption. Boom!

Secondary Modeling

So after checking the assumptions of all the features, I decided to apply a log transformation to 3 features and the target variable.

Now, I put this newly transformed data back into the model and after training, it produced an R-squared of 0.92. Fantastic! This is a solid result as the new model can explain 30% more of the variance in the data versus the baseline model. This demonstrates how important it is to transform your data to meet the assumptions of the model you have chosen.

But this was just an OLS model. We can apply the regularization techniques briefly mentioned before which should further strengthen our model. These add an extra term to the cost function, penalizing the model for complexity. This is a good idea because simpler models are typically better than complex ones as they tend to be less susceptible to overfitting.

In other words, complex models tend to fit training data super well but perform poorly on unseen data. I switched over to the scikit-learn library to do this regularization, along with adding in more rigor to the process with a test-train split and cross validation.

I experimented with both ridge and LASSO regression and did hyper parameter tuning of the alpha terms which determine how strong the regularization will be. Surprisingly, both models with optimized alphas performed basically exactly the same as the OLS model with an R-squared of 0.92, with ridge being insignificantly better than LASSO. This indicates that regularization did not significantly help the model.

The LASSO coefficients support this finding as well. LASSO typically zeroes out any redundant features, leaving only a few remaining. In contrast, the best LASSO model only zeroed out 1 out of the 38 features. This is a surprising result, indicating that most features contribute to the predictability of the model and thus stronger regularization would only hurt the model‚Äôs performance.

Feature Importance

Speaking of coefficients, we can determine the importance of each feature by looking at the sign and magnitude of the coefficients. This allows us to provide valuable business insights to our stakeholders, in this case the Center for Medicare and Medicaid Services.

Top 10 Features That Increase Medicare Costs (Positive Coefficients)

From the top 10 features, I found it interesting that the fourth most important feature was the number of white patients a provider has. This is alarming as the model actually seems to care about race in some meaningful way.

This could possibly be exposing an underlying fault of the system, indicating that the white population is over-represented and thus, makes up a significantly larger percentage of the Medicare cost in comparison to other races. Other research needs to be done to determine the root cause but it is very possible that lack of access for underserved populations may contribute to this.

This is a powerful result and a example of the value that data science has for society. I was amazed to find that not only could I use this algorithm to improve a stakeholder‚Äôs bottom line but to also reveal social disparities. This is one of the main reasons I love data science; it can have immensely powerful effects on our society.

Top 10 Most Expensive Specialties

Looking at the top most expensive specialties, surgery clearly stands out. And it makes sense; surgery is incredibly expensive. This means that the government would best spend their efforts reducing surgery costs in order to most significantly impact their bottom line.

Top 10 Most Expensive Medical Conditions

In terms of medical conditions, largely preventable chronic diseases take the cake. This finding is a double-edged sword and falls in line with what we already know. Sadly, it means that most of these Medicare patients are suffering from diseases that they didn‚Äôt ever have to suffer from if they had just had different lifestyle choices. From both a financial and an ethical perspective, this is absolutely terrible.

But on a more positive note, this means that the government can save incredible amounts of money while also reducing immense amounts of suffering by beginning to focus on preventive lifestyle medicine instead of reactive treatments, such as surgery.

Obviously, we didn‚Äôt need data science to tell us to eat better and move more. But this further supports what we all know is necessary for the well being of everyone in society.

And in the process, we built a model to accurately predict how expensive a healthcare provider is for the government. Saving money and saving lives, what more could we ask for?
Data Democratization

In today‚Äôs world, every business is bombarded with data from each and every angle possible. There always remains a constant pressure to use various insights we garner from the data to improvise on our business performance. Hence, the incredible amount of the usage of the processed data has surged the desire and demand for data-democratization. So, if you are wondering that the above mentioned term is totally new to you, let‚Äôs dive deep into it.

Diagnosis of the term ‚ÄòData Democratization‚Äô

Before grabbing some knowledge on data democratization, we need to understand what does democratization mean. We can define it as the introduction of a democratic system or democratic principles; on breaking down the term further we can elucidate it as the action of making something accessible to everyone.

And as aptly named, data democratization is the process of making data accessible to everyone, which implies that there are no gatekeepers that create a bottleneck at the gateway to the data. It provides an ease to the people for a better understanding of the data to expedite the decision-making and uncover brilliant opportunities for an organization. The ultimate mission is to have anybody use the data anytime, as and when required, to make a major impact in decision-making with no barricade or roadblocks.

Of late, data was ‚Äúretained‚Äù by the IT departments. Various business units such as marketing, sales, executives are always in need to data in order to make important business decisions. Needless to say that they always had to go through their respective IT departments to access the required data. This is the way it has been for a long time, few decades may be, and there are people out there who still believe that this is the way it should be.

Pros: Need for Data Democratization

Should we be Liberal regarding data-sharing?

Advocates of data democratization are in the belief that it should be obligatory to disperse information on all the working teams so as to gain a competitive edge. The more people with diverse expertise having the access to data, the more organization will benefit by identifying and taking actions on important and critical business insights. It is believed by many high and esteemed professionals that data democratization can be a game-changer to many organizations. When the data gets spread out all over across the organization, it empowers each and every level of the individual to use the data and participate for the contribution in decision-making.

Cons: Apprehension about Data Democratization

Or should we be Conservative regarding data-sharing?

Yet many organizational individuals are still perplexed as to sharing the data to the non-technical employees could result in misinterpretation of the data which might result in making some bad business decisions. Additionally, the more users having access to data, the bigger is the data-security risk and thus resulting in more challenges to maintain the data integrity.

Talking about the data, some data still exists in silos/warehouse/archive-mode; although several attempts have been made in recent years and the attempts are still in progress, but the reality cannot be denied that people from different departments are still finding it difficult to access the data and view it.

There is another concern which cannot be denied ‚Äî a duplication of efforts across different teams which could lead to more cost and wastage of time and effort when compared to a centralized analysis group.

Be Conservative or Liberal ‚Äî What Should be the approach?

For the sake of argument, it might be concluded that it is easy to preach one approach over the other, but the reality states that extremely conservative or liberal views rarely fit into the context of data democratization. The business leaders should carefully weigh the pros and cons of data democratization in determining which kind of approach benefits their organization the most.

The business leaders should decide upon how much liberal or conservative they should be regarding their organization ‚Äî ultimately, it all comes down to their business or organizational success. But, they can always spread out the metadata (and retaining the raw data) within the organization individuals to gain meaningful and impactful insights and can retain the sensitive data within themselves.

For example, if we speak about a particular health-care center which deals with diabetic patients, it is not permissible to share the personal details of the patients (and thereby acting as intruders to their privacy), but analysis can always be done the generic data and thus finding out the age group or the sex-ratio are being trapped under diabetics.

I agree that sometimes the perception shared by the non-technical people or the people who are not data-literate might be misleading, but attempts can surely be made to add an extra layer of scrutiny on top of that, to avoid any kind of mistakes and ensure the organizational growth ‚Äî that‚Äôs what the business leaders are there for.
When it comes to data products, a lot of the time there is a misconception that these cannot be put through automated testing. Although some parts of the pipeline can not go through traditional testing methodologies due to their experimental and stochastic nature, most of the pipeline can. In addition to this, the more unpredictable algorithms can be put through specialised validation processes.

Let‚Äôs take a look at traditional testing methodologies and how we can apply these to our data/ML pipelines.

Testing Pyramid

Your standard simplified testing pyramid looks like this:

Image by author

This pyramid is a representation of the types of tests that you would write for an application. We start with a lot of Unit Tests, which test a single piece of functionality in isolation of others. Then we write Integration Tests which check whether bringing our isolated components together works as expected. Lastly we write UI or acceptance tests, which check that the application works as expected from the user‚Äôs perspective.

When it comes to data products, the pyramid is not so different. We have more or less the same levels.

Image by author

Note that the UI tests would still take place for the product, but this blog post focuses on tests most relevant to the data pipeline.

Let‚Äôs take a closer look at what each of these means in the context of Machine Learning, and with the help of some sci-fi authors.

Unit tests

‚ÄúIt‚Äôs a system for testing your thoughts against the universe, and seeing whether they match‚Äù Isaac Asimov.

Most of the code in a data pipeline consists of a data cleaning process. Each of the functions used to do data cleaning has a clear goal. Let‚Äôs say, for example, that one of the features that we have chosen for out model is the change of a value between the previous and current day. Our code might look somewhat like this:

def add_difference(asimov_dataset):

asimov_dataset['total_naughty_robots_previous_day'] =

asimov_dataset['total_naughty_robots'].shift(1)



asimov_dataset['change_in_naughty_robots'] =

abs(asimov_dataset['total_naughty_robots_previous_day'] -

asimov_dataset['total_naughty_robots']) return asimov_dataset[['total_naughty_robots',

'change_in_naughty_robots', 'robot_takeover_type']]

Here we know that for a given input we expect a certain output, therefore, we can test this with the following code:

import pandas as pd

from pandas.testing import assert_frame_equal

import numpy as np def test_change():

asimov_dataset_input = pd.DataFrame({

'total_naughty_robots': [1, 4, 5, 3],

'robot_takeover_type': ['A', 'B', np.nan, 'A']

}) expected = pd.DataFrame({

'total_naughty_robots': [1, 4, 5, 3],

'change_in_naughty_robots': [np.nan, 3, 1, 2],

'robot_takeover_type': ['A', 'B', np.nan, 'A']

}) result = add_difference(asimov_dataset_input)



assert_frame_equal(expected, result)

For each piece of independent functionality, you would write a unit test, making sure that each part of the data transformation process has the expected effect on the data. For each piece of functionality you should also consider different scenarios (is there an if statement? then all conditionals should be tested). These would then be ran as part of your continuous integration (CI) pipeline on every commit.

In addition to checking that the code does what is intended, unit tests also give us a hand when debugging a problem. By adding a test that reproduces a newly discovered bug, we can ensure that the bug is fixed when we think that is fixed, and we can ensure that the bug does not happen again.

Lastly, these tests not only check that the code does what is intended, but also help us document the expectations that we had when creating the functionality.

Integration tests

Because ‚ÄúThe unclouded eye was better, no matter what it saw.‚Äù Frank Herbert.

These tests aim to determine whether modules that have been developed separately work as expected when brought together. In terms of a data pipeline, these can check that:

The data cleaning process results in a dataset appropriate for the model

The model training can handle the data provided to it and outputs results (ensurign that code can be refactored in the future)

So if we take the unit tested function above and we add the following two functions:

def remove_nan_size(asimov_dataset):

return asimov_dataset.dropna(subset=['robot_takeover_type']) def clean_data(asimov_dataset):

asimov_dataset_with_difference = add_difference(asimov_dataset)

asimov_dataset_without_na = remove_nan_size(

asimov_dataset_with_difference) return asimov_dataset_without_na

Then we can test that combining the functions inside clean_data will yield the expected result with the following code:

def test_cleanup():

asimov_dataset_input = pd.DataFrame({

'total_naughty_robots': [1, 4, 5, 3],

'robot_takeover_type': ['A', 'B', np.nan, 'A']

})

expected = pd.DataFrame({

'total_naughty_robots': [1, 4, 3],

'change_in_naughty_robots': [np.nan, 3, 2],

'robot_takeover_type': ['A', 'B', 'A']

}).reset_index(drop=True) result = clean_data(asimov_dataset_input).reset_index(drop=True)



assert_frame_equal(expected, result)

Now let‚Äôs say that the next thing we do is feed the above data to a logistic regression model.

from sklearn.linear_model import LogisticRegression def get_reression_training_score(asimov_dataset, seed=9787):

clean_set = clean_data(asimov_dataset).dropna()

input_features = clean_set[['total_naughty_robots',

'change_in_naughty_robots']]

labels = clean_set['robot_takeover_type']



model = LogisticRegression(random_state=seed).fit(

input_features, labels) return model.score(input_features, labels) * 100

Although we don‚Äôt know the expectation, we can ensure that we always result in the same value. It is useful for us to test this integration to ensure that:

The data is consumable by the model (a label exists for every input, the types of the data are accepted by the type of model chosen, etc)

We are able to refactor our code in the future, without breaking the end to end functionality.

We can ensure that the results are always the same by providing the same seed for the random generator. All major libraries allow you to set the seed (Tensorflow is a bit special, as it requires you to set the seed via numpy, so keep this in mind). The test could look as follows:

from numpy.testing import assert_equal def test_regression_score():

asimov_dataset_input = pd.DataFrame({

'total_naughty_robots': [1, 4, 5, 3, 6, 5],

'robot_takeover_type': ['A', 'B', np.nan, 'A', 'D', 'D']

})

result = get_reression_training_score(asimov_dataset_input,

seed=1234)

expected = 50.0 assert_equal(result, expected)

There won‚Äôt be as many of these kinds of tests as unit tests, but they would still be part of your CI pipeline. You would use these to check the end to end functionality for a component and would therefore test more major scenarios.

ML Validation

Why? ‚ÄúTo exhibit the perfect uselessness of knowing the answer to the wrong question.‚Äù Ursula K. Le Guin.

Now that we have tested our code, we need to also test that the ML component is solving the problem that we are trying to solve. When we talk about product development, the raw results of an ML model (however accurate based on statistical methods) are almost never the desired end outputs. These results are usually combined with other business rules before consumed by a user or another application. For this reason, we need to validate that the model solves the user problem, and not only that the accuracy/f1-score/other statistical measure is high enough.

How does this help us?

It ensures that the model actually helps the product solve the problem at hand

It ensures that the values produced by the model make sense in terms of the industry

It provides an extra layer of documentation of the decisions made, helping engineers joining the team later in the process.

It provides visibility of the ML components of the product in a common language understood by clients, product managers and engineers in the same way.

This kind of validation should be ran periodically (either through the CI pipeline or a cron job), and its results should be made visible to the organisation. This ensures that progress in the data science components is visible to the organisation, and ensures that issues caused by changes or stale data are caught early.

Conclusion

After all ‚ÄúMagic‚Äôs just science that we don‚Äôt understand yet‚Äù Arthur C. Clarke.

ML components can be tested in various ways, bringing us the following advantages:

Resulting in a data driven approach to ensure that the code does what is expected

Ensuring that we can refactor and cleanup code without breaking the functionality of the product

Documenting functionality, decisions and previous bugs

Providing visibility of the progress and state of the ML components of a product

So don‚Äôt be afraid, if you have the skillset to write the code, you have the skillset to write the test and gain all of the above advantages üôÇ

So long and thanks for all the testing!
How to Ease the Pain of Working with Imbalanced Data

A summary of methods and resources for creating a model using an imbalanced dataset Andrea Brown ¬∑ Follow Published in Towards Data Science ¬∑ 5 min read ¬∑ Jul 18, 2019 -- Listen Share

You‚Äôve finally collected and cleaned your data and have even completed some exploratory data analysis (EDA). All that hard work has finally paid off ‚Äî time to start playing with models! However, you quickly realize that 99% of your binary labels are of the majority class while only 1% are of the minority class. Using accuracy as the primary evaluation metric, your models are classifying all predictions as the majority class and are still 99% accurate. You do a little Googling to find the right search terms for Stack Overflow, and then you sigh deeply as you read about the pains of working with imbalanced data‚Ä¶

What is Imbalanced Data?

Fraud detection is commonly used as an example of imbalanced datasets. Maybe only 1 out of every 1,000 transactions is fraudulent, which means only 0.1% of labels are of the minority class. Disease data is also likely to be imbalanced. In environmental health, a typical threshold for an elevated risk of developing cancer is 1 in 10^6, or one in a million people. For my project, I was working with lawsuit data, and the companies that had been sued were minimal compared to the dataset as a whole. In summary, imbalanced data is exactly what it sounds like: the labels for the minority class are far and few between, making the accuracy metric somewhat useless for evaluating model results.

What Should I do Differently?

Use Different Metrics

Another key difference is to use alternative metrics to the traditional accuracy metric. Since I was primarily interested in identifying True Positives and avoiding False Negatives rather than maximizing accuracy, I used the area under the curve receiving operator characteristic (AUC ROC) metric. This TDS post has a great explanation of the AUC ROC metric. To summarize, AUC ROC = 0.5 essentially means the model is equivalent to a random guess and AUC ROC = 1 implies the model can perfectly differentiate between the minority and majority classes.

I also relied on the confusion matrix to maximize the number of True Positives and False Positives and minimize the number of False Negatives. Since I was working with lawsuit data, I decided to consider a False Positive result as an indication of a high likelihood of being sued in the future. Therefore, False Positive results were also very important to me. I liked the code in this Kaggle kernel to visualize the confusion matrix.

Summary of True Positives and False Negatives using the Confusion Matrix

I also calculated the more traditional precision and recall metrics (equations below for quick reference) to compare different learning algorithms, and I added the less traditional miss rate (false positive rate) to the list of metrics to compare as well.

Precision = TP / (TP + FP) Recall (True Positive Rate) = TP / (TP + FN) Miss Rate (False Positive Rate) = FN / (FN +TP)

Methodology

There are two primary classes of methodologies to working with imbalanced data: (1) at the algorithm level and (2) at the data level. I‚Äôll summarize the methods for each in the following sections.

Algorithm-Related Methods

Cost-Sensitive Learning

Considering solutions to an imbalanced dataset at the algorithm level requires an understanding of algorithms that enable cost-sensitive learning. In my case (and likely for disease and fraud detection as well), identifying True Positives was the primary goal of the model, even at the expense of selecting some False Positives. This is where cost-sensitive learning comes in handy. Cost-sensitive learning takes into account the different types of misclassification (False Positives & False Negatives).

Logistic Regression

A classic logistic regression algorithm is a robust model for imbalanced datasets. The logistic regression algorithm includes a loss function that calculates the cost for misclassification. Using SciKit-Learn, the loss function can be manipulated with a penalty weight that includes either ‚ÄòL1‚Äô or ‚ÄòL2‚Äô Regularization depending on the solver used.

Support Vector Machines

In SciKit-Learn, the Support Vector Classifier includes a ‚Äòclass_weight‚Äô parameter that can be used to give more weight to the minority class. ‚ÄúThe ‚Äòbalanced‚Äô mode ‚Äúuses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data.‚Äù This paper further details developing a cost-sensitive SVM.

Naive Bayes

SciKit-Learn includes a Complement Naive Bayes algorithm that is a cost-sensitive classifier that ‚Äúuses statistics from the complement of each class to compute the model‚Äôs weights.‚Äù Optimizing the model weights is an effective way of handling imbalanced datasets.

Ensemble Method ‚Äî Boosting

Boosting algorithms are ideal for imbalanced datasets ‚Äúbecause higher weight is given to the minority class at each successive iteration.‚Äù For example, ‚ÄúAdaBoost iteratively builds an ensemble of weak learners by adjusting the weights of misclassified data during each iteration.‚Äù

Data-Related Methods

Re-sampling

Solving a class imbalance problem at the data level typically involves manipulating the existing data to force the dataset used to train the algorithm to be balanced. This method is called re-sampling, and typical re-sampling techniques include:

Over-sampling the minority class,

Under-sampling the majority class,

Combining over- and under-sampling, or

Creating ensemble balanced sets.

Over-sampling the Minority Class

Over-sampling involves balancing the dataset by creating synthetic data to increase the number of outcomes in the minority class. A common method for over-sampling is called the Synthetic Minority Oversampling Technique (SMOTE), which uses k-nearest neighbors to create synthetic data.

Taking this a step further, SMOTEBoost combines over-sampling with boosting. SMOTEBoost is ‚Äúan over-sampling method based on the SMOTE algorithm that injects the SMOTE method at each boosting iteration.‚Äù

Under-sampling the Majority Class

Under-sampling involves decreasing the data included in the majority class to balance the training dataset. Note that this decreases the size of the dataset. A common technique for under-sampling is random undersampling (RUS), which randomly and selects a subset of the majority class. This Medium post goes into more detail on the various under-sampling methods.

RUSBoost combines under-sampling with boosting. Similar to SMOTEBoost, ‚ÄúRUSBoost achieves the same goal by performing random undersampling (RUS) at each boosting iteration instead of SMOTE.‚Äù

Ensemble Method ‚Äî Bagging

Bagging is an example of an ensemble technique at the data level. Bagging involves ‚Äúbuilding multiple models (typically of the same type) from different subsamples of the training dataset.‚Äù Bagging can reduce variance and prevent overfitting in algorithms such as the Random Forest Decision Trees algorithm.

Conclusion

For my dataset, the best approach was to use combine an ensemble method of boosting with over-sampling of the minority class (SMOTEBoost) and use the logistic regression algorithm to achieve an AUC ROC value of 0.81. With additional time spent on feature engineering, I could increase the AUC ROC value even more.
Saving and loading Pytorch models in Kaggle

You have heard about Kaggle being awesome. You also are striving to improve your data science skills and are getting started with deep learning or just getting started on Kaggle.

Just as you think you are getting the grasp of training your deep neural network on Kaggle, you get stuck.

So what‚Äôs the problem?

So you‚Äôve learnt you can save Pytorch models (strictly speaking, the state dictionary) and load them later at your convenience. You‚Äôve trained your model on Kaggle and saved it. When you need to access the saved model, you just can‚Äôt find it and this might force you to start all over again. üòìüòìüòì

We all know that training a model can be quite a painful process so having to do that all over again is not an option. Worry not, I(and Kaggle) got you!

In this article, I will show you how to load your trained Pytorch model in 5 simple steps.

Assumptions

I will assume that:

You already know how to train a model. You understand the concept of a Pytorch‚Äôs model state dictionary. Look at this resource for more information. You have at least a basic understanding of how to save Pytorch models and load them . If not, this post is a good place to start from.

The project I was working on and the subsequent need to author this article was born out of my participation in the Udacity Pytorch Challenge.
Selenium and SQL Combined- Top Premier League Players

Source: Jannes Glas via unsplash

The Premier League 2019/20 season is scheduled to begin on Saturday 10th August 2019. Whilst the anticipation grows for the new season, I thought I would investigate the best performers of the Season just gone.

This tutorial piece will demonstrate how we can simply scrape data on the Top Performers of the 2018/19 season, and tabulate that data into a Database Management System (DMS), MySQL. The data gathered can then be queried using SQL.

To begin this tutorial, the first objective is to find a suitable source with key statistics on the Top Premier league players. A quick search leads me to the ‚ÄòPremier League Top Scorers‚Äô page found on the BBC Football tab and shown below.

Data Mining

Key statistical metrics I would like to gather data on for each player are: their respective name and team they compete for, goals scored and assists provided, their shot accuracy and finally their minutes per goal ratio.

To begin, I need to find a wrapper that contains all the statistics for each player I am keen on scraping. I right-click in my chrome browser on the page of interest, and hover over the web element that I am searching for. Clearly the wrapper, ‚Äòdiv.top-player-stats__item.top-player-stats__item ‚Äî no-image‚Äô contains all the information of interest (as highlighted in the image below).

Before writing any Python code, a working habit I encourage is to switch across to the console tab in the developers tool and search for the elements of interest. This approach is considered best practice because we can visually inspect if we have found our elements of interest.

To use this feature type 2 dollar signs, $$, then input the tag and identifier name in quotations within parentheses.
This month, 50 years ago, the wildest science-fiction fantasies became reality when Neil Armstrong stepped off the Lunar Module ladder and left his iconic footprint on the moon‚Äôs surface.

His infamous words, ‚Äúone small step for man, one giant leap for mankind,‚Äù reverberated across the globe, to an audience of over 530 million viewers .

In a few weeks, the world will relive this historic event after half a century of continued space exploration and development.

In honor of this groundbreaking achievement, we take a look back at the history of lunar landings, the future of space exploration and the role of private companies in the burgeoning space economy.

Which Countries Have Landed on the Moon?

Although Apollo 11‚Äôs mission to the moon is the most well-known, there were other successful unmanned missions undertaken before 1969.

The Soviet Union‚Äôs Luna 2 was the first spacecraft to reach the lunar surface in 1959, while Luna 9 became the first spacecraft to soft-land on the moon in 1966.

Four months later, NASA‚Äôs Surveyor 1 successfully soft-landed on the moon, followed by four more successful unmanned missions.

After 14 years of a tension-filled space race between the Soviet Union and the United States, Apollo 11 successfully went to the moon and back-and set a precedent for generations to come.

Following this, NASA sent six more missions to the moon, five of them successful. In 2013, China also successfully soft-landed on the moon in December 2013, 37 years after the Luna 24.

In total, the United States has sent the most missions to the moon, 38 of them successful, closely followed by the former Soviet Union with 56 missions.

Future Missions to the Moon

The current US administration has ambitious plans for getting humans back on the moon by 2024 , four years earlier than what was initially on the timeline. The program, called Artemis, includes eight launches and a mini-station in lunar orbit by 2024.

Meanwhile, Russia plans to send manned flights to the moon starting in 2025, with the end goal of setting up a lunar colony by 2040 .

India hopes to become the fourth country to successfully soft-land on the moon this year, with a spacecraft scheduled to launch this month and land in September.

The space race won‚Äôt just be dominated by the nations of the world. Private companies such as SpaceX also have their sights set on the moon, even if travel to Mars is one of their main objectives. While Elon Musk‚Äôs SpaceX plans to send a group of artists on a trip around the moon in 2023, Jeff Bezos‚Äô Blue Origin hopes to take astronauts to the lunar surface by 2024.

Is Privatized Space Travel the Future?

With the rising costs of ambitious space exploration plans, such as travel to Mars, NASA is increasingly turning to private companies‚Äô deep pockets to fund such endeavors.

Billionaires such as Elon Musk, Jeff Bezos and Paul Allen are among those who are investing heavily in the commercial space sector.

According to SpaceFund‚Äôs launch database , there are currently over 100 space transportation companies from all over the world. As you can see below, the majority are based in the US, followed by the UK and China.

Many believe that private-public partnerships between NASA and companies such as SpaceX, Blue Origin and Virgin Galactic are the future of space travel.

In fact, commercial launch traffic into low-earth orbit has risen dramatically in the last decade:

Your Turn

What do you think of the future of space travel? Will increased space exploration be advantageous in the long-term? Or will it simply distract from more pressing problems here on Earth? Let us know your thoughts below‚Ä¶
D-ID Is Altering Facial Recognition‚Äôs Path Towards Privacy

While the rest of the world was caught up in the rage of selfies, face-timing and photo uploading, three young men saw a future of increasing surveillance and are now seeking to change this. Hessie Jones ¬∑ Follow Published in Towards Data Science ¬∑ 8 min read ¬∑ Jul 18, 2019 -- Share

DEPOSIT PHOTOS

Over the last decade we‚Äôve witnessed the deluge of online photo-sharing. From Pinterest to Instagram to Snapchat the rise of photo-sharing has enabled the soaring activity of selfies, citizen journalism, and events in-the-moment. Brandwatch‚Äôs recent stats revealed among 18‚Äì24 year-old‚Äôs, over 75% use Instagram and Snapchat, and 94% use Youtube:

On Instagram More than 40 billion photos have been shared so far

Over 95 million photos are uploaded each day on Instagram

In Q3 2017, 3.5bn snaps were sent.

The future of image and video is clear based on these previous estimations:

An estimated 84 percent of communications in 2018 were visual.

An estimated 79 percent of internet traffic content in 2018 was video.

Posts that include images produce 650 percent higher engagement

D-ID Co-founders: Eliran Kuta, Gil Perry, and Sella Blondheim

Three young men from Israel were watching the advent of image sharing unfold. Gil Perry, Sella Blondheim and Eliran Kuta were serving in the Israel Defense forces at the time. While the rest of the world was caught up in the rage of selfies, face-timing and photo uploading, Gil, Sella and Eliran saw things very differently,
In July 1969, the Apollo 11 spaceflight launched and landed humans on the Moon. Fifty years later, some people believe that the Moon landings did not really happen.

This article looks at two recent survey estimates of the belief that the Moon landings were faked, and the differences between those two surveys.

True or false?

Back in February 2019, YouGov asked 2,039 British adults in its internet panel, for its podcast with Yahoo:

To what extent, if at all, do you think the following statement is true or false? The moon landing was staged

People could answer along a scale from definitely true to definitely false, or that they don‚Äôt know. The survey estimated that 16% believe it is probably (12%) or definitely true (4%) that the moon landing was staged.

A few weeks ago, Number Cruncher Politics ‚Äî which is not a British Polling Council member‚Äî asked 1,000 British adults about several conspiracies. Respondents could tick all that they believed were true.

Previously, Number Cruncher Politics has used a mixture of internet panels and internet ‚Äòriver sampling‚Äô. River sampling involves inviting people to answer questions via promotional ads on selected websites.

In the survey, only 5% ticked ‚ÄúThe moon landings between 1969 to 1972 were faked and humans have not really been to the moon‚Äù.

It has been fifty years since the first Moon landings. (Photo: ABC News Australia)

So, which is it: 5% or 16%?

Differences

There are some differences in wording, comparing ‚Äòfaked‚Äô and ‚Äòstaged‚Äô, and specifying the time of multiple Moon landings.

However, the key difference is the response format.

In YouGov‚Äôs survey, people had to select an option for each statement: definitely true, probably true, probably false, definitely false, or don‚Äôt know. This is called the forced-choice format.

The Number Cruncher Politics survey was, as the article suggested, of the select-all-that-apply format. If you have taken surveys on the internet‚Ä¶
Can Congress Help Keep AI Fair for Consumers?

How do firms ensure that AI systems are not having a disparate impact on vulnerable communities, and what safeguards should regulators and Congress put in place to protect consumers? To what extent should companies be required to audit these algorithms so that they don‚Äôt unfairly discriminate? Who should determine the standards for that? We need to ensure that AI does not create biases in lending toward discrimination.

Two new House Task Forces to regulate fintech and AI

These aren‚Äôt questions from an academic discourse or the editorial pages. These were posed to the witnesses of a June 26 hearing before the US House Committee on Financial Services [1] ‚Äî by both Democrats and Republicans, representatives of Illinois, North Carolina, and Arkansas.

It is a bipartisan sentiment that, left unchecked, AI can pose a risk to fairness in financial services. While the exact extent of this danger might be debated, governments in the US and abroad acknowledge the necessity and assert the right to regulate financial institutions for this purpose.

The June 26 hearing was the first wake-up call for financial services: they need to be prepared to respond and comply with future legislation requiring transparency and fairness.

In this post, we review the notable events of this hearing, and we explore how the US House is beginning to examine the risks and benefits of AI in financial services.

Two new House Task Forces to regulate fintech and AI

On May 9 of this year, the chairwoman of the US House Committee on Financial Services, Congresswoman Maxine Waters (D-CA), announced the creation of two task forces [2]: one on fintech, and one on AI.

Generally, task forces convene to investigate a specific issue that might require a change in policy. These investigations may involve hearings that call forth experts to inform the task force.

These two task forces overlap in jurisdiction, but the committee‚Äôs objectives implied some distinctions:

The fintech task force should have a nearer-term focus on applications (e.g. underwriting, payments, immediate regulation).

The AI task force should have a longer-term focus on risks (e.g. fraud, job automation, digital identification).

And explicitly, Chairwoman Waters explained her overall interest in regulation:

Make sure that responsible innovation is encouraged, and that regulators and the law are adapting to the changing landscape to best protect consumers, investors, and small businesses.

The appointed chairman of the Task Force on AI, Congressman Bill Foster (D-IL), extolled AI‚Äôs potential in a similar statement, but also cautioned,

It is crucial that the application of AI to financial services contributes to an economy that is fair for all Americans.

This first hearing did find ample AI applications in financial services. But it also concluded that these worried sentiments are neither misrepresentative of their constituents nor misplaced.

From left to right: Maxine Waters (D-CA), Chairwoman of the US House Committee on Financial Services; Bill Foster (D-IL), Chairman of the Task Force on AI; French Hill (R-AR), Ranking Member on the Task Force on AI

Risks of AI

In a humorous exchange later in the hearing, Congresswoman Sylvia Garcia (D-TX) asks a witness, Dr. Bonnie Buchanan of the University of Surrey, to address the average American and explain AI in 25 words or less. It does not go well.

DR. BUCHANAN

I would say it‚Äôs a group of technologies and processes that can look at determining general pattern recognition, universal approximation of relationships, and trying to detect patterns from noisy data or sensory perception. REP. GARCIA

I think that probably confused them more. DR. BUCHANAN

Oh, sorry.

Beyond making jokes, Congresswoman Garcia has a point. AI is extraordinarily complex. Not only that, to many Americans it can be threatening. As Garcia later expresses, ‚ÄúI think there‚Äôs an idea that all these robots are going to take over all the jobs, and everybody‚Äôs going to get into our information.‚Äù

In his opening statement, task force ranking member Congressman French Hill (R-AR) tries to preempt at least the first concern. He cites a World Economic Forum study [3] that the 75 million jobs lost because of AI will be more than offset by 130 million new jobs. But Americans are still anxious about AI development.

In a June 2018 survey of 2,000 Americans [4] conducted by Oxford‚Äôs Center for the Governance of AI, researchers observed

overwhelming support for careful management of robots and/or AI (82% support)

more trust in tech companies than in the US government to manage AI in the interest of the public

mixed support for developing high-level machine intelligence (defined as ‚Äúwhen machines are able to perform almost all tasks that are economically relevant today better than the median human today‚Äù)

This public apprehension about AI development is mirrored by concerns from the task force and experts. Personal privacy is mentioned nine times throughout the hearing, notably in Congressman Anthony Gonzalez‚Äôs (R-OH) broad question on ‚Äúbalancing innovation with empowering consumers with their data,‚Äù which the panel does not quite adequately address.

But more often, the witnesses discuss fairness and how AI models could discriminate unnoticed. Most notably, Dr. Nicol Turner-Lee, a fellow at the the Brookings Institution, suggests implementing guardrails to prevent biased training data from ‚Äúreplicat[ing] and amplify[ing] stereotypes historically prescribed to people of color and other vulnerable populations.‚Äù

And she‚Äôs not alone. A separate April 2019 Brookings report [5] seconds this concern of an unfairness ‚Äúwhereby algorithms deny credit or increase interest rates using a host of variables that are fundamentally driven by historical discriminatory factors that remain embedded in society.‚Äù

So if we‚Äôre so worried, why bother introducing the Pandora‚Äôs box of AI to financial services at all?

Benefits of AI

AI‚Äôs potential benefits, according to Congressman Hill, are to ‚Äúgather enormous amounts of data, detect abnormalities, and solve complex problems.‚Äù In financial services, this means actually fairer and more accurate models for fraud, insurance, and underwriting. This can simultaneously improve bank profitability and extend services to the previously underbanked.

Both Hill and Foster cite a National Bureau of Economic Research working paper [6] finding where in one case, algorithmic lending models discriminate 40% less than face-to-face lenders. Furthermore, Dr. Douglas Merrill, CEO of ZestFinance and expert witness, claims that customers using his company‚Äôs AI tools experience higher approval rates for credit cards, auto loans, and personal loans, each with no increase in defaults.

Moreover, Hill frames his statement with an important point about how AI could reshape the industry: this advancement will work ‚Äúfor both disruptive innovators and for our incumbent financial players.‚Äù At first this might seem counterintuitive.

‚ÄúDisruptive innovators,‚Äù more agile and hindered less by legacy processes, can have an advantage in implementing new technology. [7] But without the immense budgets and customer bases of ‚Äúincumbent financial players,‚Äù how can these disruptors succeed? And will incumbents, stuck in old ways, ever adopt AI?

Mr. Jesse McWaters, financial innovation lead at the World Economic Forum and the final expert witness, addresses this apparent paradox, discussing what will ‚Äúredraw the map of what we consider the financial sector.‚Äù Third-party AI service providers ‚Äî from traditional banks to small fintech companies ‚Äî can ‚Äúhelp smaller community banks remain digitally relevant to their customers‚Äù and ‚Äúenable financial institutions to leapfrog forward.‚Äù

Enabling competitive markets, especially in concentrated industries like financial services, is an unadulterated benefit according to free market enthusiasts in Congress. However, ‚Äúredrawing the map‚Äù in this manner makes the financial sector larger and more complex. Congress will have to develop policy responding to not only more complex models, but also a more complex financial system.

This system poses risks both to corporations, acting in the interest of shareholders, and to the government, acting in the interest of consumers.

Business and government look at risks

Businesses are already acting to avert potential losses from AI model failure and system complexity. A June 2019 Gartner report [8] predicts that 75% of large organizations will hire AI behavioral forensic experts to reduce brand and reputation risk by 2023.

However, governments recognize that business-led initiatives, if motivated to protect company brand and profits, may only go so far. For a government to protect consumers, investors, and small businesses (the relevant parties according to Chairwoman Waters), a gap may still remain.

As governments explore how to fill this gap, they are establishing principles that will underpin future guidance and regulation. The themes are consistent across governing bodies:

AI systems need to be trustworthy.

They therefore require some government guidance or regulation from government representing the people.

This guidance should encourage fairness, privacy, and transparency.

In the US, President Donald Trump signed an executive order [9] in February 2019 ‚Äúto Maintain American Leadership in Artificial Intelligence,‚Äù directing federal agencies to, among other goals, ‚Äú foster public trust in AI systems by establishing guidance for AI development and use. ‚Äú The Republican White House and Democratic House of Representatives seem to clash at every turn, but they align here.

The EU is also establishing a regulatory framework for ensuring trustworthy AI. Likewise included among the seven requirements in their latest communication from April 2019 [10]: privacy, transparency, and fairness.

And June‚Äôs G20 summit [11] drew upon similar ideas to create their own set of principles, including fairness and transparency, but also adding explainability.

These governing bodies are in a fact-finding stage, establishing principles and learning what they are up against before guiding policy. In the words of Chairman Foster, the task force must understand ‚Äúhow this technology will shape the questions that policymakers will have to grapple with in the coming years.‚Äù

An hour before Congresswoman Garcia‚Äôs amusing challenge, Dr. Buchanan reflected upon a couple common themes of concern.

Policymakers need to be concerned about the explainability of artificial intelligence models. And we should avoid black-box modeling where humans cannot determine the underlying process or outcomes of the machine learning or deep learning algorithms.

But through this statement, she suggests a solution: make these AI models explainable. If humans can indeed understand the inputs, process, and outputs of a model, we can trust our AI. Then throughout AI applications in financial services, we can promote fairness for all Americans.

Sources

[1] United States House Committee of Financial Services. ‚ÄúPerspectives on Artificial Intelligence: Where We Are and the Next Frontier in Financial Services.‚Äù https://financialservices.house.gov/calendar/eventsingle.aspx?EventID=403824. Accessed July 18, 2019.

[2] United States House Committee of Financial Services. ‚ÄúWaters Announces Committee Task Forces on Financial Technology and Artificial Intelligence.‚Äù https://financialservices.house.gov/news/documentsingle.aspx?DocumentID=403738. Accessed July 18, 2019.

[3] Leopold, Till Alexander, Vesselina Ratcheva, and Saadia Zahidi. ‚ÄúThe Future of Jobs Report 2018.‚Äù World Economic Forum. http://www3.weforum.org/docs/WEF_Future_of_Jobs_2018.pdf

[4] Zhang, Baobao and Allan Dafoe. ‚ÄúArtificial Intelligence: American Attitudes and Trends.‚Äù Oxford, UK: Center for the Governance of AI, Future of Humanity Institute, University of Oxford, 2019. https://ssrn.com/abstract=3312874

[5] Klein, Aaron. ‚ÄúCredit Denial in the Age of AI.‚Äù Brookings Institution. April 11, 2019. https://www.brookings.edu/research/credit-denial-in-the-age-of-ai/

[6] Bartlett, Robert, Adair Morse, Richard Stanton, Nancy Wallace, ‚ÄúConsumer-Lending Discrimination in the FinTech Era.‚Äù National Bureau of Economic Research, June 2019. https://www.nber.org/papers/w25943

[7] Snyder, Scott. ‚ÄúHow Banks Can Keep Up with Digital Disruptors.‚Äù Philadelphia, PA: The Wharton School of the University of Pennsylvania, 2017. https://knowledge.wharton.upenn.edu/article/banking-and-fintech/

[8] ‚ÄúGartner Predicts 75% of Large Organizations Will Hire AI Behavior Forensic Experts to Reduce Brand and Reputation Risk by 2023.‚Äù Gartner. June 6, 2019. https://www.gartner.com/en/newsroom/press-releases/2019-06-06-gartner-predicts-75-of-large-organizations-will-hire

[9] United States, Executive Office of the President [Donald Trump]. Executive order 13859: Executive Order on Maintaining American Leadership in Artificial Intelligence. February 11, 2019. https://www.whitehouse.gov/presidential-actions/executive-order-maintaining-american-leadership-artificial-intelligence/

[10] ‚ÄúBuilding Trust in Human-Centric Artificial Intelligence.‚Äù European Commission. April 8, 2019. https://ec.europa.eu/futurium/en/ai-alliance-consultation/guidelines#Top

[11] ‚ÄúG20 Ministerial Statement on Trade and Digital Economy.‚Äù June 9, 2019. http://trade.ec.europa.eu/doclib/press/index.cfm?id=2027
Attention in Deep Networks with Keras

Courtesy of Pixabay

This story introduces you to a Github repository which contains an atomic up-to-date Attention layer implemented using Keras backend operations. Available at attention_keras .

To visit my previous articles in this series use the following letters.

A B C D* E F G H I J K L* M N O P Q R S T U V W X Y Z

[üîàüî• Latest article üî•üîà]: M ‚Äî Matrix factorization

Why Keras?

With the unveiling of TensorFlow 2.0 it is hard to ignore the conspicuous attention (no pun intended!) given to Keras. There was greater focus on advocating Keras for implementing deep networks. Keras in TensorFlow 2.0 will come with three powerful APIs for implementing deep networks.

Sequential API ‚Äî This is the simplest API where you first call model = Sequential() and keep adding layers, e.g. model.add(Dense(...)) .

and keep adding layers, e.g. . Functional API ‚Äî Advance API where you can create custom models with arbitrary input/outputs. Defining a model needs to be done bit carefully as there‚Äôs lot to be done on user‚Äôs end. Model can be defined using model = Model(inputs=[...], outputs=[...]) .

. Subclassing API ‚Äî Another advance API where you define a Model as a Python class. Here you define the forward pass of the model in the class and Keras automatically compute the backward pass. Then this model can be used normally as you would use any Keras model.

For more information, get first hand information from TensorFlow team. However remember that while choosing advance APIs give more ‚Äúwiggle room‚Äù for implementing complex models, they also increase the chances of blunders and various rabbit holes.

Why this post?

Recently I was looking for a Keras based attention layer implementation or library for a project I was doing. I grappled with several repos out there that already has implemented attention. However my efforts were in vain, trying to get them to work with later TF versions. Due to several reasons:
Object Detection On Aerial Imagery Using RetinaNet

(Left) the original image. (Right) Car detections using RetinaNet, marked in green boxes

Detecting cars and swimming pools using RetinaNet

Introduction

For tax assessments purposes, usually, surveys are conducted manually on the ground. These surveys are important to calculate the true value of properties. For example, having a swimming pool can increase the property price. Similarly, the count of cars in a neighborhood or around a store can indicate the levels of economic activity at that place. Being able to achieve this through aerial imagery and AI, can significantly help in these processes by removing the inefficiencies, and the high cost and time required by humans.

To solve this problem, we‚Äôll try to detect cars and swimming pools in RGB chips of 224x224 pixels of aerial imagery. The training dataset had 3748 images with bounding box annotations and labels in PASCAL VOC format.

This problem along with the dataset was posted by ESRI on HackerEarth as the ESRI Data Science Challenge 2019. I participated and secured the 3rd place in the public leaderboard with a mAP (mean Average Precision) of 77.99 at IoU = 0.3 using the state-of-the-art RetinaNet model. In the following post, I‚Äôll explain how I attempted this problem.

RetinaNet

RetinaNet has been formed by making two improvements over existing single stage object detection models (like YOLO and SSD):

Feature Pyramid Network

Pyramid networks have been used conventionally to identify objects at different scales. A Feature Pyramid Network (FPN) makes use of the inherent multi-scale pyramidal hierarchy of deep CNNs to create feature pyramids.

The one-stage RetinaNet network architecture uses a Feature Pyramid Network (FPN) backbone on top of a feedforward ResNet architecture (a) to generate a rich, multi-scale convolutional feature pyramid (b). To this backbone RetinaNet attaches two subnetworks, one for classifying anchor boxes (c) and one for regressing from anchor boxes to ground-truth object boxes (d). The network design is intentionally simple, which enables this work to focus on a novel focal loss function that eliminates the accuracy gap between our one-stage detector and state-of-the-art two-stage detectors like Faster R-CNN with FPN while running at faster speeds.

Focal Loss

Focal Loss is an improvement on cross-entropy loss that helps to reduce the relative loss for well-classified examples and putting more focus on hard, misclassified examples.

The focal loss enables training highly accurate dense object detectors in the presence of vast numbers of easy background examples.

Focal Loss Function

If you are further interested in the finer details of the model, I‚Äôll suggest reading the original papers and this very helpful and descriptive blog ‚ÄòThe intuition behind RetinaNet‚Äô.

Now, let‚Äôs get started with the actual implementation and get coding. Here‚Äôs the Github repository you can follow along:

Installing Retinanet

We‚Äôll use the awesome Keras implementation of RetinaNet by Fizyr. I am assuming you have your deep learning machine setup. If not, follow my guide here. Also, I would recommend using a virtual environment. The following script will install RetinaNet and the other required packages.

Alternatively, you can use a GPU instance (p2.xlarge) on AWS with the ‚Äúdeep-learning-for-computer-vision-with-python‚Äù AMI. This AMI comes pre-installed with keras-retinanet and other required packages. You can start using the model after activating the RetinaNet virtual environment by workon retinanet command.

Note: Retinanet is heavy on computation. It will require at least 7‚Äì8 GBs of GPU memory for a batch size of 4 (224x224) images.

Once the RetinaNet is installed, create the following directory structure for this project.

I‚Äôll explain each one of these in details, but here is an overview:

build_dataset.py ‚Äî Python script to create the train/test set

config/esri_retinanet_config.py ‚Äî config file to be used by the build script.

dataset/annotations ‚Äî directory to hold all image annotations

dataset/images ‚Äî directory to hold all images

dataset/submission_test_data_images ‚Äî the submission test directory for the Esri Data Science challenge. You can ignore this if you are working on your own dataset and a different project.

snapshots ‚Äî the directory where all the snapshots of training will be saved after each epoch

models ‚Äî the directory where snapshots, converted for evaluation and testing. will be saved

tensorboard ‚Äî the directory to where training log will be saved to be used by tensorboard

predict.py ‚Äî script to make predictions on the submission test files

Building the dataset

First, we need to write a config file that will hold the paths to images, annotations, output CSVs ‚Äî train, test, and classes, and the test-train split value. Having such a config file makes the code versatile for use with different datasets.

In this config file, TRAIN_TEST_SPLIT = 0.75 . It is a standard practice to have a 75‚Äì25 or a 70‚Äì30 or in some cases even 80‚Äì20 split between training and testing dataset from the original dataset. But, for the purpose of this competition, I did not make a testing dataset and used the complete dataset for training. This was done because only a small dataset of 3748 images was provided. Moreover, a testing dataset of 2703 images was provided (without annotations) on which the model could be tested by submitting the predictions online.

Next, let‚Äôs write a Python script that will read all the image paths and annotations and output the three CSVs that are required during training and evaluating the model:

train.csv ‚Äî This file will hold all the annotations for training in the following format: <path/to/image>,<xmin>,<ymin>,<xmax>,<ymax>,<label>

Each row will represent one bounding box, therefore, one image can be present in multiple rows depending on how many objects have been annotated in that image. test.csv ‚Äî Similar to train.csv in format, this file will hold all the annotations for testing the model. classes.csv ‚Äî A file with all unique class labels in the dataset with index assignments (starting from 0 and ignoring the background)

Let‚Äôs start by creating a build_dataset.py file and importing the required packages. Notice, we import the esri_retinanet_config.py file that we created earlier in the config directory and we give it an alias config .

In the code above, we create an argument parser to take in, optionally, the image and annotation paths, output CSV paths, and the train-test split. Yes, I know we defined these arguments in the config file, already. But, I also realized that there were times when I wanted to create a subsample of images for an experiment or have a different train-test split, etc. At that time, having the option to pass on these arguments when executing the script, without changing the config file, was quicker. You can see that I have provided the default values for each argument from the config file itself. So, you are not required to provide these arguments, unless you want to. After parsing the arguments assign easy variable names for each argument.

In the preceding code, we read the image paths into a list, randomize the list, split it into train and test set and store them in another list dataset in the format (<dataset_type>, <list_of_paths>, <outpuCSV>) . We‚Äôll also initialize the CLASS set to hold all the unique class labels in the dataset.

Next, we loop over each dataset (train and test) and open the output CSV file to be written. For each dataset, we loop over each image path. For each image, extract the filename and build the corresponding annotation path. This works out because, usually, the image and annotation files have the same name but different extensions. For e.g. dataset/images/0000001.jpg has its annotations in dataset/annotations/0000001.xml . Modify this section if your dataset follows a different naming convention. Using BeautifulSoup parse the annotations (XML) file. We can then find the ‚Äúwidth‚Äù and ‚Äúheight‚Äù and the ‚Äúobject(s)‚Äù from the parsed XML.

For every image, find all the objects and iterate over each one of them. Then, find the bounding box (xmin, ymin, xmax, ymax) and the class label (name) for each object in the annotation. Do a cleanup by truncating any bounding box coordinate that falls outside the boundaries of the image. Also, do a sanity check if, by error, any minimum value is larger than the maximum value and vice-versa. If we find such values, we will ignore these objects and continue to the next one.

Now, that we have all the information, we can proceed to write to output CSV, one row at a time. Also, keep adding the labels to CLASSES set. This will eventually end up having all the unique class labels.

The last thing left to build the dataset in the required format is to write the class labels with their respective indexes to a CSV. In the ESRI dataset, there are only two classes ‚Äî cars (label: ‚Äò1‚Äô, index: 1) and swimming pool (label: ‚Äò2‚Äô, index: 0). This is how the classes.csv looks for Esri dataset.

2,0

1,1

Training and Evaluating the model

Now, that the dataset is ready and RetinaNet installed, let‚Äôs proceed to train the model on the dataset.

# For a list of all arguments

$ retinanet-train --help

To train the model, I used the following command:

$ retinanet-train --weights resnet50_coco_best_v2.1.0.h5 \

--batch-size 4 --steps 4001 --epochs 20 \

--snapshot-path snapshots --tensorboard-dir tensorboard \

csv dataset/train.csv dataset/classes.csv

It is advised to load a pre-trained model or weights file instead of training from scratch to speed up the training (the losses will start to converge earlier). I used the weights from the pre-trained model with ResNet50 backbone on COCO dataset. Use the following link to download the file.

The batch-size and the steps will depend on your system config (primarily GPU) and the dataset. I usually start with batch-size = 8 and then increase or decrease by a factor of 2 depending if the model training started successfully or not. If training begins successfully I‚Äôll terminate the training (CTRL+C) and start with a higher batch size else a lower one.

Once, you have decided on the batch-size, you‚Äôll need to calculate the steps you‚Äôll need to cover the total dataset in each epoch. The following command will give you the count of rows in train.csv created earlier in the dataset directory.

$ wc -l datatset/train.csv

The calculation for step size is simple: steps = count of rows in train.csv / batch-size . Next, set the number of epochs . In my experience, RetinaNet converges quickly, so a smaller number of epochs usually does the work. If not you can always, pick up the training from the last epoch and train your model further. Therefore, we‚Äôll provide a snapshot-path where the model will be saved after every epoch.

We will also provide a tensorflow-dir where all the logs will be saved and tensorboard can be run to visualize the training as it proceeds. To launch tensorboard, open a new terminal window and run the below mentioned command. Make sure you have tensorboard installed before running it.

# To launch tensorboard

$ tensorboard --logdir <path/to/logs/dir>

Finally, provide the csv files with training dataset and class labels. And execute the training command. Now, go do an Iron Man or sleep or whatever while your model trains. Each epoch with 3748 (224x224) images took a bit over 2 hours on a K80 Tesla GPU on AWS p2.xlarge instance.

Once the model has trained to your satisfaction, convert the model in a format that can be used for evaluation and predictions.

# To convert the model

$ retinanet-convert-model <path/to/desired/snapshot.h5> <path/to/output/model.h5> # To evaluate the model

$ retinanet-evaluate <path/to/output/model.h5> csv <path/to/train.csv> <path/to/classes.csv> # Sample evaluation

95 instances of class 2 with average precision: 0.8874

494 instances of class 1 with average precision: 0.7200

mAP: 0.8037

In this sample evaluation on 125 test images, the model was able to achieve 80.37% mAP (mean Average Precision) with 375 images training for 18 epochs. It‚Äôs a good result for such a small dataset.

Predictions

Build a script predict.py that will use the trained model, make predictions on the submission images and write it on the disk.

A few methods from the keras_retinanet utils are required to pre-process the image before it is fed into the model for predictions. Also, import the config file, we created earlier, for loading a few paths.

Construct the argument parser to accept arguments when executing the script, and then parse the arguments. The argument model will take in the path to the trained model file which will be used for making predictions. For class labels and predictions output directory, the default values have been taken from the config file. Therefore, these are not required arguments. The argument input will take in the path of the directory containing images to make the predictions on. Also, the confidence argument is available to filter weak predictions.

Next, load the class label mapping from the class label CSV and make it into a dictionary. Load the model to be used for prediction. Use the dir path provided in input argument to grab and make a list of all the image paths.

Iterate over each image path so that we can make predictions on each image in the provided dataset. Lines 6‚Äì9 in the code above extract the image file name from the image path and then construct and open an output text file path where the predictions for that image will be saved. In lines 11‚Äì15, we load the image, preprocess it, resize it and then expand its dimensions before passing it to the model. In Line 18, we pass the preprocessed image to the model and it returns the predicted boxes (bounding box coordinates), probability scores for each box and the associated labels. In the last line in the block above, rescale the bounding box coordinates according to the original image size.

Next, iterate over each detection that is predicted by the model. Skip the ones whose score is less than the confidence value provided. Although, if you want to calculate the mAP (mean Average Precision) keep all the predictions. For this, pass the value of confidence argument as 0.0 . Bounding box coordinates will be float values, so convert them into int . Construct the row for each prediction in the required format: <classname> <confidence> <ymin> <xmin> <ymax> <xmax> and write it to the file. Close the file once all the detections for that image have been written to the corresponding file.

$ python predict.py --model models/output.h5 --input dataset/submission_test_data_images --confidence 0.0

Run the above command to execute the predict.py script. Feel free to change the arguments according to your dataset and project.

Experiments and Results

Initially, I trained the model using only 10% of the data (375 images) for 18 epochs. This model had mAP of 71 with a confidence value of 0.5 on the test images. I resumed training the model on the complete dataset of 3748 images for another 10 epochs to result in an increased mAP of 74. I decided to engineer the model a bit and make changes to the anchor boxes. The dataset had only square bounding boxes, and I changed the aspect ratios of the boxes from [0.5, 1, 2] to [1] . It seemed like a good experiment to try, but I realized that it wasn‚Äôt as the anchor boxes ratios will change as the images are augmented. It resulted in the network training much faster than before with the total dataset as the network size decreased. The accuracy of predictions also increased a bit but then started to drop. I decided to use the 2nd epoch results with a confidence value of 0.0 to include all predictions. This resulted in the mAP of 77.99 which secured me the 3rd place in the challenge. I also, unsuccessfully, tried a few other experiments with scales of the images to be used for FPN and data augmentation parameters but stuck with earlier results for the final submission.

Summary

In this post, we talked about the state-of-the-art RetinaNet model and how I used it for the Esri Data Science Challenge 2019 to detect cars and swimming pools in 224x224 tiles of aerial imagery. We started with structuring the project directory. Next, we built the train/test dataset to be used by the model. The model was trained with the appropriate arguments, and later the trained model was converted for evaluation and prediction. We created another script to make detections on the submission test images and to write the predictions on the disk. In the end, I briefly describe the experiments I tried and the results I achieved.

References
Six Recommendations for Aspiring Data Scientists

Data science is a field with a huge demand, in part because it seems to require experience as a data scientist to be hired as a data scientist. But many of the best data scientists I‚Äôve worked with have diverse backgrounds ranging from humanities to neuroscience, and it takes demonstrated experience to stand out. As a new grad or analytics professional making the jump to a data science career, it can be challenging to build a portfolio of work to demonstrate expertise in this space. I‚Äôve been on both sides of the hiring process for data science roles and wanted to call out some of the key experiences that can help land a job as a data scientist:

Get hands-on with cloud computing Create a new data set Glue things together Stand up a service Create a stunning visualization Write a white paper

I‚Äôll expand on these topics in detail, but the key theme in data science is being able to build data products that add value to a company. A data scientist that can build these end-to-end data products is a valuable asset, and it‚Äôs useful to demonstrate these skills when pursuing a data science career.

Get hands-on with cloud computing

Many companies are looking for data scientists with past experience in cloud computing environments, because these platforms provide tools that enable data workflows and predictive models to scale to massive volumes. It‚Äôs also likely that you‚Äôll be using a cloud platform, such as Amazon Web Services (AWS) or Google Cloud Platform (GCP) in your everyday work.

The good news is that many of these platforms provide free tiers for becoming familiar with the platform. For example, AWS has free-tier EC2 instances and free usage of services such as Lambda for low volume requests, GCP offers $300 of free credit to try out most of the platform, and Databricks provides a community edition that you can use to get hands on with the platform. With these free options you won‚Äôt be able to work with massive data sets, but you can build experience on these platforms.

One of my recommendation is to try out different features on these platforms, and see if you can use some of the tools to train and deploy models. For example, for my model services post I leveraged a tool I was already familiar with, SKLearn, and investigated how to wrap a model as a Lambda function.

Create a new data set

In academic courses and data science competitions, you‚Äôre often provided a clean data set where the focus of the project is on exploratory data analysis or modeling. However, for most real-world projects you‚Äôll need to perform some data munging in order to clean a raw data set into a transformed data set that is more useful for an analysis or modeling task. Often, data mungling requires collecting additional data sets in order to transform data. For example, I worked with Federal Reserve data in a past role in order to better understand the asset allocation of affluent households in the US.

This was an interesting project, because I worked with third-party data in order to measure the accuracy of first-party data. My second recommendation is to actually go a step further and build a data set. This can include scraping a website, sampling data from an endpoint (e.g. steamspy), or aggregating different data sources into a new data set. For example, I created a custom data set of StarCraft replays during my graduate study, which demonstrated my ability to perform data munging on a novel data set.

Glue things together

One of the skills that I like to see data scientists demonstrate is the ability to make different components or systems work together in order to accomplish a task. In a data science role, there may not be a clear path to productizing a model and you may need to build something unique in order to get a system up and running. Ideally a data science team will have engineering support for getting systems up and running, but prototyping is a great skill for a data scientists to move quickly.

My recommendation here is to try to get different systems or components to integrate within a data science workflow. This can involve getting hands on with tools such as Airflow in order to prototype a data pipeline. It can involve creating a bridge between different systems, such as the JNI-BWAPI project I started to interface the StarCraft Brood War API library with Java. Or it can involve gluing different components together within a platform, such as using GCP DataFlow to pull data from BigQuery, apply a predictive model, and store the results to Cloud Datastore.

Stand up a service

As a data scientist, you‚Äôll often need to stand up services that other teams can use within your company. For example, this could be a Flask app that provides results from a deep learning model. Being able to prototype services means that other teams will be able to use your data products more quickly.

My recommendation is to get hands on experience with tools such a Flask or Gunicorn, in order to setup web endpoints, and Dash in order to create interactive web applications in Python. It‚Äôs also useful practice to try setting up one of these services in a Docker instance.

Create a stunning visualization

While great work should stand on its own, it‚Äôs often necessary to first get your audience‚Äôs attention before explaining why an analysis or model is important. My recommendation here is to learn a variety of visualization tools in order to create compelling visualizations that stand out.

Creating visualizations is also a useful way of building up a portfolio of work. The blog post below shows a sample of the different tools and datasets I explored over 10 years as a data scientist.

Write a white paper

One of the data science skills that I‚Äôve been advocating for recently is the ability to explain projects in the form of a white paper that provides an executive summary, discusses how the work can be used, provides details about the methodology, and results. The goal is to make your research digestible by a wide audience and for it to be self explanatory, so that other data scientists can build upon it.

Blogging and other forms of writing are great ways of getting experience in improving your written communication. My recommendation here is to try writing data science articles for broad audiences, in order to get experience conveying ideas at different levels of detail.

Conclusion

Data science requires hands on experience with a number of tools. Luckily, many of these tools are becoming more accessible and it‚Äôs becoming easier to build out a data science portfolio.
One of the biggest underrated challenges in machine learning development is the deployment of the trained models in production that too in a scalable way. One joke on it I have read is ‚ÄúMost common way, Machine Learning gets deployed today is powerpoint slides :)‚Äù.

Why Docker?

Docker is a containerization platform which packages an application & all its dependencies into a container.

Activating this container results in the application being active.

Docker is used when you have a lot of services which work in an isolated manner and serve as a data provider to a web application. Depending on the load, the instances can be spun off on demand on the basis of the rules set up.

Why Docker for Machine Learning models?

Production deployment of regular software applications is hard. If that software is a Machine Learning pipeline, it‚Äôs worse! And in today‚Äôs scenario, you can‚Äôt get away from machine learning, as it is the most competitive edge you can get in the business. In production, a Machine Learning powered application would be using several models for several purposes. Some major practical challenges in Machine Learning models deployment that can be handled through docker are:

Ununiform environments across models.

There can be cases where for one model you need LANG_LEVEL set to ‚Äòc‚Äô while for another LANG_LEVEL should be ‚Äòen_us.UTF-8‚Äô. Put different models in different containers so that isolated environments for different models will be obtained.

2. Ununiform library requirements across models.

You have developed a text summarizer using tensorflow 1.10. Now we want to have a sentiment analysis using transfer learning which is supported by tensorflow2.0(suppose). Putting them in different containers will not break the app.

Another major use case is, you develop ML models in python. But the application you want to make in Go language (for some technical advantages), then exposing the ml model through docker to the app will solve it.

3. Ununiform resource requirements across models.

You have a very complex object detection model which requires GPU, and you have 5 different neural networks for other purposes which are good to run on CPU. Then on deploying the models in containers, you get the flexibility of assigning resources as per requirement.

4. Ununiform traffics across models.

Suppose you have a question identifier model and answer generation mode.w The former is called frequently while the latter one is not called that frequent. Then you need more instances of question identifier than answer generator. This can be easily handled by docker.

Another scenario is, at 10 am you have 10000 requests for your model whereas at 8 pm it is only 100. So you need to spin off more serving instances as per your requirements, which is easier in docker.

5. Scaling at model level

Suppose you have a statistical model which serves 100000 requests per second, whereas a deep learning model capable of serving 100 requests per second. Then for 10000 requests, you need to scale up only the deep learning model. This can be done by docker.

Now let‚Äôs see how to create a container of a deep learning model. Here the model I have built is a question topic identifier on the question classifier dataset available at http://cogcomp.org/Data/QA/QC/. Google‚Äôs Universal Sentence Encoder is used for word embedding.

While creating a container for a model, the workflow normally has to be followed is:

Build and train the model. Create an API of the model. (Here we have put it in a flask API). Create the requirements file containing all the required libraries. Create the docker file with necessary environment setup and start-up operations. Build the docker image. Now run the container and dance as you are done :)

Build and train the model.

To build and train the model, a basic workflow is to get the data, do the cleaning and processing of the data and then fed the data to the model architecture to get a trained model.

For example, I have built a question intent classifier model on the TREC dataset available at http://cogcomp.org/Data/QA/QC/. The training data has 6 intents with the number of instances of each is as follows:

Counter({'DESC': 1162,

'ENTY': 1250,

'ABBR': 86,

'HUM': 1223,

'NUM': 896,

'LOC': 835})

The model creation can be seen at https://github.com/sambit9238/QuestionTopicAnalysis/blob/master/question_topic.ipynb

The processing steps followed here are:

Dealing with contractions like I‚Äòll, I‚Äòve etc. Dealing with hyperlinks, mail addresses etc. Dealing with numbers and ids. Dealing with punctuations.

For embedding, Google's universal sentence encoder is used from tensorflow-hub.

The model architecture followed is a neural network with 2 hidden layers each with 256 neurons. To avoid overfitting, L2 regularization is used.

Layer (type) Output Shape Param #

=================================================================

input_1 (InputLayer) (None, 1) 0

_________________________________________________________________

lambda_1 (Lambda) (None, 512) 0

_________________________________________________________________

dense_1 (Dense) (None, 256) 131328

_________________________________________________________________

dense_2 (Dense) (None, 256) 65792

_________________________________________________________________

dense_3 (Dense) (None, 6) 1542

=================================================================

Total params: 198,662

Trainable params: 198,662

Non-trainable params: 0

_________________________________

The model is stored in .h5 file for reuse. Label encoder is stored in the pickle file for reuse.

Create an API of the model. (Here we have put it in a flask API).

The stored model is put in Flask api so that it can be used in production (https://github.com/sambit9238/QuestionTopicAnalysis/blob/master/docker_question_topic/app.py.)

The API expects a list of texts, as multiple sentences will come while using in real time. It goes through cleaning and processing to fed for prediction. The predicted results are scaled to represent the confidence percentage of each intent. The scaled results are then sent in JSON format.

For example,

input: [ ‚ÄúWhat is your salary?‚Äù]

output: {‚ÄòABBR‚Äô: 0.0012655753, ‚ÄòDESC‚Äô: 0.0079659065, ‚ÄòENTY‚Äô: 0.011016952, ‚ÄòHUM‚Äô: 0.028764706, ‚ÄòLOC‚Äô: 0.013653239, ‚ÄòNUM‚Äô: 0.93733364}

That means the model is 93% confident that the answer should be a number for this question.

Create the requirements file containing all the required libraries.

To create a Docker image to serve our API, we need to create a requirement file with all the used libraries along with their versions.

Create the Dockerfile with necessary environment setup and start-up operations.

A Dockerfile is a text document that contains all the commands a user could call on the command line to assemble an image.

For our example, the pre-built python-3.6 image is taken as a base image. Then the pre-trained Universal Sentence Encoder model files have been downloaded followed by the installation of required libraries. The 5000 port of docker is exposed, this is the port where flask app will run as it is in the default configuration.

Build the docker image. and run the container

Now we have the Dockerfile, flask API and trained model files in a directory. Hence we need to create the docker image out of it. The command for that can be:

docker build -t question_topic .

#the last option is location of the directory. Since I am in the directory so '.' is put, which represents current directory in unix.

Now the docker image is created, we need to run the image in a container docker run -p 8888:5000 --name question_topic question_topic

It will make the created docker image run. The port 5000 in the docker is mapped to 8888 port of host machine. Hence, the API will receive and response requests at port 8888. If you want to run the docker in the background and detach it from the command prompt (which will be the case in real time), run it with ‚Äò-d‚Äô option.

To check the outputs of the docker let‚Äôs send a post request using curl.

input:

To supply the input, curl ‚Äî request POST

‚Äî url http://0.0.0.0:8888/predict_topic

‚Äî header ‚Äòcontent-type: application/json‚Äô

‚Äî data ‚Äò{‚Äúrawtext_list‚Äù:[‚ÄúWhere do you work now?‚Äù, ‚ÄúWhat is your salary?‚Äù]}‚Äô

output:

{

‚Äúinput‚Äù: ‚Äú[‚ÄòWhere do you work now?‚Äô, ‚ÄòWhat is your salary?‚Äô]‚Äù,

‚Äúoutput‚Äù: ‚Äú[ {‚ÄòABBR‚Äô: 0.0033528977, ‚ÄòDESC‚Äô: 0.0013749895, ‚ÄòENTY‚Äô: 0.0068545835, ‚ÄòHUM‚Äô: 0.7283039, ‚ÄòLOC‚Äô: 0.25804028, ‚ÄòNUM‚Äô: 0.0020733867},

{‚ÄòABBR‚Äô: 0.0012655753, ‚ÄòDESC‚Äô: 0.0079659065, ‚ÄòENTY‚Äô: 0.011016952, ‚ÄòHUM‚Äô: 0.028764706, ‚ÄòLOC‚Äô: 0.013653239, ‚ÄòNUM‚Äô: 0.93733364} ]‚Äù

}

It seems the docker is running fine :)

Notes:

The mentioned example is not production ready. But it can be production ready by following few things like:
Finding similar images using Deep learning and Locality Sensitive Hashing

A simple walkthrough on finding similar images through image embedding by a ResNet 34 using FastAI & Pytorch. Also doing fast semantic similarity search in huge image embeddings collections. Aayush Agrawal ¬∑ Follow Published in Towards Data Science ¬∑ 8 min read ¬∑ Mar 17, 2019 -- 5 Listen Share

Fina output with similar images given an Input image in Caltech 101

In this post, we are trying to achieve the above result, i.e., given an image, we should be able to find similar images from the Caltech-101 database. The post guides with an end to end process on how I went about building this. The entire codebase for replicating the project is in my GitHub repository. The process to achieve the above result can be broken down in these few steps -

Transfer learning from a ResNet-34 model(trained on ImageNet) to detect 101 classes in Caltech-101 dataset using FastAI and Pytorch. Take the output of second last fully connected layer from trained ResNet 34 model to get embedding for all 9,144 Caltech-101 images. Use Locality Sensitive hashing to create LSH hashing for our image embedding which enables fast approximate nearest neighbor search Then given an image, we can convert it into image embedding using our trained model and then search similar images using Approximate nearest neighbor on Caltech-101 dataset.

Part 1 ‚Äî Data understanding and Transfer learning

As I mentioned above, for this project, my goal is to query any given image and find a semantically similar image in the Caltech-101 database. This database contains 9,144 images divided into 101 categories. Each category has about 50‚Äì800 images in them.

Image examples from Caltech-101 database

The first exercise in our project is to obtain a deep learning network which can classify these categories accurately. For this task, we will use a pre-trained ResNet 34 network which is trained on the ImageNet database and transfer learn it to classify 101 categories of Caltech-101 database using Pytorch 1.0 and FastAI library. As I have written about exactly how to do transfer learning with any given dataset in my previous blog, I am just going to outline the process in this blog. You can refer to this notebook to find the code to do the same. Find below the steps to do transfer learning for classifying Caltech-101 images -

Load the data using dataset loaders of Pytorch using FastAI library Take a pre-trained network, in this case, a ResNet 34 and remove it‚Äôs last fully connected layers Add new fully connected layers at the end of the network and train only those layers using the Caltech-101 image, while keeping all the other layers frozen Train the entire network by unfreezing all the layers

Part 2 ‚Äî Extracting image embeddings using Pytorch Hooks

Now that we have a pre-trained network, we need to extract embeddings from this network for all of our Caltech-101 images. Embedding is nothing but a representation of an object in an N-dimensional vector. An image embedding, in this case, is a representation of an image in N-dimension. The basic idea is the closer a given image to another image their embedding will also be similar and close in the spatial dimension.

Image embedding visualization. Credit ‚Äî Blog

You can see in the above image taken from this blog that image embedding is a spatial representation of an image in the vectorized form where similar images are close in spatial dimension as well.

We can obtain image embeddings from a ResNet-34 by taking the output of its second last Fully-connected layer which has a dimension of 512. To save intermediate calculations in a deep learning model in Pytorch for inspection or in our case to extract embeddings we use Pytorch Hooks. Hooks can be of two types ‚Äî forward and backward. Forward hooks are used to save information passing forward in a network to make an inference while backward hooks are used to collect information about gradients during backpropagation. In our case, we need output of our second last Fully connected layers in the inference stage which means we need to use a forward hook. Let‚Äôs look at the code for creating a hook (also in ‚ÄúExtracting Feature‚Äù section of my notebook) ‚Äî

class SaveFeatures():

features=None

def __init__(self, m):

self.hook = m.register_forward_hook(self.hook_fn)

self.features = None

def hook_fn(self, module, input, output):

out = output.detach().cpu().numpy()

if isinstance(self.features, type(None)):

self.features = out

else:

self.features = np.row_stack((self.features, out))

def remove(self):

self.hook.remove()

The above code is all you need in creating a Pytorch hook. The SaveFeatures class invokes register_forward_hook function from the torch.nn module and given any model layer it will save the intermediate computation in a numpy array which can be retrieved using SaveFeatures.features functions. Let‚Äôs see the code to use this class ‚Äî

## Output before the last FC layer

sf = SaveFeatures(learn.model[1][5]) ## By running this feature vectors would be saved in sf variable initated above

_= learn.get_preds(data.train_ds)

_= learn.get_preds(DatasetType.Valid) ## Converting in a dictionary of {img_path:featurevector}

img_path = [str(x) for x in (list(data.train_ds.items)+list(data.valid_ds.items))]

feature_dict = dict(zip(img_path,sf.features))

Line 1‚Äì2: Invokes the class SaveFeatures using model layer reference to the output of second last fully-connected layer as the input.

Line 4‚Äì6: Passing the Caltech-101 data to get their predictions. Note that we are not interested in saving predictions and that‚Äôs why we used ‚Äú_.‚Äù In this case, the intermediate output of second last layers in saved in the variable named ‚Äúsf‚Äù, which is an instance of SaveFeatures class.

Line 8‚Äì10: Creating a python dictionary where image path is the key and image embeddings is the value.

Now we have embedding representation of each image in Caltech-101 in our dictionary.

Part 3 ‚Äî Locality Sensitive Hashing for fast approximate nearest neighbor search

We can use our newly generated Caltech 101 image embeddings and get a new image, convert it into embedding to calculate distance b/w the new image and all the Caltech 101 database to find similar images. This process is computationally expensive in nature and as a new image embedding have to compare with all the 9K+ image embedding in the Caltech 101 database to find the most similar image(nearest neighbor), which in computational complexity notation is an O(N¬≤) problem and will take exponentially more time to retrieve similar images as the number of images increases.

To solve this problem, we will use locality sensitive hashing(LSH) which is an approximate nearest neighbor algorithm which reduces the computational complexity to O(log N). This blog explains LSH in good details in terms of time complexity and implementation. In short, LSH generates a hash value for image embeddings while keeping spatiality of data in mind; in particular; data items that are similar in high-dimension will have a higher chance of receiving the same hash value.

Below are the steps on how LSH converts an embedding in a hash of size K-

Generate K random hyperplanes in the embedding dimension Check if particular embedding is above or below the hyperplane and assign 1/0 Do step 2 for each K hyperplanes to arrive at the hash value

the hash value of the orange dot is 101 because it: 1) above the purple hyperplane; 2) below the blue hyperplane; 3) above the yellow hyperplane. Image Credit ‚Äî Link

Let‚Äôs now look at how LSH will perform an ANN query. Given a new image embedding, we will use LSH to create a hash for the given image and then compare the distance from image embedding of the pictures of Caltech-101 dataset which shares the same hash value. In this way, instead of doing similarity search over the whole Caltech-101 database we will only do a similarity search with a subset of images which shares the same hash value with the input image. For our project, we are using lshash3 package for an approximate nearest neighbor search. Let‚Äôs look at the code to do the same (you can find the code in the ‚ÄúUsing Locality Sensitive hashing to find near similar images‚Äù section of my notebook)-

from lshash import LSHash



k = 10 # hash size

L = 5 # number of tables

d = 512 # Dimension of Feature vector

lsh = LSHash(hash_size=k, input_dim=d, num_hashtables=L) # LSH on all the images

for img_path, vec in tqdm_notebook(feature_dict.items()):

lsh.index(vec.flatten(), extra_data=img_path)

The above code takes the image embedding dictionary and converts it into LSH table. To query the LSH table, we can use the code below ‚Äî

# query a vector q_vec

response = lsh.query(q_vec, num_results= 5)

Part 4 ‚Äî Putting it all together

Now we have our LSH table created let‚Äôs write a script which can take an image URL as an input and give us N(user-defined) similar images from CalTech 101 database. The code for this part is on my Github here.

Process flow of the find similar image script.

The script does the following task -

Load the LSH table and our ResNet 34 model (load_model function) Take the image URL from user call and download the image ( download_img_from_url function) Pass the image from ResNet-34 to get 512 dimension image embedding ( image_to_vec function) Query it with LSH table to find N(user-defined) similar images and their path ( get_similar_images function) Return the output at the desired output path, optionally display it using Open CV (get_similar_images function)

We can use a similar concept in various applications like finding similar images in our photo gallery, item-item recommendation of similar looking items, doing a web search on images, finding near-duplicate images, etc.

Summary (TL;DR).

In the blog, we saw an application of deep learning in finding semantically similar images and how to do an approximate nearest neighbor query using Locality-sensitive hashing(LSH) to speed up query time for large datasets. Also, it‚Äôs important to note that we used LSH not on the raw features(images) but on the embeddings which help do fast similarity search in huge collections.

I hope you enjoyed reading, and feel free to use my code on Github to try it out for your purposes. Also, if there is any feedback on code or just the blog post, feel free to reach out on LinkedIn or email me at aayushmnit@gmail.com. You can also follow me on Medium and Github for future blog post and exploration project codes I might write.
So what‚Äôs a Generalist and a Specialist?

Before going any further, let‚Äôs first understand what we mean when we talk about being a generalist and a specialist in data science.

A generalist is someone that has knowledge in many areas whereas a specialist knows a lot in one area. Simple as that.

Particularly in data science, it‚Äôs notoriously hard to become a generalist in all phases of data science project lifecycle. It takes years to acquire all the skills in different areas, yet it‚Äôs not necessary to master all of them.

Similarly, it‚Äôs not easy to be a specialist in data science either.

Now you might define a generalist as the Jack of all trades, master of none.

I couldn‚Äôt agree more about that.

And this is precisely the reason why I‚Äôd choose to be a specialist in the later stage of my data science path.

Why be a Generalist first?

After all, being a generalist is not meant to master anything, but rather to understand the full picture of the whole data science project lifecycle.

The question is: Why is understanding the full flow of the data science project lifecycle important in the first place?

You see. As a data scientist, we don‚Äôt build a machine learning model just for the sake of building it. We first understand the business problem and frame that into a problem that can be solved through data science approach. Then you need to identify data sources and define success metrics. Depending on your company‚Äôs stage of maturity, you might also need to build a pipeline to collect data (YES, you may not even have data in place)‚Ä¶

We can still go on and on but here is the point ‚Äî all the job scopes above are part of being a generalist. The good thing is that you‚Äôll get to know the full picture of the data science problem as a generalist ‚Äî as a data scientist in the beginning of your career.

In other words, you‚Äôll learn and you‚Äôll grow, tremendously.

What I‚Äôm advocating here is this: If you‚Äôre someone who is starting out in data science, my recommendation is to be a generalist first. Go join a startup and take on many hats as you will probably be the only data scientist in your company. Generalists add more value than specialists in a company‚Äôs early days, since you‚Äôre building most of your product from scratch and something is better than nothing.

Your machine learning models don‚Äôt have to be a game changer but should be able to provide actionable insights and results.

Learn how you can help the company generate more revenues. Learn how you can leverage the existing data or build some pipeline to collect data to solve some problems.

Start with the low-hanging fruit first. There isn‚Äôt always a need to go for AI if the company isn‚Äôt ready for that. In fact, normal statistical approach is typically sufficient to tackle some simple problems.

The ideal data scientist is a strong generalist who also brings unique specialties that complement the rest of the team

Be a strong generalist. Be the Jack of a trades.

Once you‚Äôve enough experience and you‚Äôve found your interest and passion in a specific area (say NLP), then you can deep dive into that, which leads us to the next stage.

Why be a Specialist later?

Say if you‚Äôre a NLP specialist. Your focus could be solely on building the best NLP classifier model given the data. And that‚Äôs it.

All the things are already set for you. The business problems are well defined (done by product managers). The pipeline is ready and maintained 24/7 (done by data engineers) and the data is there for collection. What you need to do is do what you‚Äôre best at. This is crucial as you can focus on your expertise and strength to add the highest values to the project.

It‚Äôs perfectly fine to be specialist in data science. Being a specialist in your niche plays an important role in a company, which is also something that makes you irreplaceable and valuable to others.

At this stage, since now you‚Äôre already experienced in different areas as a specialist in data science. Your experience and expertise are not something that can be easily substituted by others.

Even better, you‚Äôll be able to focus on your specialization and work with others as a team with your broad knowledge and understanding of other parts of the data science workflow.
Reading SQL queries into Pandas dataframes is a common task, and one that can be very slow. Depending on the database being used, this may be hard to get around, but for those of us using Postgres we can speed this up considerably using the COPY command. However, there are several ways of using the COPY command to get data from SQL into pandas, with different memory/speed tradeoffs. In this article, we‚Äôll test several different methods against each other.

The test dataset is simply the first five million rows of a sample Triage predictions table, which is just one I had handy. I tried to use all thirteen million rows I had in my local Postgres database, but pandas.read_sql crashed so I decided to bring down the dataset to something it could handle as a benchmark.

Included which each method are three statistics:

Peak memory ‚Äî the highest amount of memory used during the sql read code. This is the important one to see if your program will crash!

Increment memory ‚Äî the amount of memory that is still used at the end of the sql read code. In theory, this would be the same for all of the methods, but memory leaks can make different methods retain more memory.

Elapsed time ‚Äî the clock time used by the program.

The pandas version used here is 0.24.1.

First, a quick rundown of the different methods being tested:

pandas.read_sql ‚Äî the baseline

tempfile ‚Äî Using the tempfile module to make a temporary file on disk for the COPY results to reside in before the dataframe reads them in

StringIO ‚Äî Using a StringIO instead of disk; more memory used, but less disk I/O

Compressed BytesIO, pandas decompress ‚Äî Using a BytesIO instead of a StringIO, and compressing the data; should use less memory, but take longer

Compressed BytesIO, gzip decompress ‚Äî Same as the other compressed bytesio, but using GzipFile to decompress rather than pandas

Compressed tempfile ‚Äî Applying the compression idea to the diskfile; should reduce the disk I/O needed

Compressed BytesIO, low compression level ‚Äî A lower compression level to try and split the difference between uncompressed and compressed methods

pandas.read_sql

This is the baseline. Nothing fancy here.

Peak memory: 3832.7 MiB / Increment memory: 3744.9 MiB / Elapsed time: 35.91s

Using a temporary file

Here is our first attempt of using the COPY command. The data from the COPY command has to go do a filehandle: what simpler way to do this than using a temporary file?

Peak memory: 434.3 MB / Increment memory: 346.6 MB / Elapsed time: 8.93s

That‚Äôs‚Ä¶much better. I‚Äôm not surprised that the elapsed time is far quicker than read_sql, but I‚Äôm a bit surprised that the memory usage is so much different. Anyway, let‚Äôs keep going

Using a StringIO

Disk I/O can be expensive, especially depending on what type disk is available. Can we speed it up by using a StringIO for the filehandle? This would take more memory, of course, but maybe that‚Äôs a tradeoff we can make.

Peak memory: 434.2 MB / Increment memory: 346.6 MB / Elapsed time: 9.82s

This is a surprising result. I would have expected this to use more memory and be faster, but it‚Äôs neither. My hypothesis would be that the peak memory used by the StringIO ends up being surpassed by a spike during the dataframe creation process.

Also of note: the increment memory was the same as the temporary file version, which probably tells us that the 346.6 MB is a good reference for what the baseline for that memory should be without any memory leaks.

Using a Compressed BytesIO, pandas decompression.

Could we bring down on the memory required for the in-memory option? Given the prior results this may seem like a fool‚Äôs errand, but I already wrote the code so I‚Äôm not going to cut off the test early! Python‚Äôs GzipFile interface wraps a filehandle (in this case, a BytesIO) and handles compression. We let pandas handle the decompression by passing `compression=‚Äôgzip‚Äô` to read_csv

Peak memory: 613.6 MB Increment memory: 525.8 MB, Elapsed time: 1:30m

Not good! It actually used more memory (and leaked some) compared to the uncompressed versions.

Using a Compressed BytesIO, Gzip decompression

Same as the last one, except we bypass pandas‚Äô decompression routines in case they introduced a problem. GzipFile can handle the decompression for us, too!

Peak memory: 504.6 MB Increment memory: 416.8 MB, Elapsed time: 1:42m

Well, this is better RAM-wise than the pandas decompression, for sure, but this is still worse than the uncompressed versions.

Using a Compressed tempfile

The compression idea can also apply to the tempfile method from before. In this case, compression should help us cut down on disk I/O.

Peak memory: 517.2 MB Increment memory: 429.5 MB, Elapsed time: 1:35m

Similar to the other gzip examples. Not a good option.

Using a compressed BytesIO, low compression level

Since we‚Äôre trying things out, we have one more avenue to explore: the gzip compression level. The default for all of the prior examples is 9, the highest compression possible. It‚Äôs possible that in doing this, it takes extra memory to do the compression in addition to extra time. What if we flip one of them to the lowest compression level (1)?

Peak memory: 761.5 MB Increment memory: 673.8 MB, Elapsed time: 1:13m

Slightly better on time, but worse on RAM: it appears that the gzipping process is using a bunch of memory no matter what and doesn‚Äôt stream well.

Conclusion

What did we learn here?
Convolutional Neural Network: A Step By Step Guide Shashikant ¬∑ Follow Published in Towards Data Science ¬∑ 9 min read ¬∑ Mar 17, 2019 -- 1 Listen Share

‚ÄúArtificial Intelligence, deep learning, machine learning ‚Äî whatever you‚Äôre doing if you don‚Äôt understand it ‚Äî learn it. Because otherwise, you‚Äôre going to be a dinosaur within three years‚Äù ‚Äî Mark Cuban, a Serial Entrepreneur

Hello and welcome, aspirant!

If you are reading this and interested in the topic, I‚Äôm assuming that you are familiar with the basic concepts of deep learning and machine learning.

If not, don‚Äôt worry! The tutorial is designed in a way that gets you started with deep learning skills from the beginning to the end‚Äïfrom perceptron to deep learning.

In this tutorial, we‚Äôll touch base on the aspects of neural networks, models, and algorithms, some use cases, libraries to be used, and of course, the scope of deep learning. In addition to it, other important concepts for deep learning will also be discussed.

Step 1: Pre-requisites

Any Tom, Dick, and Harry cannot just hear about deep learning wonders, develop interest and start a tutorial. There has to be a fine way of learning, and that‚Äôs why we have laid the foundation work for you. Following are the points that highlight what all you need to do before you fire up your learning process:

Knowledge of R/Python: These are the two most commonly used and preferred languages for deep learning. One of the main reasons is that there is enough support/ community available for both. Before you jump into the world of machine learning, select one of these at your convenience. Undoubtedly, Python is leading the field; however, you can see the comparison here.

These are the two most commonly used and preferred languages for deep learning. One of the main reasons is that there is enough support/ community available for both. Before you jump into the world of machine learning, select one of these at your convenience. Undoubtedly, Python is leading the field; however, you can see the comparison here. Basic understanding of linear algebra, calculus, and probability: There are an incredible amount of online videos and courses are available for each lesson of which, many of which are free. We are not suggesting you hone the skill but just brush it up for a bright understanding of the tutorial. You may try starting with Stanford‚Äôs CS231n.

There are an incredible amount of online videos and courses are available for each lesson of which, many of which are free. We are not suggesting you hone the skill but just brush it up for a bright understanding of the tutorial. You may try starting with Stanford‚Äôs CS231n. Primary know-how of neural networks and deep learning: As I said earlier, plenty of sources are available online for free as well as paid. Online videos are always helpful anyway. If you want to read through the concept, we suggest you follow Neural Networks and Deep Learning, which is absolutely free. (Also, pay attention to 25 Must-Know Terms & concepts for Beginners in Deep Learning)

As I said earlier, plenty of sources are available online for free as well as paid. Online videos are always helpful anyway. If you want to read through the concept, we suggest you follow Neural Networks and Deep Learning, which is absolutely free. (Also, pay attention to 25 Must-Know Terms & concepts for Beginners in Deep Learning) Set-up requirements: Since deep learning relies heavily on computational concepts, we need faster machines to operate at that level. So, all that you need for now is:

GPU (4+ GB, preferably Nvidia) ‚Äî It is the heart of deep learning applications today CPU (e.g. Intel Core i3 or above will do) 4 GB RAM or depending on the dataset

Note: (If you want to learn more about hardware requirements, go through this hardware guide, and most importantly, do not install deep learning libraries at this step. You‚Äôll be told further in this tutorial.)

Step 2: Introduction to Concepts and Technical Aspects

How Can You Dive Into Deep Learning? Essentially, it all starts with neural networks, and deep learning is nothing but the logical implementation of those networks to abstract useful information from data. In technical terms, it is a discreet method of classification of unstructured input data such as media which includes images, sound, video, and text.

Firstly, you need to decide which learning medium suits you the best for your research and study deep learning. It could be blogs, books, videos, or online courses approach. We are listing the sources for you to start with the simplest concept that will help you to get a grip on the subject gradually.

Blog Approach

- Fundamentals of Deep Learning ‚Äî Starting with Artificial Neural Network

- A Deep Learning Tutorial: From Perceptron to Deep Networks

Book Approach

- Neural networks and Deep Learning (A free book by Michael Neilson)

- Deep Learning (An MIT Press book)

Video Approach

- Deep Learning SIMPLIFIED

- Neural networks class ‚Äî Universit√© de Sherbrooke

Online Course Approach

- Neural Network by (Enroll starts 27 Nov)

- Machine Learning by Andrew Ng (Enroll Starts 27 Nov)

- Machine Learning By Nando de Freitas (contains videos, slides, and a list of assignments)

Dear learners, accept the fact that transformation to becoming a deep learning expert would require plentiful time, many additional resources, and dedicated practice in building and testing models. We, however, do believe that utilizing the resources listed above could set you in motion with deep learning.

Step 3: Select Your Adventure

After you have got the basics, here comes the interesting part‚Äïhands-on experience in deep learning state-of-the-art technology. There are numerous exciting applications and opportunities that the field has to offer. Techniques in deep learning will vary based on your interest and purpose, see below:

Computer vision/ pattern recognition: Both are not much different since pattern recognition is also a part of computer vision, many times. However, in broader terms, computer vision includes analyzing only images and is used for object detection, segmentation, vision-based learning, etc. whereas pattern recognition is not restricted to images. It is about the classification of anything which can possess a pattern.

To learn, go here:

Deep Learning for Computer Vision

CS231n: Convolutional Neural Networks for Visual Recognition

For Video and use cases:

Detailed lectures on computer vision

Speech and audio recognition: Ever said ‚ÄúOk Google‚Äù? I‚Äôm sure, you did. It comprises a speech recognition system that helps you find what you‚Äôre looking for on Google.

Technically, it consists of a type of neural network that involves sequences of inputs to create cycles in the network graph called recurrent neural networks (RNNs). They are called ‚ÄòRecurrent‚Äô because they perform the same task for every element of the sequence and perform tasks such as machine translation or speech recognition.

To learn, go here:

The Unreasonable Effectiveness of Recurrent Neural Networks

Recurrent Neural Networks Tutorial

Understanding LSTM Networks (a wildly used RNN variant)

For Videos:

A friendly introduction to Recurrent Neural Networks

Recurrent Neural Networks (RNN)

Natural Language Processing OR NLP: NPL is a way for computers to read, analyze and respond by simulating the human language in a smart and useful manner. Today, technology is widely applied to multiple industry segments such as advertising, customer care, insurance, etc. to automate the process of human-computer interaction.

The NPL layer translates user requests or queries into information and searches for a proper response from its database. An advanced example of NLP would be a language translation‚Äïfrom one human language to another. For instance, English to German.

To learn, go here:

Ultimate Guide to Understand & Implement Natural Language Processing

NLPFORHACKERS.IO

How to Get Started with Deep Learning for Natural Language Processing

For videos:

Introduction to Natural Language Processing

Natural Language Processing with Deep Learning

Reinforcement Learning OR RL: Imagine a robot trained to learn from its previous actions and perform a new task, whenever required, wouldn‚Äôt that be great, and automatic! In fact, it is for real.

Reinforce learning introduces a similar concept for a computer agent; whether it fails or succeeds in a particular task, the agent receives rewards and punishments for the action on an object. It gains knowledge on it as part of the deep learning model controlling its actions.

To learn, go here:

A Beginner‚Äôs Guide to Reinforcement Learning (for Java)

Simple Beginner‚Äôs guide to Reinforcement Learning & its implementation

For Videos:

Deep Reinforcement Learning

Reinforcement Learning

Step 4: Choosing the Right Framework

We discussed many applications and usage of deep learning technologies in step 3. Chances are, for some tasks, traditional machine learning algorithms would be enough. But, if you‚Äôre dealing with a large collection of images, videos, text or speech, deep learning is bliss and everything for you. However, in deep learning, which framework will be the right choice for you is a question for many.

Remember, there‚Äôs no right framework, there is only a suitable framework. Here‚Äôs what your selection criteria should primarily depend on:

Availability of pre-trained models

Open-source

Supported operating systems and platforms

Licensing model

Ease of model definition and tuning

Availability of debugging tools

Ease of extensibility (for example, able to code new algorithms)

Connected to a research university or academia

Supported deep learning algorithmic families and models

To assist you in selecting one, let me take you on a brief tour of Deep Learning frameworks:

(a) TensorFlow: Backed by Google, TensorFlow is an all-purpose deep learning library for numerical computation based on data flow graph representation.

- Try out its introductory tutorial

- To install TensorFlow, visit here

- Refer to its documentation

- Have a look at its whitepaper

(b) Theano: Theano, a math expression compiler, is an actively developed architecture that efficiently defines, optimizes, and evaluates mathematical expressions having multi-dimensional arrays.

- Try out an introductory tutorial

- To install Theano, visit here

- Keep the documentation handy

(c) Caffe: While Theano and TensorFlow can be your ‚Äúgeneral-purpose‚Äù deep learning libraries, Caffe is made by keeping expression, speed, and modularity in mind. The framework, developed by a computer vision group, enables simple and flexible deep learning to organize computation. To its update, Caffe2 is also available.

- To install Caffe, visit here for Caffe and Caffe2

- Familiarize yourself with an introductory tutorial presentation

- Here you‚Äôll find its documentation

(d) Microsoft Cognitive Toolkit: Microsoft Cognitive Toolkit ‚Äî previously known as CNTK ‚Äî is a unified deep-learning toolkit that easily realizes and combines popular model types such as CNN, RNN, LTSM and more, across multiple GPUs and servers.

- To install Microsoft Cognitive Toolkit, visit here

- For tutorials, reach here

- Model Gallery collection of code samples, recipes, and tutorials for various use cases.

Please note that the above-listed architectures are not the only popular libraries in use today. We have listed some more with their key features:

- Written in Python; a minimalist and highly modular neural networks library

- Capable of running on top of either Theano or TensorFlow

- Enables fast experimentation.

- A scientific computing framework

- Offers wide support for machine learning algorithms

- Based on the Lua programming language

- Python supported, a flexible and intuitive neural networks library

- designed on the principle of define-by-run

- let you modify networks during runtime

To learn more on criteria-based selection and a detailed review on other frameworks, visit the page- How to Select a Deep Learning Framework (we suggest you bookmark the link as it is updated very often).

Step 5: Exploring Deep Learning

Deep learning is a complex, yet prominent field of artificial intelligence where the real magic is happening right now. The three key points which steer the field of deep learning are:

The availability of huge amounts of training data Powerful computational infrastructure Advances in academia

However, what you need to do to pioneer deep learning is simple:

Repeat from step 2 ‚Äî step 4 with a different adventure every time

Keep testing your deep learning skills (e.g. Kaggle)

Join Deep Learning communities and ask questions (e.g. Google Group, DL Subreddit)

Follow recent researches/ researchers (e.g. Most Cited Deep Learning Papers)

EndNote

Today, researchers, who were also learners just like us a few years back, are working hard to defy impossibility in the field of technology. In the beginning, you might find difficulty in learning concepts but, tenacity is the key.

You may find yourself baffling with deep learning algorithms and think why it didn‚Äôt work as you expected or why am I getting this error ABC?‚Ä¶believe me, that‚Äôs normal. If required, try out a sample algorithm you trust would work on a small set of data first.

In this field of learning, give everything a try that makes sense to you. While you gain new skills, try to build something different using your mind. Remember the dialogue from the movie Spiderman ‚Äî ‚ÄúWith great power, comes great responsibility.‚Äù The trend for deep learning is rising non-stop. To make a dent in the Deep Learning hall of fame, the universe is open to you. Come out and showcase your talent as many things are still unexplored.

At last, I request you to support (by Clapping) this piece and SHARE it so that we do not leave any aspiring talent behind and miss any opportunity on inventions to come!! Love you all‚Ä¶
How to setup the Python and Spark environment for development, with good software engineering practices Bogdan Cojocar ¬∑ Follow Published in Towards Data Science ¬∑ 6 min read ¬∑ Mar 17, 2019 -- 4 Share

In this article we will discuss about how to set up our development environment in order to create good quality python code and how to automate some of the tedious tasks to speed up deployments.

We will go over the following steps:

setup our dependencies in a isolated virtual environment with pipenv

how to setup a project structure for multiple jobs

how to run a pyspark job

how to use a Makefile to automate development steps

to automate development steps how to test the quality of our code using flake8

how to run unit tests for PySpark apps using pytest-spark

running a test coverage, to see if we have created enough unit tests using pytest-cov

Step 1: setup a virtual environment

A virtual environment helps us to isolate the dependencies for a specific application from the overall dependencies of the system. This is great because we will not get into dependencies issues with the existing libraries, and it‚Äôs easier to install or uninstall them on a separate system, say a docker container or a server. For this task we will use pipenv.

To install it on a mac os system for example run:

brew install pipenv

To declare our dependencies (libraries) for the app we need to create a Pipfile in the route path of our project:



url = '

verify_ssl = true

name = 'pypi' [[source]]url = ' https://pypi.python.org/simple' verify_ssl = truename = 'pypi' [requires]

python_version = "3.6" [packages]

flake8 = "*"

pytest-spark = ">=0.4.4"

pyspark = ">=2.4.0"

pytest-cov = "*"

There are three components here. In the [[source]] tag we declare the url from where all the packages are downloaded, in [requires] we define the python version, and finally in [packages] the dependencies that we need. We can bound a dependency to a certain version, or just take the latest one using the ‚Äú*‚Äùsymbol.

To create the virtual environment and to activate it, we need to run two commands in the terminal:
If you‚Äôre anything like me, you subscribe to newsletters. Lots of newsletters. Like, too many newsletters. I can‚Äôt help myself. When done right, newsletters are an excellent form of content curation that can help deliver interesting and insightful information to you with minimal effort on your part. Lucky for us, data newsletters are no exception to this rule.

In this post, I‚Äôll outline why I think newsletters are such an awesome tool for continued growth in data science. More importantly, I‚Äôll lay out the data science and AI-focused newsletters that I look forward to most throughout the week.

It‚Äôs also worth noting that this post has been remastered from a previous post. More concretely, I‚Äôve added more newsletters, elaborated on each one with a small blurb, and cleaned up some structural things.

Why Newsletters?

Looking back on when I started out in data science, one of the first and most valuable things that I did was seek out tons of data newsletters. This forced me to stay on top of the latest news in the ever-changing realm of data science.

It helped me learn techniques and technologies, introduced me to new concepts and learning resources that I wouldn‚Äôt have known about otherwise and notified me when it came to networking opportunities, public tech talks, and job openings. Most importantly, it did this on a weekly basis.

Consistently delivered content curation is invaluable in our current world of clickbait titles and fake news. The amount of information out there is overwhelming. Newsletters help solve this problem by differentiating between signal and noise for you. Why take the time to weed through thousands of posts and find the best ones when others will gladly do it for you?

The following collection of newsletters is a perfect example of this. Ranging from data science to machine learning to artificial intelligence, the links delivered by these content curators can serve as a game-changer when it comes to your continued growth as a data scientist.
A deduplication process is always important for companies with a huge amount of data. From one thing, deduplication minimizes the amount of space required to store business data and will bring a lower infrastructure cost and better performance for our pipelines. From another thing, reducing the number of duplicates will reduce the pipelines complexity and will increase the time to business via continuing integration and continuous delivery (CI/CD).

Sometimes a deduplication process consists of a simple text to text matching and you can simply choose either a CRC32-Checksum or an MD5 matching. However, there are some situations where the dataset rows differ only for some small text discrepancies on some of the columns, even though they represent the same entity. Thus, this article shows an entities recognition and linking process using two different spark approaches over a specific dataset of products collected by scrapping e-commerce websites will be used.

The entire code and process describe following could be found here:

The general process could be found on this trait (‚Ä¶ Yes, I use scala for data science !!!):

package com.sample.utils



import org.apache.spark.sql.DataFrame



trait OperationsHelper {

def ds: DataFrame



def preparedDataSet()(df: DataFrame): DataFrame



def deduplicateDataSet()(df: DataFrame): DataFrame



def resultsDataFrame()(df: DataFrame): DataFrame

}

As you will see the idea behind this helper will be having a functional pipe from where easily chain dataframe transforms could be called.

Preprocessing Products Data

Techniques to reduce dimensionality are widely used by the data science community to get a smaller set of features to analyze and obtain better performance while training and evaluating models. The PCA method allows dimensionality reduction while keeping those features that describe a large amount of the information. Consequently, this pre-processing stage follows these steps:

Data Cleaning: Cleaning the data to have a common scale. For the case of the products, dataset consists of a simple text cleaning for cases, white spaces, encoding, and symbols.

Cleaning the data to have a common scale. For the case of the products, dataset consists of a simple text cleaning for cases, white spaces, encoding, and symbols. Features Selection: Using the PCA technic a set of features are selected.(‚ÄútitleChunk‚Äù, ‚ÄúcontentChunk‚Äù, ‚Äúcolor‚Äù, ‚Äú productType‚Äù)

The content found on the features above contains most of the discrepancies for candidate duplicate products.

1 ‚Äî Approach A: Locality-sensitive hashing (LSH)

Locality-sensitive hashing is a technic used for entity resolution, then records that represent the same entity will be found. The spark MLlib has a custom LSH implementation used here to find duplicates as follow:

First, hashes are generated using a concatenation of selected features (PC above). For a real-world example hashes for each feature could be generated. However, for this example and in order to get results faster a simple concatenated column is used.

Then, this column is used for generating LSH vectors as follow:

‚Äî A tokenizer generates the list of words for a record using a words stopper.

‚Äî A CountVectorizerModel creates the vectors with hashes and buckets (similar hashes) for the LSH algorithm.

val pccTokenizer = new Tokenizer()

.setInputCol(OperationsHelperLSH.ConcatComments)

.setOutputCol(OperationsHelperLSH.ColumnWordsArray)

val wordsArrayDF = pccTokenizer.transform(df)



val remover = new StopWordsRemover()

.setCaseSensitive(false)

.setStopWords(OperationsHelperLSH.stopWords)

.setInputCol(OperationsHelperLSH.ColumnWordsArray)

.setOutputCol(OperationsHelperLSH.ColumnFilteredWordsArray)



val wordsFiltered = remover.transform(wordsArrayDF)



val validateEmptyVector = udf({ v: Vector => v.numNonzeros > 0 }, DataTypes.BooleanType) val vectorModeler: CountVectorizerModel = new CountVectorizer()

.setInputCol(OperationsHelperLSH.ColumnFilteredWordsArray)

.setOutputCol(OperationsHelperLSH.ColumnFeaturesArray)

.setVocabSize(VocabularySHLSize)

.setMinDF(10)

.fit(wordsFiltered)



val vectorizedProductsDF = vectorModeler.transform(wordsFiltered)

.filter(validateEmptyVector(col(OperationsHelperLSH.ColumnFeaturesArray)))

.select(col(OperationsHelperWindowStrategy.ConcatComments),

col(OperationsHelperLSH.ColumnUniqueId),

col(OperationsHelperLSH.ColumnFilteredWordsArray),

col(OperationsHelperLSH.ColumnFeaturesArray))



(vectorizedProductsDF, vectorModeler)

Class: com.sample.products.OperationsHelperLSH.scala

In order to finish the training step, a MinHashLSHModel is used to train the products data, generating the final buckets of similar products.

Finally, using KNN similar hashes for a category could be found.

/**

* Uses the dataset to train the model.

*

*/

def deduplicateDataSet(df: DataFrame): (DataFrame, MinHashLSHModel) = {



val minLshConfig = new MinHashLSH().setNumHashTables(hashesNumber)

.setInputCol(OperationsHelperLSH.ColumnFeaturesArray)

.setOutputCol(OperationsHelperLSH.hashValuesColumn)



val lshModel = minLshConfig.fit(df)



(lshModel.transform(df), lshModel)

}





/**

* Applies KNN to find similar records.

*

*/

def filterResults(df: DataFrame,

vectorModeler: CountVectorizerModel,

lshModel: MinHashLSHModel,

categoryQuery: (String, String)

): DataFrame = {

val key = Vectors.sparse(VocabularySHLSize,

Seq((vectorModeler.vocabulary.indexOf(categoryQuery._1), 1.0),

(vectorModeler.vocabulary.indexOf(categoryQuery._2), 1.0)))



lshModel.approxNearestNeighbors(df, key, nearNeighboursNumber).toDF() }



To run an example: Go to the test com.sample.processor.products.ProcessorProductsLshTest and you will see a complete flow running.

Input Params:

category ‚Üí color = ‚Äònegro‚Äô and productType = ‚Äòtdi‚Äô.

nearNeighboursNumber ‚Üí 4

hashesNumber ‚Üí 3 (More hashes more precision but more computing cost).

3 products with almost the same text for selected features.

Results Analysis:

Pros:

Accurate: If a complete set of fields (representing the string) is used, the correct value for hashes and neighbors could detect almost all the repeated values.

Faster: compared with other ML strategies as Term-frequency-inverse, etc.

Cons :

A cluster with good resources is needed.

Needs a process for data cleaning.

2 ‚Äî Approach B: Fuzzy Matching with Levenshtein + Spark Windows:

Levenshtein is an algorithm used for strings fuzzy matching. Basically, this method measures the difference between two strings. Furthermore, the spark windows functions allow dataset analytics function in a concise way, avoiding multiple groupBy and Join operations. Thus, this method defines a 2-level window to group similar data and then applies Levenshtein to values in the same windows to discover duplicates. The process is described here:

First of all, a set of records described as non-fuzzy is selected. The list contains columns that represent categories and are were free of errors most of the times in the PCA process: (‚ÄúproductType‚Äù, ‚Äúcity‚Äù, ‚Äúcountry‚Äù, ‚Äúregion‚Äù, ‚Äúyear‚Äù).

This window represents the general windows hash for the analysis.

Secondly, a second window to discover quite similar records is applied. This list represents records that are neither part from the fuzzy list (PCA) nor from the non-fuzzy list: (‚Äúdoors‚Äù, ‚Äúfuel‚Äù, ‚Äúmake‚Äù, ‚Äúmileage‚Äù, ‚Äúmodel‚Äù, ‚Äúcolor‚Äù, ‚Äúprice‚Äù)

Note: the ‚Äúdate‚Äù field helps to order and get only the most recent.

Then, For each group applies levenshtein (string difference only in the second window) over the concatenated most fuzzy fields from PCA results: (‚ÄútitleChunk‚Äù, ‚ÄúcontentChunk‚Äù).

As you can see an MD5 representation of the columns is used instead of each String to have better performance:

keyhash: MD5 for the category column set. The picture below shows many products within the same category.

hashDiff: MD5 hash that represents the non-fuzzy set. The picture below shows products within the same category but with a different description (> levenshteinThreshold) and also those with a small levenshtein (< levenshteinThreshold) difference having the same hashDiff.

Finally, the values with the same hashes (both) and rank only change the row_num. Filtering row_num == 1 is possible to get the deduplicate Data set.

/**

* Applies windows functions and Levenshtein to group similar categories.

*

*/

override def deduplicateDataSet()(df: DataFrame): DataFrame = {

df

.withColumn(OperationsHelperWindowStrategy.ColumnRank, dense_rank().over(windowProductKeyHash))

.withColumn(OperationsHelperWindowStrategy.ColumnHashWithDiff,

concat(col(OperationsHelperWindowStrategy.ColumnCategoryFieldsHash),

when(levenshtein(

first(OperationsHelperWindowStrategy.ConcatComments).over(windowProductsCategoryRank),

col(OperationsHelperWindowStrategy.ConcatComments)) >= levenshteinThreshold, lit("1"))

.otherwise(lit(""))))

.withColumn(OperationsHelperWindowStrategy.ColumnRowNum, row_number().over(windowProductsCategoryRank))

}

Class: com.sample.products.OperationsHelperWindowStrategy.scala

To run an example: Go to the test com.sample.processor.products.ProcessorProductsWindowsTest and you will see a complete flow running.

Input Params: levenshteinThreshold ‚Üí 6

Results:

2 groups example with almost exact values.

The results are deduplicated after filtering rn == 1. This removes > 1/3 of the data in the sample dataset.

Results Analysis:

Pros:

More control in the spark partitioner and functions.

Cons :

Could have much more false positives.

Final Conclusion

A deduplication process depends always on the company needs and the amount of data to analyze. This article describes two different strategies. As a result, Levenshtein with windows functions is good enough for small dimensionality problems; otherwise, LSH is always the best option
I‚Äôm a Manager of Data Science & Machine Learning, specialising in design & building of solutions powered by DS & ML for enterprises for their analytics requirements. I have more than 5 years of extensive experience now in the field and like many, I did not begin my career in Data Science & Machine Learning. I was a Business Intelligence professional with close to 10 years of experience when I decided to take the plunge into Data Science. I‚Äôd like to share my experience of transitioning into the field in the hope that it might help a few who‚Äôre are looking to transition into the field.

It‚Äôs definitely not this hard

I‚Äôd imagine it to be something like this

For those of you who are not familiar with Business Intelligence(BI), it is a branch of technology that enabled the organization to gather all the historical enterprise data that is available to them and start using them to make business decisions both tactical & strategic. As the field of data science & machine learning was just beginning to take off in the early part of the decade (Thanks to the article ‚ÄúData Scientist: The Sexiest Job of the 21st Century‚Äù https://hbr.org/2012/10/data-scientist-the-sexiest-job-of-the-21st-century that got published in HBR) I felt a strong urge to ride the wave. But later felt that this is a transition that I had to make sooner or later because it was, in my opinion, a logical extension to the field of BI. While the field of Data Science & Machine Learning was about giving business the edge by providing actionable insights and intelligence about the future (some may argue that it‚Äôs also about the present), the field of BI did the same, but using data from the past using lesser sophisticated techniques through dashboarding and visualization techniques.

Or Maybe, little harder

That said, here a few pointers that I‚Äôd like to share with all the wannabe Data Science/Machine Learning professionals

Almost every Data Scientist is a Citizen Data Scientist

Given the sudden surge in the demand for data scientists & machine learning professionals when the industry does not have enough talent, most of the demand is being met with people who transitioned to ML & DS from fields as diverse as Bach of Arts, Psychology etc. So don‚Äôt ever feel that you are late in the game. In fact, if you are from the field of BI, you can leverage a lot of your existing data skills in DS/ML (Pandas Dataframe operations could very well be done on a relational table using plain SQL).

Apparently even during the age of Darwin, many scientists were ordinary people who took to science out of curiosity & interest and were hence called citizen scientists. Hence, you could be a citizen data scientist too if you have the interest and drive in you.

2. Have a structured learning path

MOOCs on DS & ML are plenty these days. I chose to certify in a couple of courses.

Its okay to be slow, but be steady

Machine Learning Specialisation from University of Washington (Coursera) https://www.coursera.org/specializations/machine-learning

This is 4-course specialization by Carlos Guestrin and Emily Fox who founded Turi that later got acquired by Amazon, covering all important areas of machine learning ‚Äî Classification, Regression & Clustering using python which is one of the reasons I chose this program

2. Deeplearning.ai(Coursera) https://www.deeplearning.ai/deep-learning-specialization/

This is a 5-part specialization in Deep learning offered by Andew Ng himself explaining the concepts of popular topics ‚Äî Deep Neural Networks, CNN, Sequence Models et al

3. Udacity Nanodegree in Deep Learning Specialization https://in.udacity.com/course/deep-learning-nanodegree--nd101

This is a 4-month specialization in Deep Learning concepts using Pytorch which is one of the emerging and widely used DL frameworks in the deep learning research community today. I chose this primarily because I wanted to stick to a framework with Python flavour for easier adaptability.

Btw, this is not an exhaustive account of courses that I tried and there are many more courses that I‚Äôve done partially but not naming them here. I‚Äôll cover all my course experience in an exclusive story later.

3. Get ready to be hands-on

Be ready to code, regardless of your title

While these courses help you to get a grounding on the concepts with some hands-on projects, it's important that one practices these concepts in order to build expertise. One of the activities that helped me was to participate in competitions in the popular competition sites such as Kaggle, Analytics Vidhya to keep in touch with your learning. For eg. I took part in the competition ‚ÄúLoan Prediction Challenge III‚Äù from Analytics Vidhya and secured 31st rank in the overall leader board. https://datahack.analyticsvidhya.com/contest/practice-problem-loan-prediction-iii/lb

Even though this is a playground competition, it helps you to build confidence in the skills that one has learnt.

4. Open other channels of Learning

Courses are one of the sources of learning, but it cannot be the only source of learning. It's important to tap into other sources of learning as well in order to develop a wide and broad perspective of topics. Here are other learning activities that I undertake.

a) Other online learning content ‚Äî Subscribed to ‚ÄúPractical Deep Learning with PyTorch‚Äù by an NUS researcher on Udemy. https://www.udemy.com/practical-deep-learning-with-pytorch/learn/v4/content‚Äù

I liked the way he explains the concept of a convolution filter as a filter that looks for specific shapes/patterns in an image. This gives a different perspective on the concepts learnt in the courses mentioned above.

b) Follow the works of topmost influencers in the field ‚Äî I follow Jeremy Howard@jeremyphoward, Rachel Thomas@math_rachel ‚Äè(co-founders of fast.ai an advanced pytorch based framework that makes Deeplearning coding a lot easier), Andrej KarpathyAndrej Karpathy,Andrew Ng@AndrewYNg ‚Äè, Jason Brownlee@TeachTheMachine ‚Äè,Soumith Chintala@soumithchintala ‚Äè

You may follow them on twitter or read their blogs to keep a tab on their works from time to time. Since I chose python based frameworks for my learning, almost all of the above use Pytorch as a framework or a python based libraries for ML & DS. So my list could be biased that way, but essentially the point is that you‚Äôll need to follow those people who are the biggest influencers in the field to be in tune with what they are doing.

c) Read blogs ‚Äî Follow blogs of top technology companies that do path-breaking work in ML & DS. I choose the following companies blogs closely through Feedly.

Facebook AI Research (FAIR), fast.ai, Google AI blog, AWS ML Blog, Open AI blog. This is not an exhaustive list by any account, but I‚Äôm just calling the top ones

5. Don‚Äôt wait to land the ‚Äúperfect‚Äù ML/DL project

Once the learning is done or even when the learning is on, try to implement the learnings in your current project by proposing ML, DL concepts in your solution design. From my experience working with clients from Fortune 500, most clients don‚Äôt specifically ask for an ML/DL project. Their priorities are always meeting the business objective and the design & approach of the solution is entirely left to us. So, I advise you to look for opportunities where you can put some of the things you learnt to practical use. For instance, I took up implementing text classification tasks using ML and simple sentiment classifier using DL in my projects.

6. Take up pet projects

In addition to projects for clients, taking up pet projects is another great learning opportunity for you and also the organization. This is especially true in the case of DL as most of the projects are in the realm of unstructured data in the form of text & images. So working on some pet project could not only help your knowledge grow but also gets your organization to take notice of the skills that you bring to the table and for your team to get hooked onto these concepts.

For instance, I‚Äôd taken up the task of building a prototype to automatically detect license plates of cars using object detection technique. More on this later, but below is a snapshot of tool that my team & I built.
Due to keen interest in learning new topics, I decided to work on a project where a Latent Dirichlet Allocation (LDA) model can recommend Wikipedia articles based on a search phrase.

This article explains my approach towards building the project in Python. Check out the project on GitHub below.

Structure

Photo by Ricardo Cruz on Unsplash

I developed the complete project in Python using classes and did not use Jupyter notebooks like I usually do to understand about classes and how to develop general Python projects. The modules, WikipediaCrawler, Cleaner and Content are defined as classes inside the Modules folder. config file includes the configurations. collectData, generateLDA and evaluator are used to develop and run the model.

Modules

|- __init__.py

|- WikipediaCrawler.py

|- Cleaner.py

|- Content.py config.yml

collectData.py

generateDLA.py

evaluator.py sample_images

|- recommendations.png .gitignore

LICENSE

Pipfile.lock

README.md

requirements.txt

When you try to run the project, you can use either Pipfile.lock or requirements.txt to install all dependencies.

Configuration

Photo by Tim Mossholder on Unsplash

It‚Äôs always a good practice to include any configurations for your project in a common file. While there isn‚Äôt much information in this project, I did define the paths for storing the database, LDA Model, dictionary and corpus inside config.yml file. I decided to keep all of these inside the data folder.

The configuration file is based on YAML which is a commonly used data serialisation method in the industry to store human readable configurations. The pyyaml package is required to read YAML files in Python.

Modules

Photo by Louis Reed on Unsplash

I developed and designed three modules (as classes) to be used for scraping data from Wikipedia, and working with the data.

Wikipedia Crawler

The class WikipediaCrawler let‚Äôs us crawl Wikipedia articles based on a certain category. On initialisation of this class, it creates a sqlite3 connect and then adds a table wikiData that stores the page id , category , url and content . The collect_data method uses the wptools package to extract the pages and store them in the table. wptools is a Python package that allows us to scrape Wikipedia articles based on a given category.

I‚Äôve added two additional methods, get_ids to fetch all page ids and get_urls to fetch all urls, if needed.

Cleaner

This module takes in document text and pre-processes that text. I just need to use the function clean_text as it calls all the other functions on our behalf and returns the final result. It does the following:

Removes unnecessary new line characters

Removes punctuation Removes numbers Removes stopwords (words that are too common and do not qualify for being good keywords for search) Applies lemmatization (converts each word to its lemma word like ran, running are converted to run)

Content

This module connects to sqlite3 database and helps us iterate over the pages and clean their content using Cleaner module. I added other methods to get the page and url by id.

Application

Photo by Jason Leung on Unsplash

Once I had the modules set up, I began scraping for data, training the LDA model and recommending articles.

Collect Data

First, I run the file collectData.py which expects two arguments to begin extracting data from Wikipedia and storing it in the database.

category: The category for which we want to develop the article recommender system depth: To what depth do we want to extract the webpages for a given category. For example, when browsing through an article when beginning with depth 2, it‚Äôll go one step deeper (i.e. its related articles) with depth 1 but will end at the next depth as it will be 0.

It creates the directory data if it does not exist. Using WikipediaCrawler , it extracts the pages and stores them to wikiData.db to be used by other files. On completion, it outputs the message: The database has been generated

Generate LDA

The next step is to use the database we created, build a LDA model from it and store it in the data folder.

First, I read the database and create a dictionary. I remove all words that appear in less than 5 documents and that appear in more than 80% documents. I tried multiple values and concluded on these numbers by hit and trial. Then, using doc2bow , I create the bag of words which act as the list of keywords. Finally, I generated the LDA Model and saved the model, dictionary and corpus.

Evaluator

Finally, everything is ready. We invoke the evaluator.py and pass in a query string based on which we identify the keywords and list the top 10 articles that match the search criteria.

I read the query and identify the keywords from it. Then, by invoking the get_similarity method, I calculated the similarity matrix and sort them in the decreasing order so the maximum similarity documents are at the top.

Next, I iterate over these results and present the top 10 urls which represent the recommended articles.

Real Example

Use Case

I created the database with depth 2 and category Machine Learning. It generated the file, wikiData.db . Next, using the generateLDA.py , I created the LDA model.

Usage

I used the search query as Machine learning applications and was recommended the articles as can be seen in the image below:

Recommended articles for ‚ÄòMachine learning applications‚Äô

Conclusion

In this article, I went about how I developed a LDA model to recommend articles to users based on a search query. I worked with Python classes and devised a complete application.
Handwriting is a one of the challenge in NLP task. It is because it can be various among different people. On the other hand, some characters (e.g. English) are quite similar. As a human beginning, we leverage contextualize information, lexicon matching.

Sometimes, ‚ÄúO‚Äù can be written as ‚Äú0‚Äù while human begin has the capability to distinguish whether it is ‚ÄúO‚Äù or ‚Äú0‚Äù from contextualize information. For example, ‚Äú0‚Äù will be used in phone number while ‚ÄúO‚Äù will be used as part of English word. Another skill is lexicon searching. It helps to guess words even though we cannot recognize every single characters.

Handwriting in different language (Carbune et al., 2019)

How can we tackle in via deep learning era? This story will discuss about Fast Multi-language LSTM-based Online Handwriting Recognition (Carbune et al., 2019) and the following are will be covered:

Data

Architecture

Experiment

Data

Carbune et al. leverage both open and close dataset to validate the model. As usual, IAM-OnDB dataset is used to train a model. During the experiment, they use two representations which are Raw Touch Points and B√©zier Curves .

Raw Touch Points

Data will be convert to 5-dimensional points which are x coordinate, y coordinate, timestamp of the touchpoint since the first touch, pen-up or pen-down and new stroke or not. Some preprocessing a necessary

Since the size of image can be various, normalization of x and y coordinates are necessary. Surrogate are 20% larger than observed touch points if writing area is unknown

Equidistant linear resampling along the strokes with value of 0.05. In other word, a line of length 1 will have 20 points.

B√©zier Curves
Member-only story 7 Machine Learning lessons that stuck with me this year

I‚Äôve been a student of Machine Learning for the past two years, but this past year was when I finally got to apply what I learned and solidify my understanding of it. So I decided to share 7 lessons I learned during my ‚Äúfirst‚Äù year of Machine Learning and hopefully make this article an annual tradition.

1. Data is Queen

Think about the data before you get started

Nowadays, it is relatively easy to learn about Machine Learning thanks to the vast selection of learning resources that exist online. Unfortunately, many of them tend to gloss over the data collection and cleaning steps.

During my first serious Machine learning project, my team and I run into the BIG question of where do we get our data from? We needed thousands of images of animals and after giving up on the idea of scraping the web, we came across the CIFAR-100 dataset. But just when we thought all of our problems where solved, we realized that we needed to pre-process our data (remove background, scale down, turn into vectors etc.) before feeding it into our Deep Learning model. This turned out to be the most time consuming part of our project (aside from training and tweaking our model over night for about a week).

The data part of any Machine Learning project shouldn‚Äôt be overlooked, ‚Äúwhere is the data coming from?‚Äù should be one of the first questions you ask before you get started.

2. Domain Knowledge Matters

Domain knowledge can sometimes matter just as much as technical skills

It is easy to get caught up on the idea that you only need technical skills to solve problems using Machine Learning. The reality is that you‚Äôll have a hard time getting very far if you only think of the problem in front of you in terms of just numbers and algorithms. For once, it can be difficult to come up with project ideas in a domain that you don‚Äôt know much about. But even if you manage to come up with a great idea, domain knowledge can be extremely helpful when deciding what kind of data you need to collect and the type of features you should use when‚Ä¶
The core competency of a data analyst is ‚ÄúSpeed to Insight‚Äù.

A data team often consists of many people, with many skills, using potentially overlapping techniques. This focus on speed distinguishes this role from data scientists or statisticians.

Today I‚Äôm focused on answering questions about the business or about how users behave. I‚Äôll refer to these types of questions as mostly in the realm of data analysts, though some organizations call these folks data scientists, too.

A good data analyst should be able to interface directly with folks in the business unit that they‚Äôre working with. They need to have a solid understanding of business fundamentals in addition to data chops. A junior analyst may rely on business people asking smart questions, and answering the questions that they‚Äôre asked, quickly. While this is clearly helpful, it‚Äôs not the highest-leverage opportunity for an analyst.

The best analysts don‚Äôt only answer the questions that they‚Äôre asked. Actually doing analysis is often the easy part. It‚Äôs other skills that separate an average analyst from the best.

Help teammates rephrase their question.

Ask the ‚Äúnext 3 questions‚Äù that they should know

When you‚Äôre asked a particular question, it can be tempting to think ‚Äúsure, I can answer that‚Äù. While that might be the first step, it‚Äôs important to get at the root reason for the question. If someone asks for the signup conversion rate across a section of your website, it‚Äôs the analyst‚Äôs job to dig in.

Why are you wondering about signup conversion? Would we rather measure conversion to active users? Are you interested in a particular segment? Does the signup rate vary across paid, direct, organic, and social traffic?

It‚Äôs unusual that a PM wants a metric for the sake of a metric. They‚Äôre really trying to learn something about the nature of your product or your audience. It‚Äôs your job to know enough about your data sources and about the business itself to answer the next three questions that they didn‚Äôt ask. Short circuiting the back and forth will help your team move‚Ä¶
Photo by AC De Leon on Unsplash

Professional eSports are becoming increasingly popular, and the industry is growing rapidly. Many of these professional game leagues are based on games that have two teams that battle it out. Call of Duty, League of Legends, and Overwatch are all examples. Although these are comparable to traditional team sports, the novelty of eSports means that less attention has been paid to predicting the outcomes of this burgeoning industry.

How do you model adversarial team games? I was first inspired to start this project by reading Christopher Bishop‚Äôs new machine learning eBook, which has a chapter devoted to calculating player rankings in games using the Elo rating system. For those of you familiar with Chess, you may have encountered this metric before. I found a very instructive blog post on The Data Skeptic for this exact topic. All equations posted below were borrowed from the aforementioned Skeptic.

In essence, Elo is a framework where the strength of a player is measured by a single number (r_1 = 2000). The predicted strength of any given team is represented by a Logistic Function (R), and the outcome of a given match as a ratio of the strength of a single team over the sum of their rating (E).

Logistic Function for player i.

Expected Outcome for player i

If a player of strength 2000 is to face off against a player with an Elo rating of 1000, we would predict a very likely win for the former player (E=0.9968).

Rating update for player i (K~32 in Chess)

At the end of each match, the players‚Äô scores are updated based on their relative strengths. If the favored player wins (S_i = 1.0), they gain a small positive adjustment in their score. If the underdog wins the match, they can gain a larger amount, as the result was unexpected. In the given example of the underdog winning, their score would change from 1000 to 1031. The favored player‚Äôs score would also decrease.

What does this have to do with Overwatch, the immensely popular team-based shooter? I wondered how someone could model the incredible complexity of a team-based sport that is so dynamic without resorting to incredibly convoluted architectures? I saw an example code on the PyMC3 website about encoding professional rugby teams using Hierarchical Normal Distributions. Having just done a blog post on Hierarchical models, I thought I could try something similar!

Essentially, we are treating each team as a single entity. This is in spite of the fact that teams consist of multiple players, each working autonomously. We will justify this choice in two ways: 1) It is easier to model a team as a single rating, rather than each individual player, 2) the teammates are working together (ideally), so they can be modeled as a collective unit. The Elo rating is also incredibly helpful if two teams have not played against one another before. Without any historical matchups, we can leverage team matches that the two adversaries had in common to make predictions.

Data were scraped from the 2018 season of The Overwatch League. Unfortunately, there are not that many matches from which we can train our model. As the 2019 season is currently ongoing, any interested reader could begin to add this data to the model and see how well it performs! The raw data format had to be cleaned and processed prior to training our model.

Win/Loss aggregations for the regular season

There are two basic ways we can represent our data, which will change how we construct the model. The simplest approach is to make this a classification challenge. Each Overwatch matchup involves multiple rounds, and the winner of the match secures the most winning rounds. For instance, San Francisco Shock beats Florida Mayhem 3‚Äì2 . We can represent this as a total of 5 records in the data with varying levels of success and failure (0/1).

Classification or Bernoulli trial for this simple example

We can construct our data split as follows. For each matchup in the regular season, we model each win and loss as a binary target. Each team‚Äôs Elo rating will be inferred based on the equations listed above, which transform a rating into a Bernoulli trial. Then, we will use our learned ratings to predict all of the matches in the playoffs. Our PyMC3 model will then consist of a Binomial: for a given matchup, do we predict a success or a failure?

A sample trace of our simple model.

We can also represent our matchups as fractions. For the same example of a 3‚Äì2 match, with San Francisco Shock winning, we can encode this as a 0.6 win. Now we have slightly more flexibility, as we can model this as a regression task, or still keep it as a classification. This result offers the model more nuance as well. A 0.8 win is a much more favorable outcome than a 0.6 versus the same opponent.

The ROC suggests the model is decent at predicting matchups

Regardless of your choice, the training error seems to be pretty stable. Our model is able to predict which teams will win based on their Elo distributions. What will happen when we apply this model to the Playoffs?

We are making a few assumptions during our playoffs evaluation:

Teams at the end of the regular season are fixed in skill, players, strategy, etc. That the playoffs are substantive enough that less probable events cannot dominate the results (small number of samples) That our regular season matchups were fairly distributed among all teams (round-robin)

These assumptions will definitely rear their ugly head. There are only 12 playoff matches in 2018, and eliminations come quickly. This means that if a team had a fairly poor match that is un-characteristic of their true skill, they may not have a chance to redeem themselves. Teams also can gain in strength when a true challenge appears, as is the nature of playoffs.

When we apply our model to the playoff data, we get very poor performance. Our model might as well be randomly guessing! What happened here?

Playoff Record and the predictions

In the simplest terms, we had a few teams that upset the apple cart. Philly Fusion (Team 9) beat both Boston and New York (winning 4/5 matches) , despite having a lower Elo rating. Fusion then lost to London Spitfire, who ended up winning the tournament, despite being lower ranked than LA Valiant. Go figure! This is why people love to watch sports, as you never know who might win.

If you are itching to try a few different Bayesian methods to games like this, Allen Downey‚Äôs book Think Bayes has a section on modelling teams via Poisson processes and their historical scoring record. Recently Tuan posted an article on Towards Data Science on building a betting strategy for professional soccer leagues, which I think would also apply here.

Feel free to download the data and my notebooks via my Kaggle repo. I also have a Git repo with some selected notebooks. Leave a comment or a question below and I will do my best to answer it. Thanks for reading!
I am currently building a Flask app for teachers. Since Google Drive has been adopted by teachers, Google sheets are used by them also. One of my app‚Äôs features is to easily allow teachers to copy and paste the sheet link into the app and submit it through a form. It will then convert it into a pandas data frame and drop the null rows and columns.

Google Sheets Icon

It is a work in progress but I wanted to share the class made so far since I don‚Äôt find many Python class examples for data collection or preprocessing. Many of the examples I have seen are useful for teaching the concept of classes. But most of the time they are not practical enough or specific to data science or ML. After all, we could always use more practical examples of Python classes.

See, Yoda knows. (Self-made on imgflip.com)

But why would you need a class? Can‚Äôt you just write the code and be ok?

If this were for analysis purposes, then‚Ä¶yes. But production code, as far as I know, follows an object-oriented programming approach. This means that your code is split between scripts. These scripts are primarily composed of classes and functions. Code written in this manner is also easier to reuse for additional purposes. Take the ecdf function I used in this post as an example.

With this function, you can quickly reproduce this plot immediately as opposed to retyping the same matplotlib templating code over and over again.

Great! Show me your example!

Of course! I will share this example in pieces and explain each part as we go.

# Import necessary libraries

import gspread

import pandas as pd

from gspread_dataframe import get_as_dataframe

from oauth2client.service_account import ServiceAccountCredentials
I‚Äôm not a huge Reddit user but when I‚Äôm there I usually hangout in just a few subreddits (r/dataisbeautiful, r/geography, r/philadelphia, r/emo and a couple others). Other than the popular data viz subreddit, none of these generate the scores you see on the front page of Reddit.

I only say all that to point out that I am not tapped into all the nuances of Reddit culture. I did however have a question after perusing the front page of the internet one day ‚Äî is there a correlation between a popular post‚Äôs score and the amount of conversation that garners around it?

It looks like r/worldnews really struck up some conversations!

First things first, getting the data. I took a look at the Reddit homepage api but didn‚Äôt see a way to pull the top posts for a given date and time. So I turned to the Wayback Machine‚Äôs API which can use a specific date and time as an endpoint and will return the url of the closest web capture.

The front page is pretty well archived it appears.

Feeling pretty confident that I could scrape a good amount of data for 2018, I jumped over to R and generated a list of complete urls to call the API.



dateRange <- gsub(‚Äú-‚Äù,‚Äù‚Äù,seq(ymd(‚Äò20180101‚Äô),ymd(‚Äò20181231‚Äô), by = ‚Äò1 day‚Äô, truncated=2))

base_url <- " library(lubridate)dateRange <- gsub(‚Äú-‚Äù,‚Äù‚Äù,seq(ymd(‚Äò20180101‚Äô),ymd(‚Äò20181231‚Äô), by = ‚Äò1 day‚Äô, truncated=2))base_url <- " https://archive.org/wayback/available?url=reddit.com #create list of api urls

url_list <- c()

for (date in dateRange) {

full_url <- paste(base_url, "√ótamp=",date, "120000", sep="")

url_list <- c(url_list, full_url)

}

Now we can call the Wayback Machine in order to get a list of web captures.

#create list of archive links

archive_list <- c()

archive_times <- c() for (url in url_list) {

#get raw result from api call

raw.result <- GET(url = url)

raw.result$status_code

#get raw content from results

this.raw.content <- rawToChar(raw.result$content)

#put content into list

this.content <- fromJSON(this.raw.content)

#extract archive url from list and add to archive_list

archive_times <- c(archive_times, this.content$archived_snapshots$closest$timestamp)

archive_list <- c(archive_list, this.content$archived_snapshots$closest$url)

}

This gives us a list of 365 urls to captures from around noon of each day. Now onto the actual web scraping. There are probably quicker ways to do this, but I went with an old fashioned for loop and used the rvest package to scrape the score, number of comments, and r/subreddit of each of the page‚Äôs 25 posts.

I included some simple error handling by checking to make sure the length of the r/subreddit value is greater than 0 (i.e. any posts were actually pulled) before adding it to the datalist variable.

After the loop is complete, I use rbind to fill the data frame and filter out any problematic data.

#create empty list

datalist = list()

for (i in 1:length(archive_list)) {

#get all the html from the webpage

webpage <- read_html(archive_list[i])

#filter all the .things

things <- webpage %>%

html_node("#siteTable") %>%

html_nodes(".thing")

#get votes

score <- things %>%

html_node(".score") %>%

html_text()

#get number of comments

comments <- things %>%

html_node(".comments") %>%

html_text()

#remove " comments" and convert to number

comments <- as.numeric(gsub(" comments","", comments))

# get post subreddit

subreddit <- things %>%

html_node(".subreddit") %>%

html_text()

#get date of page

date <- gsub(" #loop through archive urlsfor (i in 1:length(archive_list)) {#get all the html from the webpagewebpage <- read_html(archive_list[i])#filter all the .thingsthings <- webpage %>%html_node("#siteTable") %>%html_nodes(".thing")#get votesscore <- things %>%html_node(".score") %>%html_text()#get number of commentscomments <- things %>%html_node(".comments") %>%html_text()#remove " comments" and convert to numbercomments <- as.numeric(gsub(" comments","", comments))# get post subredditsubreddit <- things %>%html_node(".subreddit") %>%html_text()#get date of pagedate <- gsub(" http://web.archive.org/web/|/https://www.reddit.com/ ", "", archive_list[i]) if (length(subreddit) > 0) {

print(paste(unique(date),length(subreddit),sep=" "))

#create temp df

temp <- data.frame(date = date, score = score, comments = comments, subreddit = subreddit)

#add it to the list

datalist[[i]] <- temp

}

} #make a df from the datalist

main_data = do.call(rbind, datalist)

#remove incomplete posts

reddit_posts <- main_data %>%

filter(score != "‚Ä¢",

!is.na(score),

!is.na(comments)

) %>%

mutate(score = as.numeric(sub("k", "e3", score, fixed = TRUE)),

subreddit = gsub(".*r/","r/",subreddit))

How did the scrape do? Not too bad. The scrape successfully pulled daily posts for 75% of the year. I didn‚Äôt investigate this too thoroughly since I had enough data to work with, but I think the Wayback Machine had some issues with the Reddit site redesign.

Now we have a freshly minted dataset, but in order to produce the visualization I want it needs some wrangling.

Identify the eight subreddits that sent the most posts to the front page Change the subreddit value of posts that came from non-top subs as ‚Äúother‚Äù Reclassify the subreddit factor levels so that they are in descending order with ‚Äúother‚Äù at the end.

#get top 8 subreddits

top_subs <- reddit_posts %>%

group_by(subreddit) %>%

summarise(count=n()) %>%

top_n(8, count) %>%

ungroup() #create vector of top_subs

top_subs <- as.character(top_subs$subreddit) #make notin operator

'%!in%' <- function(x,y)!('%in%'(x,y)) reddit_posts_reduc <- reddit_posts %>%

mutate(subreddit = case_when(

subreddit %!in% top_subs ~ 'other',

TRUE ~ as.character(.$subreddit)

)) #get list of factors in descending order

factor_order <- reddit_posts_reduc %>%

group_by(subreddit) %>%

summarise(count=n()) %>%

arrange(desc(count)) %>%

select(subreddit) #overwrite with list

factor_order <- as.vector(factor_order$subreddit)

#remove "other" from first position

factor_order <- factor_order[-1]

#create new factor level list

factor_order2 <- factor_order

#update new factor list with ordering info

for (i in 1:length(factor_order)) {

factor_order2[[i]] <- paste("#",i," ",factor_order[[i]], sep = "")

}

#append other to both factor lists

factor_order <- append(factor_order, "other")

factor_order2 <- append(factor_order2, "other") #update dataframe levels with update factor levels

reddit_posts_reduc$subreddit_f <- mapvalues(reddit_posts_reduc$subreddit, from = factor_order, to = factor_order2)

levels(reddit_posts_reduc$subreddit_f)

Now, time to plot. I plotted the number of comments on the x-axis and the score on the y-axis. I used axis limits to account for outliers, The end result was a plot of small multiples grouped by subreddit and labeled with its correlation coefficient.

#plot data

reddit_posts_reduc %>%

ggplot(aes(

x=score,

y=comments,

color=subreddit_f)

) +

geom_point(size=3, alpha=0.4) +

facet_wrap(~subreddit_f, ncol = 3) +

geom_smooth(se=F) +

theme_fivethirtyeight() +

theme(axis.title=element_text()) +

# labs(title = "Correlation between score and comments on front page",

# subtitle = "Posts from the front page of Reddit in 2018 plotted to show correlation between score and the number of comments. Posts are grouped by the eight subreddits that sent the most posts to the front page with all other posts grouped in other.",

# caption = "Data from Reddit via Archive.org

Chart by @jared_whalen"

# ) +

theme(legend.position="none") +

stat_cor(method = "pearson", label.x = 110000, label.y = 9000) +

scale_y_continuous(label=unit_format(unit = "K", scale = 1e-3, sep=""),

limits=c(0,10000)) +

scale_x_continuous(label=unit_format(unit = "K", scale = 1e-3, sep=""),

limits=c(0,150000)) +

xlab("Score") +

ylab("Number of comments")

Things I gained from this project

How to use the Wayback Machine‚Äôs API to scrape archived pages

A better understanding of reassigning factor levels in order to customize ordering when plotting

Here is a gist of the entire source code.
Cross the AI Commercial Divide to the Enterprise

Those startups that are doing well generally understand the nature of AI technology and the opportunity in the enterprise. But more than that AI startups that are starting to scale have all crossed the commercial divide from a technical world to the enterprise. They have learnt:

1. Don‚Äôt move too fast and break things; embrace Responsible AI

This culture popularised by Silicon Valley works well in a business to consumer (B2C) world where the consequences of a bug in an application are relatively limited. Software developers globally have embraced the lean startup and agile methodology. But this means live applications can often have bugs and break. This doesn‚Äôt really work well in the enterprise.

This is especially true in highly regulated enterprises such as financial services, agriculture or pharmaceuticals. Technology is driving greater more and more regulation. Europe introduced GDPR regulation in 2018 that provides rights to individuals as to the use of their personal data by companies. There will be significant fines ‚Äî up to 2% of a company‚Äôs revenue ‚Äî for the misuse of consumers‚Äô personal data. And in the financial industry the response of ‚Äúwhoops, I guess the app didn‚Äôt work‚Äù doesn‚Äôt work when you are dealing with real financial data and financial transactions. Revolut a new UK challenger bank that relies heavily on AI has found itself in hot waters with the regulators over alleged compliance lapses. And in a world where the consequences of automated decision can be life-changing, such as autonomous vehicle crashing or critical health care treatment diagnoses, you better make sure you are confident in the reliability and accuracy of your automated decision making.

Corporates are increasingly putting in place board governance and oversight to manage reputational risks to their firm from the use of AI. For example if the datasets used to train AI algorithms have sample biases then a company‚Äôs brand can suffer if they are seen as being discriminatory. We all saw the recent headlines where algorithms were shown to be much better at recognising the gender of white males compared to other ethnic groups. Or the recent case from Amazon where they abandoned their hiring recommendations systems as the machine learning simply mirrored the fact that hiring historically was heavily male, white and young.

Increasingly startups will need to offer enterprises some level of assurance around the risks of their AI offering. Does your AI startup technology provide explainability for fully automated decisions that have legal effect? For example if your technology can be used to automate hiring decisions then you will need to explain how the algorithms work under GDPR. You need to demonstrate that the algorithm is not biased against people based on protected classes such as gender, age, socio-demographics or health challenged (feel free to take a look at our introductory readings on the topic of explainable AI). The Information Commissioners Office in the UK recently released a discussion paper that identifies eight AI specific risk areas that enterprises are likely need to manage including (a) fairness and transparency in profiling which is especially concerned with bias and discrimination, (b) accuracy of the AI models, (c) the level of automated decision making be it fully automated or human in the loop, (d) security and cyber risks and (e) tradeoffs in accuracy versus privacy versus explainability.

Startups and corporates are going to need to be really well versed in this topic which is increasingly being referred to as Responsible AI practices. As a startup don‚Äôt move too fast and break too many things. It could get you in a lot trouble. And ensure you are embracing and demonstrating Responsible AI practices (a topic I will write more on later).

2. Solve really high value use cases, not nice to haves

In the past few years we have seen a tidal wave of consumer mobile applications that addresses any imaginable consumer need. Apps can be built in weeks, launched and consumer traffic bought by placing ads on Google and Facebook. But this approach doesn‚Äôt work in the enterprise. There is something of a zero sum game where there is little appetite from the chief information officer (CIO) and other executive leaders to embrace, yet, another technology solution. We all recognise work frustrations trying to use the latest application from HR or finance or sales or marketing. We struggle to remember our passwords. None of the new applications work the same. And then we often can‚Äôt remember where we stored that file be it in the project or personal cloud folder. Technology leaders are frustrated trying to integrate more technologies into their existing and often fraying legacy databases and technology platforms.

To get the attention of a CIO and a head of department, such as a chief marketing officer (CMO), your solution better be addressing a really important problem. The type of problem it should be addressing should be one where the manager is going to bed worrying about it and their bonus plan is clearly tied to it. ‚ÄúNice to haves‚Äù do not work in the enterprise. I was chatting recently with a CTO of an AI startup who has spent two years building an AI product suite to drive enterprise intelligence but the sales aren‚Äôt coming. Why? Not high enough value yet.

An example of a really high value use is from HireVue. They worked with Unilever to save over 50,000 hours in candidate interview time and delivered over ¬£1M annual savings and improved candidate diversity with machine analysis of video-based interviewing. That‚Äôs a lot of money. Reinfer, a British startup that uses advanced NLP algorithms to sift through billions of emails and messaging to determine what people are communicating about, recently completed a pilot with a major international bank. It identified major issues in post-trade operations by analysing mailboxes with the use of machine learning with the potential for millions in operational savings. These are really high value use cases.

3. Master B2B enterprise sales and learn calculated patience

Startups need to ensure they master enterprise sales. These skills are at a premium for AI suppliers as it takes time and hands-on experience to master consultative sales . The most valuable training course I took early in my career was the SPIN sales methodology ‚Äî S(ituation), P(roblem), I(mplication), N(eed). Selling requires time to identify stakeholders, get a meeting with those stakeholders, assess the current business situation, ask questions to identify critical problems, assess the implications of those problems across multiple departments, build a consensus as to the implications and the need. And even then there is no guarantee that your solution to this need will get budgetary prioritisation.

A typical corporate manager has a to-do list that lasts from here to kingdom come. For a startup solving a high value use case need is critical so don‚Äôt just base your company on a ‚Äúproduct hunch.‚Äù

And patience is at a premium when a sales cycle can last 12‚Äì18 months. Many AI startups run out of money or assign precious resources to sales opportunities that are not qualified. Make sure you qualify out opportunities quickly if you think they will ultimately not lead anywhere. Re:infer found the sales cycle was really really long but they stuck with it identifying a really high value use case, finding an internal champion, and completing a successful sales pilot. They also identified that enterprises‚Äô readiness to rollout AI across an organisation can really slow the sales process.

4. Translate AI for the real world

Startups need to be able to explain the value of their unique algorithm, technology, product or solution in a language that is readily understandable by a business audience is critical. They need to be bi-lingual. With many AI startups being founded by young, wicked smart and highly technical minds there is often a communications gap with the ‚Äúsuits.‚Äù We see executives‚Äô eyes glaze over all the time as the minutiae of this cutting-edge technology is explained. And if little context is provided as to how this technology can help drive a business forward ‚Äî by impacting revenues, efficiency and customer service ‚Äî the executive will lose patience rapidly. It is critical that startups speak the language of technology and business. They need to translate between technical and commercial languages.

5. Lower the barriers to trial (a pilot)

It is important that startups make it really simple to implement a pilot. AI solutions often require much data and much time to train the machine learning models. And AI often requires significant engineering integration into back-end technologies to work well. It can take months to acquire, clean and wrangle data then months to use it to train models. In enterprises that are very busy there is little appetite to do a lot of heavy lifting to setup a pilot. Companies want quick proof of concepts. DigitalGenius has done a good job on this front. Their customer service technology works on top of existing enterprise platforms, such as Salesforce, and they have minimised the amount of time it takes to train the technology to answer customer queries. They have also made it very simple to integrate into existing work-flows providing human-over-the-loop decision making. KLM, the Dutch airline, claims to respond to over 50% of customer enquiries on social media by implementing a machine learning chatbot with DigitalGenius.

6. Technical founders need to hire business people.

Startups need to ensure they have the skills to cross the commercial divide from the world of techies to the enterprise. The Harvard Business Review recently reported that the startups most likely to succeed have technical founders who have quickly hired business people. ‚ÄúOne theory for why technical skills seem to matter more for a founder is simply that the average technical founder has better business skills than the average business-trained founder has technical skills.‚Äù But blending the DNA of founders with commercial people often requires alchemy. I have lost count of the number of times that I have seen startups struggling to get this cultural fit right. The most common is technical founders will hire sales ‚Äúfarmers‚Äù instead of sales ‚Äúhunters.‚Äù Farmers don‚Äôt know how to go out and hunt for business which is what is required for a startup . The are used to big, fat marketing departments bringing them a torrent of leads to harvest.
A Simple CNN: Multi Image Classifier

Using Tensorflow and transfer learning, easily make a labeled image classifier with convolutional neural network Iftekher Mamun ¬∑ Follow Published in Towards Data Science ¬∑ 11 min read ¬∑ Apr 7, 2019 -- 10 Listen Share

Computer vision and neural networks are the hot new IT of machine learning techniques. With advances of neural networks and an ability to read images as pixel density numbers, numerous companies are relying on this technique for more data. For example, speed camera uses computer vision to take pictures of license plate of cars who are going above the speeding limit and match the license plate number with their known database to send the ticket to. Although this is more related to Object Character Recognition than Image Classification, both uses computer vision and neural networks as a base to work.

A more realistic example of image classification would be Facebook tagging algorithm. When you upload an album with people in them and tag them in Facebook, the tag algorithm breaks down the person‚Äôs picture pixel location and store it in the database. Because each picture has its own unique pixel location, it is relatively easy for the algorithm to realize who is who based on previous pictures located in the database. Of course the algorithm can make mistake from time to time, but the more you correct it, the better it will be at identifying your friends and automatically tag them for you when you upload. However, the Facebook tag algorithm is built with artificial intelligence in mind. This means that the tagging algorithm is capable of learning based on our input and make better classifications in the future.

We will not focus on the AI aspect, but rather on the simplest way to make an image classification algorithm. The only difference between our model and Facebook‚Äôs will be that ours cannot learn from it‚Äôs mistake unless we fix it. However, for a simple neural network project, it is sufficient.

Since it is unethical to use pictures of people, we will be using animals to create our model. My friend Vicente and I have already made a project on this, so I will be using that as the example to follow through. The GitHub is linked at the end.

The first step is to gather the data. This in my opinion, will be the most difficult and annoying aspect of the project. Remember that the data must be labeled. Thankfully, Kaggle has labeled images that we can easily download. The set we worked with can be found here: animal-10 dataset. If your dataset is not labeled, this can be be time consuming as you would have to manually create new labels for each categories of images. Another method is to create new labels and only move 100 pictures into their proper labels, and create a classifier like the one we will and have that machine classify the images. This will lead to errors in classification, so you may want to check manually after each run, and this is where it becomes time consuming.

Now that we have our datasets stored safely in our computer or cloud, let‚Äôs make sure we have a training data set, a validation data set, and a testing data set. Training data set would contain 85‚Äì90% of the total labeled data. This data would be used to train our machine about the different types of images we have. Validation data set would contain 5‚Äì10% of the total labeled data. This will test how well our machine performs against known labeled data. The testing data set would contain the rest of the data in an unlabeled format. This testing data will be used to test how well our machine can classify data it has never seen. The testing data can also just contain images from Google that you have downloaded, as long as it make sense to the topic you are classifying.

Let‚Äôs import all the necessary libraries first:

import pandas as pd

import numpy as np

import itertools

import keras

from sklearn import metrics

from sklearn.metrics import confusion_matrix

from keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img

from keras.models import Sequential

from keras import optimizers

from keras.preprocessing import image

from keras.layers import Dropout, Flatten, Dense

from keras import applications

from keras.utils.np_utils import to_categorical

import matplotlib.pyplot as plt

import matplotlib.image as mpimg

%matplotlib inline

import math

import datetime

import time

Defining Dimensions and locating images:

#Default dimensions we found online

img_width, img_height = 224, 224



#Create a bottleneck file

top_model_weights_path = ‚Äòbottleneck_fc_model.h5‚Äô # loading up our datasets

train_data_dir = ‚Äòdata/train‚Äô

validation_data_dir = ‚Äòdata/validation‚Äô

test_data_dir = ‚Äòdata/test‚Äô



# number of epochs to train top model

epochs = 7 #this has been changed after multiple model run

# batch size used by flow_from_directory and predict_generator

batch_size = 50

In this step, we are defining the dimensions of the image. Depending on your image size, you can change it but we found best that 224, 224 works best. Then we created a bottleneck file system. This will be used to convert all image pixels in to their number (numpy array) correspondent and store it in our storage system. Once we run this, it will take from half hours to several hours depending on the numbers of classifications and how many images per classifications. Then we simply tell our program where each images are located in our storage so the machine knows where is what. Finally, we define the epoch and batch sizes for our machine. For neural networks, this is a key step. We found that this set of pairing was optimal for our machine learning models but again, depending on the number of images that needs to be adjusted.

Importing transfer learning model VGG16:

#Loading vgc16 model

vgg16 = applications.VGG16(include_top=False, weights=‚Äôimagenet‚Äô) datagen = ImageDataGenerator(rescale=1. / 255)

#needed to create the bottleneck .npy files

This is importing the transfer learning aspect of the convolutional neural network. Transfer learning is handy because it comes with pre-made neural networks and other necessary components that we would otherwise have to create. There are many transfer learning model. I particularly like VGG16 as it uses only 11 convolutional layers and pretty easy to work with. However, if you are working with larger image files, it is best to use more layers, so I recommend resnet50, which contains 50 convolutional layers.

For our image classifier, we only worked with 6 classifications so using transfer learning on those images did not take too long, but remember that the more images and classifications, the longer this next step will take. But thankfully since you only need to convert the image pixels to numbers only once, you only have to do the next step for each training, validation and testing only once- unless you have deleted or corrupted the bottleneck file.

Creation of the weights and feature using VGG16:

#__this can take an hour and half to run so only run it once.

#once the npy files have been created, no need to run again. Convert this cell to a code cell to run.__ start = datetime.datetime.now()



generator = datagen.flow_from_directory(

train_data_dir,

target_size=(img_width, img_height),

batch_size=batch_size,

class_mode=None,

shuffle=False)



nb_train_samples = len(generator.filenames)

num_classes = len(generator.class_indices)



predict_size_train = int(math.ceil(nb_train_samples / batch_size))



bottleneck_features_train = vgg16.predict_generator(generator, predict_size_train)



np.save(‚Äòbottleneck_features_train.npy‚Äô, bottleneck_features_train)

end= datetime.datetime.now()

elapsed= end-start

print (‚ÄòTime: ‚Äò, elapsed)

Since we are making a simple image classifier, there is no need to change the default settings. Just follow the above steps for the training, validation, and testing directory we created above. However, you can add different features such as image rotation, transformation, reflection and distortion.

Once the files have been converted and saved to the bottleneck file, we load them and prepare them for our convolutional neural network. This is also a good way to make sure all your data have been loaded into bottleneck file. Remember to repeat this step for validation and testing set as well.

Creating a bottleneck file for the training data. (Same step for validation and testing):

#training data

generator_top = datagen.flow_from_directory(

train_data_dir,

target_size=(img_width, img_height),

batch_size=batch_size,

class_mode=‚Äôcategorical‚Äô,

shuffle=False)



nb_train_samples = len(generator_top.filenames)

num_classes = len(generator_top.class_indices)



# load the bottleneck features saved earlier

train_data = np.load(‚Äòbottleneck_features_train.npy‚Äô)



# get the class labels for the training data, in the original order

train_labels = generator_top.classes



# convert the training labels to categorical vectors

train_labels = to_categorical(train_labels, num_classes=num_classes)

Creating our Convolutional Neural Network code:

#This is the best model we found. For additional models, check out I_notebook.ipynb start = datetime.datetime.now()

model = Sequential()

model.add(Flatten(input_shape=train_data.shape[1:]))

model.add(Dense(100, activation=keras.layers.LeakyReLU(alpha=0.3)))

model.add(Dropout(0.5))

model.add(Dense(50, activation=keras.layers.LeakyReLU(alpha=0.3)))

model.add(Dropout(0.3))

model.add(Dense(num_classes, activation=‚Äôsoftmax‚Äô)) model.compile(loss=‚Äôcategorical_crossentropy‚Äô,

optimizer=optimizers.RMSprop(lr=1e-4),

metrics=[‚Äòacc‚Äô]) history = model.fit(train_data, train_labels,

epochs=7,

batch_size=batch_size,

validation_data=(validation_data, validation_labels)) model.save_weights(top_model_weights_path) (eval_loss, eval_accuracy) = model.evaluate(

validation_data, validation_labels, batch_size=batch_size, verbose=1) print(‚Äú[INFO] accuracy: {:.2f}%‚Äù.format(eval_accuracy * 100))

print(‚Äú[INFO] Loss: {}‚Äù.format(eval_loss))

end= datetime.datetime.now()

elapsed= end-start

print (‚ÄòTime: ‚Äò, elapsed)

Now we create our model. First step is to initialize the model with Sequential(). After that we flatten our data and add our additional 3 (or more) hidden layers. This step is fully customizable to what you want. We made several different models with different drop out, hidden layers and activation. But since this is a labeled categorical classification, the final activation must always be softmax. It is also best for loss to be categorical crossenthropy but everything else in model.compile can be changed. Then after we have created and compiled our model, we fit our training and validation data to it with the specifications we mentioned earlier. Finally, we create an evaluation step, to check for the accuracy of our model training set versus validation set.
The best way of to learn anything is by doing.

What do you do after you have completed hundreds of MOOCs, consumed thousands of books and notes, and listened to a million people rant about their experience in Data Science? You start applying the concepts. The only way to apply machine learning concepts is by getting your hands dirty. Either find some real-world problems in your area of interest or participate in Hackathons and Machine learning Competitions.

Competitive Data Science is not all about applying algorithms. An algorithm is essentially a tool, and anybody can use it by writing just a few lines of code. The main takeaway from participating in these competitions is that they provide a great learning opportunity. Of course, real-life problems are not necessarily the same as those provided in the competitions; still, these platforms enable you to apply your knowledge to processes and see how you fare compared to others.

Advantages of participating in Data Science Competitions

You have a lot to gain and practically nothing to lose by participating in these competitions. It has both tangible and intangible benefits like:

Great learning opportunity.

Getting exposed to state-of-the-art approaches and datasets.

Networking with like-minded people. Working in teams is even great since it helps to think over a problem from different perspectives.

Showcasing your talent to the world and a chance of getting recruited

It is also fun to participate and see how you fare on the leaderboard.

The prize is a bonus but shouldn‚Äôt be the sole criteria.

Kaggle is a well-known platform for Data Science competitions. It is an online community of more than 1,000,00 registered users consisting of both novice and expert. However, apart from Kaggle, there are other Data Mining Competition Platforms worth knowing and exploring. Here is a brief overview of some of‚Ä¶
Many times, as Data Scientists, we have to deal with huge amount of data. In those cases, many approaches won‚Äôt work or won‚Äôt be feasible. A massive amount of data is good, it‚Äôs very good, and we want to utilize as much as possible.

Here I want to introduce the MapReduce technique, which is a broad technique that is used to handle a huge amount of data. There are many implementations of MapReduce, including the famous Apache Hadoop. Here, I won‚Äôt talk about implementations. I‚Äôll try to introduce the concept in the most intuitive way and present examples for both toy and real-life examples.

Let‚Äôs start with some straightforward task. You‚Äôre given a list of strings, and you need to return the longest string. It‚Äôs pretty easy to do in python:

def find_longest_string(list_of_strings):

longest_string = None

longest_string_len = 0 for s in list_of_strings:

if len(s) > longest_string_len:

longest_string_len = len(s)

longest_string = s return longest_string

We go over the strings one by one, compute the length and keep the longest string until we finished.

For small lists it works pretty fast:

list_of_strings = ['abc', 'python', 'dima'] %time max_length = print(find_longest_string(list_of_strings)) OUTPUT:

python

CPU times: user 0 ns, sys: 0 ns, total: 0 ns

Wall time: 75.8 ¬µs

Even for lists with much more than 3 elements it works pretty well, here we try with 3000 elements:

large_list_of_strings = list_of_strings*1000 %time print(find_longest_string(large_list_of_strings)) OUTPUT:

python

CPU times: user 0 ns, sys: 0 ns, total: 0 ns

Wall time: 307 ¬µs

But what if we try for 300 million elements?

large_list_of_strings = list_of_strings*100000000

%time max_length = max(large_list_of_strings, key=len) OUTPUT:

python

CPU times: user 21.8 s, sys: 0 ns, total: 21.8 s

Wall time: 21.8 s

This is a problem, in most applications, 20 seconds response time is not acceptable. One way to improve the computation time is by buying a much better and faster CPU. Scaling your system by introducing better and faster hardware is called ‚ÄúVertical Scaling‚Äù. This, of course, won‚Äôt work forever. Not only it‚Äôs not so trivial to find a CPU that work 10 times faster, but also, our data will probably get bigger, and we don‚Äôt want to upgrade our CPU every time the code gets slower. Our solution is not scalable. Instead, we can do ‚ÄúHorizontal Scaling‚Äù, we‚Äôll design our code so it could run in parallel, and it will get much faster when we‚Äôll add more processors and/or CPUs.

To do that, we need to break our code into smaller components and see how we can execute computations in parallel. The intuition is as follows: 1) break our data into many chunks, 2) execute the find_longest_string function for every chunk in parallel and 3) find the longest string among the outputs of all chunks.

Our code is very specific and it hard to break and modify, so instead of using the find_longest_string function, we‚Äôll develop a more generic framework that will help us perform different computations in parallel on large data.

The two main things we do in our code is computing the len of the string and comparing it to the longest string until now. We‚Äôll break our code into two steps: 1) compute the len of all strings and 2) select the max value.

%%time # step 1:

list_of_string_lens = [len(s) for s in list_of_strings]

list_of_string_lens = zip(list_of_strings, list_of_string_lens) #step 2:

max_len = max(list_of_string_lens, key=lambda t: t[1])

print(max_len) OUTPUT:

('python', 6)

CPU times: user 51.6 s, sys: 804 ms, total: 52.4 s

Wall time: 52.4 s

(I‚Äôm calculating the length of the strings and then zip them together because this is much faster than doing it in one line and duplicating the list of strings)

In this state, the code runs actually slower than before because instead of performing a single pass on all of our strings, we do it 2 times, first to compute the len and then to find the max value. Why it is good for us? because now our ‚Äústep 2‚Äù gets as input not the original list of strings, but some preprocessed data. This allows us to execute step two using the output of another ‚Äústep two‚Äùs! We‚Äôll understand that better in a bit, but first, let‚Äôs give those steps a name. We‚Äôll call ‚Äústep one‚Äù a ‚Äúmapper‚Äù because it maps some value into some other value, and we‚Äôll call ‚Äústep two‚Äù a reducer because it gets a list of values and produces a single (in most cases) value. Here‚Äôre two helper functions for mapper and reducer:

mapper = len def reducer(p, c):

if p[1] > c[1]:

return p

return c

The mapper is just the len function. It gets a string and returns its length. The reducer gets two tuples as input and returns the one with the biggest length.

Let‚Äôs rewrite our code using map and reduce , there are even built-in functions for this in python (In python 3, we have to import it from functools ).

%%time #step 1

mapped = map(mapper, list_of_strings)

mapped = zip(list_of_strings, mapped) #step 2:

reduced = reduce(reducer, mapped) print(reduced) OUTPUT:

('python', 6)

CPU times: user 57.9 s, sys: 0 ns, total: 57.9 s

Wall time: 57.9 s

The code does exactly the same thing, it looks bit fancier, but also it is more generic and will help us parallelize it. Let‚Äôs look more closely at it:

Step 1 maps our list of strings into a list of tuples using the mapper function (here I use the zip again to avoid duplicating the strings).

Step 2 uses the reducer function, goes over the tuples from step one and applies it one by one. The result is a tuple with the maximum length.

Now let's break our input into chunks and understand how it works before we do any parallelization (we‚Äôll use the chunkify that breaks a large list into chunks of equal size):

data_chunks = chunkify(list_of_strings, number_of_chunks=30) #step 1:

reduced_all = []

for chunk in data_chunks:

mapped_chunk = map(mapper, chunk)

mapped_chunk = zip(chunk, mapped_chunk)



reduced_chunk = reduce(reducer, mapped_chunk)

reduced_all.append(reduced_chunk)



#step 2:

reduced = reduce(reducer, reduced_all) print(reduced) OUTPUT:

('python', 6)

In step one, we go over our chunks and find the longest string in that chunk using a map and reduce. In step two, we take the output of step one, which is a list of reduced values, and perform a final reduce to get the longest string. We use number_of_chunks=36 because this is the number of CPUs I have on my machine.

We are almost ready to run our code in parallel. The only thing that we can do better is to add the first reduce step into a single the mapper. We do that because we want to break our code into two simple steps and as the first reduce works on a single chunk and we want to parallelize it as well. This is how it looks like:

def chunks_mapper(chunk):

mapped_chunk = map(mapper, chunk)

mapped_chunk = zip(chunk, mapped_chunk)

return reduce(reducer, mapped_chunk) %%time data_chunks = chunkify(list_of_strings, number_of_chunks=30) #step 1:

mapped = map(chunks_mapper, data_chunks) #step 2:

reduced = reduce(reducer, mapped) print(reduced) OUTPUT:

('python', 6)

CPU times: user 58.5 s, sys: 968 ms, total: 59.5 s

Wall time: 59.5 s

Now we have a nice looking two steps code. If we‚Äôll execute it as is, we‚Äôll get the same computation time, but, now we can parallelize step 1 using the multiprocessing module simply by using the pool.map function instead of the regular map function:

from multiprocessing import Pool pool = Pool(8) data_chunks = chunkify(large_list_of_strings, number_of_chunks=8) #step 1:

mapped = pool.map(mapper, data_chunks) #step 2:

reduced = reduce(reducer, mapped) print(reduced) OUTPUT:

('python', 6)

CPU times: user 7.74 s, sys: 1.46 s, total: 9.2 s

Wall time: 10.8 s

We can see it runs 2 times faster! It‚Äôs not a huge improvement, but the good news is that we can improve it by increasing the number of processes! We can even do it on more than one machine, if our data is very big, we can use tens or even thousands of machines to make our computation time as short as we want (almost).

Our architecture is built using two functions: map and reduce . Each computation unit maps the input data and executes the initial reduce. Finally, some centralized unit executes the final reduce and returns the output. It looks like this:

This architecture has two important advantages:

It is scalable: if we have more data, the only thing we need to do is to add more processing units. No code change needed! It is generic: this architecture supports a vast variety of tasks, we can replace our map and reduce function with almost anything and this way computer many different things in a scalable way.

It is important to note that in most cases, our data will be very big and static. It means the breaking into chunks every time is inefficient and actually redundant. So in most applications in real life, we‚Äôll store our data in chunks (or shards) from the very beginning. Then, we‚Äôll be able to do different computations using the MapReduce technique.

Now let's see a more interesting example: Word Count!

Say we have a very big set of news articles and we want to find the top 10 used words not including stop words, how would we do that? First, let's get the data:

from sklearn.datasets import fetch_20newsgroups

data = news.data*10

For this post, I made the data x10 larger so we could see the difference.

For each text in the dataset, we want to tokenize it, clean it, remove stop words and finally count the words:

def clean_word(word):

return re.sub(r'[^\w\s]','',word).lower() def word_not_in_stopwords(word):

return word not in ENGLISH_STOP_WORDS and word and word.isalpha()





def find_top_words(data):

cnt = Counter()

for text in data:

tokens_in_text = text.split()

tokens_in_text = map(clean_word, tokens_in_text)

tokens_in_text = filter(word_not_in_stopwords, tokens_in_text)

cnt.update(tokens_in_text)



return cnt.most_common(10)

Let‚Äôs see how much time does it take without MapReduce:

%time find_top_words(data) OUTPUT: [('subject', 122520),

('lines', 118240),

('organization', 111850),

('writes', 78360),

('article', 67540),

('people', 58320),

('dont', 58130),

('like', 57570),

('just', 55790),

('university', 55440)] CPU times: user 51.7 s, sys: 0 ns, total: 51.7 s

Wall time: 51.7 s

Now, let‚Äôs write our mapper , reducer and chunk_mapper :

def mapper(text):

tokens_in_text = text.split()

tokens_in_text = map(clean_word, tokens_in_text)

tokens_in_text = filter(word_not_in_stopwords, tokens_in_text)

return Counter(tokens_in_text) def reducer(cnt1, cnt2):

cnt1.update(cnt2)

return cnt1 def chunk_mapper(chunk):

mapped = map(mapper, chunk)

reduced = reduce(reducer, mapped)

return reduced

The mapper gets a text, splits it into tokens, cleans them and filters stop words and non-words, finally, it counts the words within this single text document. The reducer function gets 2 counters and merges them. The chunk_mapper gets a chunk and does a MapReduce on it. Now let‚Äôs run using the framework we built it and see:

%%time data_chunks = chunkify(data, number_of_chunks=36) #step 1:

mapped = pool.map(chunk_mapper, data_chunks) #step 2:

reduced = reduce(reducer, mapped) print(reduced.most_common(10)) OUTPUT:

[('subject', 122520),

('lines', 118240),

('organization', 111850),

('writes', 78360),

('article', 67540),

('people', 58320),

('dont', 58130),

('like', 57570),

('just', 55790),

('university', 55440)] CPU times: user 1.52 s, sys: 256 ms, total: 1.77 s

Wall time: 4.67 s

This is 10 times faster! Here, we were able to really utilize our computational power because the task is much more complex and requires more.

To sum up, MapReduce is an exciting and essential technique for large data processing. It can handle a tremendous number of tasks including Counts, Search, Supervised and Unsupervised learning and more. Today there‚Äôs a lot of implementations and tools that can make our lives much more comfortable, but I think it is very important to understand the basics.
The simplest way to describe any machine learning project is that it‚Äôs a program that when given pieces of data it hasn‚Äôt seen before, it will process them based on previous experience and tell you something(s) you did not already know.

‚ÄúData is the new oil.‚Äù ‚Äî Clive Humb ‚Äî Chief Data Scientist and Executive Director of Starcount

Data is at the core of nearly every business decision made.

Human resources directors are gathering data from online resources to determine the best people to recruit and confirm details about them. Marketing departments are lasering in on market segmentation data to find consumers who are ready to buy, speeding up the sale-closing process whenever possible.

Business executives must examine bigger trends in the market, such as changes in pricing of resources, shipping or manufacturing.

Your project is only as powerful as the data you bring.

Step 1: Gathering the data

The choice of data entirely depends on the problem you‚Äôre trying to solve.

Picking the right data must be your goal, luckily, almost every topic you can think of has several datasets which are public & free.

3 of my favorite free awesome website for dataset hunting are:

Kaggle which is so organized. You‚Äôll love how detailed their datasets are, they give you info on the features, data types, number of records. You can use their kernel too and you won‚Äôt have to download the dataset. Reddit which is great for requesting the datasets you want. Google Dataset Search which is still Beta, but it‚Äôs amazing. UCI Machine Learning Repository, this one maintains 468 data sets as a service to the machine learning community.

The good thing is that data is means to an end, in other words, the quantity of the data is important but not as important as the quality of it. So, if you‚Äôd like to be independent and create your own dataset and begin with a couple of‚Ä¶
Hello everyone and welcome to the second article in the ‚ÄúFrom Scratch‚Äù series. (Previous one: From Scratch: Bayesian Inference, Markov Chain Monte Carlo and Metropolis Hastings, in python)

In this article we explain and provide an implementation for ‚ÄúThe Game of Life‚Äù. I say ‚Äòwe‚Äô because this time I am joined by my friend and colleague Michel Haber. The code is provided on both of our GitHub profiles: Joseph94m, Michel-Haber.

The first section will be focused on giving an explanation of the rules of the game as well as examples of how it is played/defined. The second section will provide the implementation details in Python and Haskell for the game of life. In the third section, we compare the performance and elegance of our implementations. We chose these two languages because they represent two of the most used programming paradigms, imperative (Python) and functional (Haskell), and because of the relative ease they provide in writing expressive and understandable code.

1-Introduction to the Game of Life

The game of life is a cellular automaton imagined by John H. Conway in the 1970s and is probably, the best known of all cellular automata. Despite very simple rules, the game of life is Turing-complete and deterministic.

The game of life is a game in the mathematical sense rather than a playable game. It is ‚Äúzero player game‚Äù.

The game takes place on a two-dimensional finite or infinite grid whose cells can take two distinct states: ‚Äúalive‚Äù or ‚Äúdead‚Äù.

At each stage, the evolution of a cell is entirely determined by its current state and the state of its eight neighbours as follows:

1) A dead cell with exactly three living neighbours becomes alive.

2) A living cell with two or three living neighbours remains alive.

3) In all other cases, the cell becomes (or remains) dead.

Let‚Äôs assume a simple initial state where only 3 cells are alive: left cell, middle cell, and right cell.

Starting from the simple configuration in Figure 1, and letting the simulation run for one iteration results in Figure 2.

So how does this happen?

Figure 2

The cells on the left and on the right have only one neighbour, the middle cell, so they die. The middle cell has two neighbours, left and right, so it stays alive. The top and bottom cells have 3 neighbours (middle, right and left) so they become alive. It is really important to note that a cell does not die or live until the end of the iteration. I.e. the algorithm decides which cells are going to die or come to life and then gives them the news all together. This ensures that the order in which cells are evaluated does not matter.

Running one more iteration gives us the state represented in Figure 1(initial state).

As such, starting with this configuration, the game enters the infinite loop represented in Figure 3. This configuration is called a blinker.

Figure 3: Blinker

Figure 4: An early state from figure 5 that shows some interesting structures.

Now for a more interesting setup to show what impressed Conway the most. Starting from the simple configuration in the left image of Figure 5 and letting the code run, the board evolves into something completely unexpected. We can observe stable structures: blocks and beehives. We can also observe looping structures such as the blinkers from Figure 3 and the larger blinkers that are made up of 3 or 4 smaller ones. Another interesting structure observed in this simulation is the glider. A glider looks like an old video game spaceship. It is unbounded by space, and keeps on going ‚Äî Ad vitam √¶ternam.
Subspace clustering

This post addresses the following questions:

What are the challenges of working with high dimensional data? What is subspace clustering? How to implement a subspace clustering algorithm in python

High dimensional data consists in input having from a few dozen to many thousands of features (or dimensions). This is a context typically encountered for instance in bioinformatics (all sorts of sequencing data) or in NLP where the size of the vocabulary if very high. High dimensional data is challenging because:

it makes the visualization and thus understanding of the input difficult, it often requires applying a dimensionality reduction technique beforehand. It leads to the ‚Äòcurse of dimensionality‚Äô which means that the complete enumeration of all subspaces becomes intractable with increasing dimensionality

most underlying clustering techniques depend on the results and the choice of the dimensionality reduction technique

many dimensions may be irrelevant and can mask existing clusters in noisy data

one common technique is to perform feature selection (remove irrelevant dimensions) but there are cases when identifying redundant dimensions is not easy

What is subspace clustering?

Subspace clustering is a technique which finds clusters within different subspaces (a selection of one or more dimensions). The underlying assumption is that we can find valid clusters which are defined by only a subset of dimensions (it is not needed to have the agreement of all N features). For example, if we consider as input patient data observing the gene expression level (we can have more than 20000 features), a cluster of patients suffering from Alzheimer can be found only by looking at the expression data of a subset of 100 genes, or stated differently, the subset exists in 100D. Stated differently, subspace clustering is an extension of traditional N dimensional cluster analysis which allows to simultaneously group features and observations by creating both row and column clusters.

The resulting clusters may be overlapping both in the space of features and observations. Another example is shown in the figures below, taken from the paper. We can notice that points from 2 clusters can be very close which can confuse many traditional clustering algorithms analyzing the entire feature space.

Further more, we can see that subspace clustering manages to find a subspace (dimensions a and c) where the expected clusters are easily identifiable.

Types of subspace clustering

Based on the search strategy, we can differentiate 2 types of subspace clustering, as shown in the figure below: bottom up approaches start by finding clusters in low dimensional (1 D) spaces and iteratively merging them to process higher dimensional spaces (up to N D). Top down approaches find clusters in the full set of dimensions and evaluate the subspace of each cluster. The figure below, taken from the same paper provides an overview of the most common subspace clustering algorithms.

Clique algorithm

In order to better understand subspace clustering, I have implemented the Clique algorithm in python here.

In a nutshell, the algorithm functions as follows: for each dimension (feature) we split the space in nBins(input parameter) and for each bin we compute the histogram (number of counts). We only consider dense units, that is the bins with a count superior to a threshold given as second input parameter. A dense unit is characterized by the following:

the dimension it belongs to (e.g. feature 1)

the index (or the position) of the bin (from 0 to nBins)

the observations lying in the bin

In my implementation I have generated 4 random clusters in a 2D space and I have chosen 8 bins and 2 points as minimal density threshold. The figure below shows the resulting grid applied to the input space.

Input space split in 8 bins per dimension

The intuition behind the clique algorithm is that clusters existing in a k dimensional space can also be found in k-1. We start from 1D and for each dimension we try to find the dense bins. If 2 or more dense bins are neighbors, we merge them into one bigger bin. This operation can be easily implemented by transforming all existing dense bins into a graph, where an edge is drawn if 2 dense units belong to the same dimension and the difference between their bin index is no more than 1 ( e.g a dense unit corresponding to feature 3 and bin 4 is neighbor for dense units of the same feature and bins 3 and 5). The dense units to be merged can be identified by calculating the connected components on the graph described above.

The result of this merging operation retrieves the following clusters in 1 D (one plot per cluster) for the first dimension:

and for the second dimension:

Next, we want to calculate all valid clusters in each subspace from 2 to the number of input dimensions. This operation comes down to calculating combinations of dense units in k dimensions and only keeping results having an overlap of dense continuous bins with the size greater than the initial minimal density threshold. Once we calculated the dense units for k-1 dimensions we can extend to k dimensions by computing all combinations of these last k-1 candidates.

Thus, in 2 D we are able to retrieve the clusters shown in the figure below. Note that some points (in purple) outside the 4 clusters because they belong to bins having a density inferior to the arbitrary input of 2.

Clique clustering has been criticized for its high sensitivity to the input parameters (the number of bins and the minimal density) which can lead to very different results. However, it is an essential algorithm in the family of bottom-up subspace clustering. There are multiple ways to optimize the clique algorithm, for instance by using a density adaptive grid as proposed in the MAFIA algorithm.

References

Clique paper

Mafia algorithm

Comparative study of subspace clustering methods

Presentation of subspace clustering methods
The interplay of loss function and network architecture is subtle, and as we will see there are often multiple ways to solve a problem, but first we need to define a conditional loss for a generative adversarial network. The key principle here is the same as any GAN, the generative adversarial min max game between the two networks to optimise the loss function:

where G is the generator and D the discriminator, x the conditional input image, y the true image and z a random noise vector.

(A more detailed discussion here, but the basic idea is G outputs an image such that D(G) is maximised, while D is optimised to output 1 for true images. Why this ever converges, and to what extent we‚Äôre barking up the wrong tree, is another interesting discussion here.)

This already has one key difference to the vanilla GAN case: it‚Äôs conditional. Conditional here means that rather than receiving just a random noise vector, the generator takes in additional information. This could simply be class information as in the cDCGAN, or as in this case where the conditional information is the original image. Where the vannilla GAN depends as G:z -> y, the conditional GAN goes as G:{x, z} -> y.

Interestingly, this isn‚Äôt actually the full picture. When the network trains, it generally learns to ignore the random noise vector, so to keep the network non-deterministic dropout was used to reintroduce the stochastic behaviour.

In many GANs, an L2 regularisation loss is added to the optimisation on the basis that by minimising the Euclidean distance between the target and generated image (MSE), the generator would learn structure and colour of the image. However, it was found that this generally leads to blurred images, so to combat this the L1 regularisation loss is added with some pre-factor weight instead, as:

Which gives the total loss function to optimise as:

The next key question is about the structure of the networks.

In this analysis, the generator is based on a U-Net structure, a variation on an auto encoder, while the discriminator is called patch-GAN.

Generator Architecture

The point of using a U-Net structure is that the network forces all information to be passed through a tight bottle neck in the middle of the network. This forces a latent representation of the input image to be found that can be reconstructed to the original, the idea being that a finite amount of information can pass through - the network is forced to learn an optimal reduced mapping, and cannot simply memorise the training set.

This has one obvious limitation; there is a significant proportion of the output and input image that ought to share the same description. The content of the image is supposed to remain unchanged. The variation in U-Net over an auto-encoder is the addition of a skip connection between each symmetric layer in the U-net structure, as can be seen in figure above. These concatenated layers have the effect of passing higher level information directly at the appropriate scale in the network, and reduce the amount of information needed to pass through the latent bottleneck layer. The idea here is that the information passed through focuses on finer details rather than large scale structure.

The discriminator is more atypical and might need more context. In general terms, the L1 and L2 regularisation is a weak constraint on the network that doesn‚Äôt produce sharp details as there are many paths to get a small L value. However, this doesn‚Äôt write off this part of the loss function, as it encourages generating the high level structure, which is exploited in the choice of discriminator.

Introducing crisp details to the generated image can be done via a number of paths:

Tuning the weight of the lambda pre-factor on the L1/L2 loss ‚Äî as discussed above, this caps out with relatively blurred images, generally correct but without sharp details. Adding an additional loss that quantifies the performance of the output image ‚Äî a number of methods using a pre-trained network to assess the quality of the output images have been tried. Specifically in the case of SRGAN, the distance between the latent space of the VGG network on the target and output images is minimised. Update the discriminator to promote the crisp details ‚Äî patch-GAN ‚Äî seems almost too obvious, doesn‚Äôt it?

Patch-GAN Discriminator [here]

The way patch GAN works is that rather than classifying the entire image, only a portion (NxN) of the image is classified as real or fake. The ‚Äòpatch‚Äô scans over the image, makes a prediction for each, and the mean is optimised.

This assumes the image can be treated as a Markov random field, where pixels separated by N or more pixels are independent. This wouldn‚Äôt be true for the entire image when considering high level structure, but is a fair assumption when looking at low level detail.

The advantage here is that fine detail is preserved with the patch-GAN; to pass the discriminator test realistic low level details are encouraged, and the general structure is preserved using the regularisation loss. Another vital advantage of this patch method over a whole image classifier is that it‚Äôs fast, as small NxN patches are faster to evaluate and train than one single discriminator. It also scales efficiently to arbitrary sized images.

There are two technical points on the training of the network. Firstly, rather than minimising (1 ‚Äî log D), the training maximises log(D). Secondly, the Discriminator objective function is halved during training to restrict the rate at which the discriminator trains compared to the generator.
A collaborative filtering model/recommendation system seeks to predict the rating or preference a user would give to an item given his old item ratings or preferences. Recommendation systems are used by pretty much every major company in order to enhance the quality of their services.

In one of my first articles, I created a book recommendation system using Keras a high-level API build on Tensorflow. In this article, I will show you how to build the same recommendation system using the FastAI library as well as how to build a neural network based model to get even better results.

For our data, we will use the goodbooks-10k dataset which contains ten thousand different books and about one million ratings. It has three features the book_id, user_id and rating. If you want you can get the all the data files as well as the complete code covered in this article from my Github repository.

If you prefer a visual tutorial you can check out my FastAi videos.
Is your algorithm confident enough?

When machine learning techniques are used in ‚Äúmission critical‚Äù applications, the acceptable margin of error becomes significantly lower.

Imagine that your model is driving a car, assisting a doctor or even just interacting directly with an (perhaps easily annoyed) end user. In these cases, you‚Äôll want to ensure that you can be confident in the predictions your model makes before acting on them.

Measuring prediction uncertainty grows more important by the day, as fuzzy systems become an increasing part of our fuzzy lives.

Here‚Äôs the good news: There are several techniques for measuring uncertainty in neural networks and some of them are very easy to implement! First, let‚Äôs get a feel for what we‚Äôre about to measure.

Photo credit: Juan Rumimpunu.

Putting a number on uncertainty.

When you make models of the world, your models cannot always provide accurate answers.

This is partly due to that fact that models are simplifications of a seriously complicated world. Since some information is unknown, the predictions from your model are subject to some degree of uncertainty.

Parts of our world (and the ways we measure it) are simply chaotic. Some things happen randomly, and this randomness is also a source of uncertainty in your model‚Äôs predictions.

Prediction uncertainty can be divided into 3 categories:

1. Model uncertainty.

Model uncertainty comes from ‚Äúignorance‚Äù of the problem. That is, model uncertainty quantifies the things that could be correctly captured by the model but isn‚Äôt.

Yoel and Inbar from Taboola provide a fun example:

You want to build a model that gets a picture of an animal, and predicts if that animal will try to eat you. You trained the model on pictures of lions and giraffes. Now you show it a zombie. Since the model wasn‚Äôt trained on pictures of zombies, the uncertainty will be high. If trained on enough pictures of zombies, this uncertainty will decrease.

Sometimes it is also referred to as epistemic or structural uncertainty. Measuring model uncertainty is an area of statistics which is considered to be particularly challenging. One reason for this, is that principled techniques like Bayesian model averaging become very costly as models grow more complex.

2. Model misspecification.

If your model produces good predictions during training and validation but not during evaluation (or in production), it might be misspecified.

Model misspecification uncertainty captures scenarios where your model is making predictions on new data with very different patterns from the training data.

3. Inherent noise.

This is uncertainty produced by noise present in the dataset. It could be attributed to imperfect measurement techniques or an inherent randomness in the thing being measured.

Imagine your dataset contains 2 images of cards facing down. You‚Äôre feeling optimistic and you want to build a model to predict the suit and value of each card. The first card is labeled as ace of spades and the other is labeled as 8 of hearts. Here, the exact same features (an image of a card facing down) can be linked to different predictions (either ace of spades or 8 of hearts). Therefore, this dataset is subject to lots of inherent noise.

Inherent noise is also sometimes called aleatoric or statistical uncertainty. The amount of inherent noise is linked to the Bayes error rate which the lowest achievable error rate of a given classifier. As you can imagine, the lowest possible error rate that a model can achieve is tightly linked to the amount of error produced by noise in the data itself.
Interactive data visualization has an important impact on exploratory data analysis. Before applying any descriptive or predictive algorithm to a dataset we must first understand how the features are related with each other and how they are distributed inside. It is obvious that many visualization libraries provide numerous types of charts that satisfy this requirement. But another obvious thing is that it is a hard job to do the same plotting work for each feature and scroll over each chart to compare findings for each feature.

For the last couple of weeks, I had to do this job so much that I had to find a shortcut for this. Yes, it is true that I am a lazy man, and yes it is true that laziness is a key to creativity. That is how I met dash and plotly as a solution to my problem. In this post you will find how this couple would be a good solution for exploratory data analysis.

Let me first explain what dash and plotly are for whom did not hear before. Plotly is a data analytics and visualization company. In this writing, we are interested with the two python libraries of this company; plotly.py and dash. Plotly.py library provides interactive visualization for python applications. As indicated on their website, you can ‚ÄúCreate interactive, D3 and WebGL charts in Python. All the chart types of matplotlib and more.‚Äù

Dash is also another product of the same company, providing a framework for building web based applications for Python. If you are working with a team or just want share your work with others, a web application is the simplest way, eliminating the library version or interface issues. We will see how convenient sharing our findings over the web is, during the rest of this writing.

So, let‚Äôs start coding‚Ä¶

A Simple Dash App

Below is a simple dash web application consisting of six lines of code. Just write it down into a .py file and call the file and your app is up and running, that‚Äôs all.

#this is the dash_test.py file import dash

import dash_html_components as html app = dash.Dash(__name__) app.layout = html.H1('hello dash') if __name__ == '__main__':

app.run_server(debug=True, port=8080)

Call the file from command prompt as follows, with the exact path of your file. You will see a console windows that tells the server is running.

python "c:\users\alper\documents\dash_test.py"

We can now open a web browser and navigate to the localhost url with the given port number: 127.0.0.1:8080.

In the first two lines of the code, we simply import the required dash libraries. The third line initializes the dash app, fourth line prepares the page layout with a header tag which we will be displaying on the page, and the last two lines run the server with debug and port options. (See the detailed explanation on stackoverflow for the ‚Äúif __name__ ‚Ä¶ ‚Äù line)

Yes, we are far from both interactivity and visuality but be patient, we are on the way. First, we place the required elements. For this, we will modify app.layout and insert a button and a label element into a div. Note that the two elements are placed in a list as the children of the div element. Dash stores html elements in dash_html_components library, you can find the whole list on their website and github repo.

app.layout = html.Div(

[

html.Button('create random number', id='button1'),

html.Label('...', id='label1')

]

)

When we save the file, we will see a new line on the console window with a new debugger pin. If there is a problem in the code, then we will see the error message instead. In this case, we need to call the file again and refresh the browser.

Now, lets add some styling to the elements we had insert. I can‚Äôt say I am good at styling, but I‚Äôm sure you can do better. We can add style to an element with style attribute accepting a dictionary of css tags.

html.Button('create random number',

id='button1',

style={'display':'block', 'background-color':'#aabbcc'}

),

html.Label('...',

id='label1',

style={'display':'inline-block', 'margin':'10'}

)

And it‚Äôs time to go a step further and add some responsiveness. First we import the required libraries

from dash.dependencies import Input, Output

import random

Then we add the callback decorator and the function we want to execute on callback.

update_output function simply generates a random number and returns it as result.

@app.callback decorator binds the button click event to the update_output function, and the result of the function to the label1 element. This is the core part of the responsiveness. There will be another post on callbacks and state parameters.

Adding a Simple Chart

Since we covered the interactivity enough for introduction, it is time to add some charts. First, we will keep it simple and put a bar chart with random values on each button click. So, we need to add a graph object to our layout:

app.layout = html.Div(

[

html.Button(‚Äòcreate random number‚Äô,

id=‚Äôbutton1',

style={‚Äòdisplay‚Äô:‚Äôblock‚Äô, ‚Äòpadding‚Äô:‚Äô5', ‚Äòbackground-color‚Äô:‚Äô#aabbcc‚Äô}),

html.Label(‚Äò‚Ä¶‚Äô,

id=‚Äôlabel1',

style={‚Äòdisplay‚Äô:‚Äôinline-block‚Äô, ‚Äòmargin‚Äô:‚Äô10'} ),

dcc.Graph(id=‚Äôgraph1') # this is the graph we add

]

)

And we need to modify our callback function to produce the chart:



Output(component_id='graph1', component_property='figure'),

[Input(component_id='button1', component_property='n_clicks')]

)

def update_output(input_value):

random_x = [i for i in range(5)]

random_y = [random.random() for _ in range(5)]

figure = {

'data': [

{'x':random_x, 'y':random_y, 'type':'bar', 'name': 'Series1'}

],

'layout': {

'title': 'Dash Data Visualization'

}

}

return figure @app .callback(Output(component_id='graph1', component_property='figure'),[Input(component_id='button1', component_property='n_clicks')]def update_output(input_value):random_x = [i for i in range(5)]random_y = [random.random() for _ in range(5)]figure = {'data': [{'x':random_x, 'y':random_y, 'type':'bar', 'name': 'Series1'}],'layout': {'title': 'Dash Data Visualization'return figure

In the callback decorator, we first replace the label in our Output statement with the graph object we recently added to our layout. Then inside the function we crate x and y values for the chart, and the figure object. That is all. The result is an interactive bar chart inside your browser.

Some more complexity

If the above chart is not fancy enough for you, don‚Äôt worry, here is another example for you. Let‚Äôs get some deeper.

Was it too fast? Ok, let‚Äôs look at the code then.

# coding=utf8

import pandas as pd

import dash

from dash.dependencies import Input, Output

import dash_html_components as html

import dash_core_components as dcc

import plotly.graph_objs as go

app = dash.Dash(__name__)

names = ['sepal-length', 'sepal-width', 'petal-length', 'petal-width', 'class']

data = pd.read_csv('

app.layout = html.Div(

[

html.Div([

dcc.Dropdown(

id='ddl_x',

options=[{'label': i, 'value': i} for i in names],

value='sepal-width',

style={'width':'50%'}

),

dcc.Dropdown(

id='ddl_y',

options=[{'label': i, 'value': i} for i in names],

value='petal-width',

style={'width':'50%'}

),

],style={'width':'100%','display':'inline-block'}),

html.Div([

dcc.Graph(id='graph1')

],style={'width':'100%','display':'inline-block'})

]

)

.callback(

Output(component_id='graph1', component_property='figure'),

[

Input(component_id='ddl_x', component_property='value'),

Input(component_id='ddl_y', component_property='value')

]

)

def update_output(ddl_x_value, ddl_y_value):

figure={

'data': [

go.Scatter(

x=data[data['class'] == cls][ddl_x_value],

y=data[data['class'] == cls][ddl_y_value],

mode='markers',

marker={ 'size': 15 },

name=cls

) for cls in data['class'].unique()

],

'layout':

go.Layout(

height= 350,

hovermode= 'closest',

title=go.layout.Title(text='Dash Interactive Data Visualization',xref='paper', x=0)

)



}

return figure

if __name__ == '__main__':

app.run_server(debug=True, port=8080) import randomimport pandas as pdimport dashfrom dash.dependencies import Input, Outputimport dash_html_components as htmlimport dash_core_components as dccimport plotly.graph_objs as goapp = dash.Dash(__name__)names = ['sepal-length', 'sepal-width', 'petal-length', 'petal-width', 'class']data = pd.read_csv(' https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data' , names=names)app.layout = html.Div(html.Div([dcc.Dropdown(id='ddl_x',options=[{'label': i, 'value': i} for i in names],value='sepal-width',style={'width':'50%'}),dcc.Dropdown(id='ddl_y',options=[{'label': i, 'value': i} for i in names],value='petal-width',style={'width':'50%'}),],style={'width':'100%','display':'inline-block'}),html.Div([dcc.Graph(id='graph1')],style={'width':'100%','display':'inline-block'}) @app .callback(Output(component_id='graph1', component_property='figure'),Input(component_id='ddl_x', component_property='value'),Input(component_id='ddl_y', component_property='value')def update_output(ddl_x_value, ddl_y_value):figure={'data': [go.Scatter(x=data[data['class'] == cls][ddl_x_value],y=data[data['class'] == cls][ddl_y_value],mode='markers',marker={ 'size': 15 },name=cls) for cls in data['class'].unique()],'layout':go.Layout(height= 350,hovermode= 'closest',title=go.layout.Title(text='Dash Interactive Data Visualization',xref='paper', x=0)return figureif __name__ == '__main__':app.run_server(debug=True, port=8080)

The code structure is exactly the same as the previous. After initializing the app,

we added two lines for data reading.

in the app.layout section, we added two dropdown lists, and fill the options with a loop of data columns.

in the @app.callback decorator, we added these two dropdowns as input components

and in the update_output function, we draw a scatter plot graph with the data and the columns selected by the dropdown lists. Here, there is a tricky part. We draw the scatter plot for each class. You see, there is a for loop at the end of the go.Scatter() function and inside the ‚Äòdata‚Äô list. And this for loop, also called as list comprehension, returns Scatter() objects n times, where n is the number of unique records in the ‚Äòclass‚Äô column of the data. And following line is for the layout properties of the chart.

The code is ready to run. Just;

save it into a file with .py extension, -> ‚Äúc:\‚Ä¶\dash_test.py‚Äù

call it via command prompt using python -> python ‚Äúc:\‚Ä¶\dash_test.py‚Äù

open a browser and navigate to the app -> http://localhost:8080

Your interactive data visualization application is ready in 60 lines of code.
Information Entropy

If you were to watch me cross the street, or watch me play Russian roulette, which one would be more exciting? The possibilities are the same- me living or dying, but we can all agree that the crossing of the street is a bit boring, and the Russian roulette‚Ä¶ maybe too exciting. This is partially because we pretty much know what will happen when I cross the street, but we don‚Äôt really know what will happen in Russian roulette.

Another way of looking at this, is to say we gain less information observing the result of crossing the street than we do from Russian roulette. A formal way of putting that is to say the game of Russian roulette has more ‚Äòentropy‚Äô than crossing the street. Entropy is defined as ‚Äòlack of order and predictability‚Äô, which seems like an apt description of the difference between the two scenarios.

When is information useful?

Information is only useful when it can be stored and/or communicated. We have all learned this lesson the hard way when we have forgotten to save a document we were working on. In a digital form, information is stored in ‚Äòbits‚Äô, or a series of numbers that can either be 0 or 1. The letters in your keyboard are stores in a ‚Äòbyte‚Äô, which is 8 bits, which allows for 2‚Å∏ =256 combinations. It is important to know that information storage and communication are almost the same thing, as you can think of storage as communication with a hard disk.

Examples of symbols and their 8 digit codes

Information Storage

The mathematician Claude Shannon had the insight that the more predictable some information is, the less space is required to store it. Crossing the street is more predictable than Russian roulette, therefore you would need to store more information about the game of Russian roulette. Shannon had a mathematical formula for the ‚Äòentropy‚Äô of a probability distribution, which outputs the minimum number of bits required, on average, to store its outcomes.

Entropy

Formula from entropy from Wikipedia

Above is the formula for calculating the entropy of a probability distribution. It involves summing P*log(p) with base 2, for all the possible outcomes in a distribution. Here is a function to do this in Python:

import numpy as np

def entropy(dist):

su=0

for p in dist:

r= p/sum(dist)

if r==0:

su+=0

else:

su+= -r*(np.log(r))

return su/np.log(2)

Example: Russian Roulette

If we were to quantify the crossing the street example as having a 1 in a billion chance of death, and Russian roulette as 1 in 2, we‚Äôd get entropy([1, 999_999_999]) ‚âà 3.1*10^-8 bits , and entropy([50,50])=1 bit, respectively. This means that if we repeated both experiments a trillion times, it would take at least 31,000 bits to store the results of crossing the street, and 1 trillion bits to store the results of Russian roulette, in line with our earlier intuition.

Some distributions and their entropies

Example: English Language

The English language has 26 letters, if you assume each letter has a probability of 1/26 of being next, the language has an entropy of 4.7 bits. However, some letters are more common than other letters, and some letters appear often together, so through clever ‚Äòguessing‚Äô (i.e. not assigning probabilities of 1/26), we can be much more efficient.

Random guessing on average takes us 13.5 guesses to get the correct letter. Let us say we are given the first letter of every word in this sentence:

H_ _ /A_ _ /Y_ _ /D_ _ _ _ / M_ /F_ _ _ _ _?

It would be very bad if it took us 13.5*16=216 guesses to fill in the 16 blanks. It would likely take us less than an average of two guesses per blank to figure out the sentence is ‚ÄúHow are you doing my friend?‚Äù. So even if we exhaustively guessed the first letter and it took us 13.5 guesses, it would take us roughly 5.1 guesses/letter to fill in all the blanks, a huge improvement on random guessing.

Experiments by Shannon showed that English has an entropy between 0.6 and 1.3 bits. To put that into perspective, a 3 sided die has an entropy of 1.58 bits, and takes on average 2 guesses to predict. Also, note that the encoding system on your keyboard uses 8 bits per letter. So it could theoretically make all files in only the English language at least 6 times smaller!

Applications

Shannon‚Äôs work found uses in data storage, spaceship communication, and even communication over the internet. . Even if we are not working in any of those fields, ‚ÄòKL divergence‚Äô is an idea derived from Shannon‚Äôs work, that is frequently used in data science. It tells you how good one distribution is at estimating another by comparing their entropies.

Communication and storage of information is what has made humans great, and Shannon‚Äôs work revolutionised the way we do so in the digital age.
Photo by Alejandro Luengo on Unsplash

Apophenia or the ‚Äú tendency to mistakenly perceive connections and meaning between unrelated things‚Äù. Humans it turns out are quite rubbish at telling things that are random apart from things that are not. In fact we sometimes see patterns where there are none and for others they can say something is random but have no scientific way of explaining it. You see this most often when people think that lottery numbers ‚Äú1 2 3 4 5 6‚Äù are less likely to occur than say ‚Äú12 3 15 82 90‚Äù or that if a sequence of numbers has come up it is less likely to come up again.

Don‚Äôt believe me? Then have a go at the question below.

I have two binary strings generated by two processes:

0001011000111110011111000000100101111110110111111111010111111011010000010001000100110000000001100001 1010101001011011001011100001101010101111001000101001010010100101000101011011010101011101010101010101

If I told you one was made by a random process and another by a biased process, can you tell which is which?

Would you believe me if I said there was an easy way to find out?

How about if I told you there was an easy way to tell them apart and in a factual way?

I have in fact given this question to around 60 highly educated consultants and the most common answer was a gut estimate that the top one (string 1) seems less structured than the bottom, so string 2 is the non-random one.

I mean they are not wrong and what they were picking up on was the ‚ÄúFrequency Stability Property‚Äù (also another link here), but it seems a bit hard to convince someone with only an experience led decision. For example, if they said it was string 1 and I said it wasn‚Äôt how would we tell who was lying or not? We need some harder evidence. This is where this easy method I eluded to earlier comes into play.

If you were to count the number of 1‚Äôs and 0‚Äôs in the strings you would find they both have the same number of each and so tells you little. However, for any random process any sequence within it (e.g. ‚Äú11‚Äù or ‚Äú101‚Äù) should have equal probability compared to another sequence of the same length. This is because the number before in a string should not‚Ä¶
Written communication is found in every instance of our history ‚Äî dating back to our earliest days as a species. Prior to our discovery of agriculture & permanent settlements, nomadic tribes famously left behind stories of triumphs & hardships in primitive language structures known as proto-writing(s). Once we began settling, however, the utility of written communication rightly evolved. Previously, nomadic tribes feared nature most in the form of predatory animals & cataclysmic climate events; post-settlement, however, other tribes took the mantle as the largest threats.

Interactions among early settlements forced the birth of diplomacy, which, in turn, pushed communication & language to evolve. Specifically, it led to the necessity of secrecy & encryption. How could communities guard their most trusted secrets, or trust messengers carrying their most trusted secrets, without risking said secrets falling into the wrong hands?

Enter Cryptography

Building Blocks of Cryptography: Ciphers

Ciphers are arguably the basic building blocks of cryptography ‚Äî they‚Äôre certainly the earliest recorded implementations of encryption. In fancy crypto-lingo, a cipher is simply a series of steps that scrambles (encrypts) the original message, known as a plaintext, to a resultant message known as a ciphertext.

The earliest ciphers recorded in history were quite rudimentary, usually encrypted by hand or a simple mechanical device. In order to gain a deeper understanding of goals & principles behind ciphers, we‚Äôll walk through three of these simple, manually-encrypted ciphers of historical significance:

While these early examples provide ample learning opportunities & highlight key points, they‚Äôre light-years behind their modern counterparts. Powerful, publicly-known ciphers make up the cornerstone of communication security ‚Äî for example, the famous AES (Advanced Encryption Standard) that‚Äôs commonly used all‚Ä¶
Data Science is a field filled with fancy sounding things. Concepts both simple and complex get cool names, and let you make claims like being ‚ÄúPowered by AI‚Äù. While this isn‚Äôt necessarily a problem, it can mislead aspiring Data Scientists. Like any field, fancy names and complex concepts get much of the attention. This can help give the impression that cutting edge modeling is where the party is at. However, Data Science isn‚Äôt all about who‚Äôs got the most convolutional neural networks or the deepest learning. Crazy AI skills may prove valuable in certain situations, but Data Science is about picking the right tool for the job and using it effectively to solve real world problems. That last part, solving real world problems, should always the ultimate goal. Consistently hitting that goal is the foundation of a Data Science career.

To help form that foundation, there are three simple steps to follow,

Focus on solving real world problems, while measuring your solution. Pick the right tool for the job, which is probably the simplest. Repeat consistently.

Keep it simple

The right tool for the job is often the simplest one, at least at first. Complex models break, their behaviour can be hard to develop an intuition for, and implementing them is time consuming. Focus on simplicity and you‚Äôll start more projects, which will themselves probably have a higher success rate. When it comes to starting a career in Data Science, having a track record of providing real world value will give you a tremendous boost. An education in machine learning, statistics, or programming will provide you with an essential base of skills, but proving you can apply those skills to real world problems and communicate your solution effectively is far more valuable.

Starting simple projects and seeing them through to the end will help you build that track record. A project can start as simple as a SQL query, so try making a list of possible projects in and out of your company and having a go at each. Generating and measuring value should take precedent over almost everything. It‚Äôs important to remember that how you measure your solution depends on what problem you‚Äôre trying to solve. If I‚Äôm trying to create a mortgage approval model, I probably care more about correctly screening out fraudsters than mistakenly rejecting those with good intentions. How I measure my solution should reflect those priorities. This approach isn‚Äôt just the best way to build a portfolio; it‚Äôs a vital part of any Data Science work. Talk to stakeholders, get to the root of a problem, and find the best way to measure the value your solution provides.

Summary

If you‚Äôre looking to break into Data Science, find real world problems and aim to make a dent in them as quickly as you can. The simplest method will help you finish more of what you start, and get you to a result faster. It‚Äôs important to build a solid base of skills, but being able to show what impact you‚Äôve had with your skills is far more valuable than how exciting they sound.
Fire Your BI Team

The term ‚ÄúBusiness Intelligence‚Äù gained widespread popularity in the 1990s and was originally defined as ‚Äúconcepts and methods to improve business decision making by using fact-based support systems.‚Äù Clearly, this sounds like a strategically important function for any company.

So why the harsh headline? While the description above may capture what BI teams did in the 1990s, that‚Äôs not what BI teams have been up to in the last decade. As adoption and sophistication of data analytics changed, BI teams became the bottom-feeders of the analytical ecosystem while data scientists received all the fun and glory. BI teams ended up being order-takers executing precise requests prescribed by people who do not do analytics for a living. This has severely reduced their impact.

But it doesn‚Äôt have to be like this. And you don‚Äôt need to fire your BI team. We can restore BI‚Äôs place in the analytical ecosystem by changing their mandate and how they interact with stakeholders. This will lead to more impactful insights while ensuring that the analysts themselves feel professional ownership and purpose.

Let‚Äôs walk through why and how.

To see what‚Äôs wrong with BI today just consider the common, ticket-based engagement model: The BI analyst receives a ticket, executes the request, returns the answer, gets feedback through more tickets, and the cycle continues. A conversation may not even take place.

Now, if you‚Äôre someone who works with BI teams you might think this type of engagement model is exactly what you want. Fair enough ‚Äî it does sound efficient and organized.

But trust me, it‚Äôs not what you want.

Just think about the nature of the work required to extract meaningful and actionable insights. Even if the goal is to keep the analysis simple, you always end up on an analytical journey that involves slicing the data by segments, creating baselines, constructing derived metrics, and often more. This requires ownership and creativity from the analyst.

But in the BI engagement model, the analyst is merely following someone else‚Äôs train of thought. Ownership and creativity are lost.

So what‚Äôs the solution here? First, always kick off any ‚ÄúBI‚Äù analysis or dashboard with these two questions: (1) what‚Äôs the problem we‚Äôre trying to solve? and (2) what actions are we looking to take? From there, the onus is on the analyst to seek the question behind the question and share insights as they surface. Tickets will be replaced with face-to-face meetings and a symbiotic partnership will be formed. And if people can‚Äôt find time for that, then the question wasn‚Äôt important to begin with.

If you‚Äôve gotten this far, you might think I‚Äôm suggesting to convert all BI analysts into data scientists. But that‚Äôs not the case. While ETL and data exploration are core to any team that turns data into cashflows, there are some key differences:

Data scientists should (mainly) be focused on efforts that lead to ‚Äúdata products‚Äù based on advanced methods. This could be ML models that drive product recommendations, or pricing and matching algorithms.

The focus of the analysts, on the other hand, might be to deliver a slide deck with strategic recommendations or a dashboard. In fact, I favor the term ‚Äústrategic analytics‚Äù for these teams because it describes the purpose of their roles.

Both functions ‚Äî data science and strategic analytics ‚Äî are tremendously important, have clear purposes, require different competencies, and offer rewarding career paths. Most companies need both teams, but they don‚Äôt need a BI team. So if you‚Äôre managing a BI team, it‚Äôs time to change their mandate and the stakeholder engagement model. And let‚Äôs retire the term ‚ÄúBusiness Intelligence‚Äù once and for all.
Screenshot from a video made by Joseph Redmon on Youtube

We are a team of experts to help your business adopt AI solutions or build new AI products. Contact us at info@imaginaire.ai or visit our website https://www.imaginaire.ai

For full source code, please go to https://github.com/ethanyanjiali/deep-vision/tree/master/YOLO/tensorflow. I really appreciate your STAR that supports my efforts.

Foreword

When a self-driving car runs on a road, how does it know where are other vehicles in the camera image? When an AI radiologist reading an X-ray, how does it know where the lesion (abnormal tissue) is? Today, I will walk through this fascinating algorithm, which can identify the category of the given image, and also locate the region of interest. There‚Äôs plenty of algorithms introduced in recent years to address object detection in a deep learning approach, such as R-CNN, Faster-RCNN, and Single Shot Detector. Among those, I‚Äôm most interested in a model called YOLO ‚Äî You Only Look Once. This model attracts me so much, not only because of its funny name, but also some practical design that truly makes sense for me. In 2018, this latest V3 of this model had been released, and it achieved many new State of the Art performance. Because I‚Äôve programmed some GANs and image classification networks before, and also Joseph Redmon described it in the paper in a really easy-going way, I thought this detector would just be another stack of CNN and FC layers that just works well magically.

But I was wrong.

Perhaps it‚Äôs because I‚Äôm just dumber than usual engineers, I found it really difficult for me to translate this model from the paper to actual code. And even when I managed to do that in a couple of weeks (I gave up once put it away for a few weeks), I found it even more difficult for me to make it work. There‚Äôre so quite a few blogs, GitHub repos about YOLO V3, but most of them just gave a very high-level overview of the architecture, and somehow they just succeed. Even worse, the paper itself is too chill that it fails to provide many crucial details of implementation, and I have to read the author‚Äôs original C implementation (when is the last time did I write C? Maybe at college?) to confirm some of my guesses. When there‚Äôs a bug, I usually have no idea why it would occur. Then I end up manually debugging it step by step and calculating those formulas with my little calculator.

Fortunately, I didn‚Äôt give up this time and finally made it work. But in the meantime, I also felt really strongly that there should be a more thorough guide out there on the internet to help dumb people like me to understand every detail of this system. After all, if one single detail is wrong, the whole system would go south quickly. And I‚Äôm sure that if I don‚Äôt write these down, I would forget all these in few weeks too. So, here I am, presenting you this ‚ÄúDive Really Deep into YOLO V3: A Beginner‚Äôs Guide‚Äù. I hope you‚Äôll like it.

Prerequisite

Before getting into the network itself, I‚Äôll need to clarify with some prerequisites first. As a reader, you are expected to:

1. Understand the basics of Convolutional Neural Network and Deep Learning

2. Understand the idea of object detection task

3. Have curiosity about how the algorithm works internally

If you need help on first two items, there‚Äôre plenty of excellent resources like Udacity Computer Vision Nanodegree, Cousera Deep Learning Specialization and Stanford CS231n

If you just want to build something to detect some object with your custom dataset quickly, check out this Tensorflow Object Detection API

YOLO V3

YOLO V3 is an improvement over previous YOLO detection networks. Compared to prior versions, it features multi-scale detection, stronger feature extractor network, and some changes in the loss function. As a result, this network can now detect many more targets from big to small. And, of course, just like other single-shot detectors, YOLO V3 also runs quite fast and makes real-time inference possible on GPU devices. Well, as a beginner to object detection, you might not have a clear image of what do they mean here. But you will gradually understand them later in my post. For now, just remember that YOLO V3 is one of the best models in terms of real-time object detection as of 2019.

Network Architecture

Diagram by myself

First of all, let‚Äôs talk about how this network look like at a high-level diagram (Although, the network architecture is the least time-consuming part of implementation). The whole system can be divided into two major components: Feature Extractor and Detector; both are multi-scale. When a new image comes in, it goes through the feature extractor first so that we can obtain feature embeddings at three (or more) different scales. Then, these features are feed into three (or more) branches of the detector to get bounding boxes and class information.

Darknet-53

The feature extractor YOLO V3 uses is called Darknet-53. You might be familiar with the previous Darknet version from YOLO V1, where there‚Äôre only 19 layers. But that was like a few years ago, and the image classification network has progressed a lot from merely deep stacks of layers. ResNet brought the idea of skip connections to help the activations to propagate through deeper layers without gradient diminishing. Darknet-53 borrows this idea and successfully extends the network from 19 to 53 layers, as we can see from the following diagram.

Diagram from YOLOv3: An Incremental Improvement

This is very easy to understand. Consider layers in each rectangle as a residual block. The whole network is a chain of multiple blocks with some strides 2 Conv layers in between to reduce dimension. Inside the block, there‚Äôs just a bottleneck structure (1x1 followed by 3x3) plus a skip connection. If the goal is to do multi-class classification as ImageNet does, an average pooling and a 1000 ways fully connected layers plus softmax activation will be added.

However, in the case of object detection, we won‚Äôt include this classification head. Instead, we are going to append a ‚Äúdetection‚Äù head to this feature extractor. And since YOLO V3 is designed to be a multi-scaled detector, we also need features from multiple scales. Therefore, features from last three residual blocks are all used in the later detection. In the diagram below, I‚Äôm assuming the input is 416x416, so three scale vectors would be 52x52, 26x26, and 13x13. Please note that if the input size is different, the output size will differ too.

Diagram by myself

Multi-scale Detector

Once we have three features vectors, we can now feed them into the detector. But how should we structure this detector? Unfortunately, the author didn‚Äôt bother to explain this part this his paper. But we could still take a look at the source code he published on Github. Through this config file, multiple 1x1 and 3x3 Conv layers are used before a final 1x1 Conv layer to form the final output. For medium and small scale, it also concatenates features from the previous scale. By doing so, small scale detection can also benefit from the result of large scale detection.

Diagram by myself

Assuming the input image is (416, 416, 3), the final output of the detectors will be in shape of [(52, 52, 3, (4 + 1 + num_classes)), (26, 26, 3, (4 + 1 + num_classes)), (13, 13, 3, (4 + 1 + num_classes))]. The three items in the list represent detections for three scales. But what do the cells in this 52x52x3x(4+1+num_classes) matrix mean? Good questions. This brings us to the most important notion in pre-2019 object detection algorithm: anchor box (prior box).

Anchor Box

The goal of object detection is to get a bounding box and its class. Bounding box usually represents in a normalized xmin, ymin, xmax, ymax format. For example, 0.5 xmin and 0.5 ymin mean the top left corner of the box is in the middle of the image. Intuitively, if we want to get a numeric value like 0.5, we are facing a regression problem. We may as well just have the network predict for values and use Mean Square Error to compare with the ground truth. However, due to the large variance of scale and aspect ratio of boxes, researchers found that it‚Äôs really hard for the network to converge if we just use this ‚Äúbrute force‚Äù way to get a bounding box. Hence, in Faster-RCNN paper, the idea of an anchor box is proposed.

Anchor box is a prior box that could have different pre-defined aspect ratios. These aspect ratios are determined before training by running K-means on the entire dataset. But where does the box anchor to? We need to introduce a new notion called the grid. In the ‚Äúancient‚Äù year of 2013, algorithms detect objects by using a window to slide through the entire image and running image classification on each window. However, this is so inefficient that researchers proposed to use Conv net to calculate the whole image all in once (technically, only when your run convolution kernels in parallel.) Since the convolution outputs a square matrix of feature values (like 13x13, 26x26, and 52x52 in YOLO), we define this matrix as a ‚Äúgrid‚Äù and assign anchor boxes to each cell of the grid. In other words, anchor boxes anchor to the grid cells, and they share the same centroid. And once we defined those anchors, we can determine how much does the ground truth box overlap with the anchor box and pick the one with the best IOU and couple them together. I guess you can also claim that the ground truth box anchors to this anchor box. In our later training, instead of predicting coordinates from the wild west, we can now predict offsets to these bounding boxes. This works because our ground truth box should look like the anchor box we pick, and only subtle adjustment is needed, whhich gives us a great head start in training.

Diagram by myself

In YOLO v3, we have three anchor boxes per grid cell. And we have three scales of grids. Therefore, we will have 52x52x3, 26x26x3 and 13x13x3 anchor boxes for each scale. For each anchor box, we need to predict 3 things:

1. The location offset against the anchor box: tx, ty, tw, th. This has 4 values.

2. The objectness score to indicate if this box contains an object. This has 1 value.

3. The class probabilities to tell us which class this box belongs to. This has num_classes values.

In total, we are predicting 4 + 1 + num_classes values for one anchor box, and that‚Äôs why our network outputs a matrix in shape of 52x52x3x(4+1+num_classes) as I mentioned before. tx, ty, tw, th isn‚Äôt the real coordinates of the bounding box. It‚Äôs just the relative offsets compared with a particular anchor box. I‚Äôll explain these three predictions more in the Loss Function section after.

Anchor box not only makes the detector implementation much harder and much error-prone, but also introduced an extra step before training if you want the best result. So, personally, I hate it very much and feel like this anchor box idea is more a hack than a real solution. In 2018 and 2019, researchers start to question the need for anchor box. Papers like CornerNet, Object as Points, and FCOS all discussed the possibility of training an object detector from scratch without the help of an anchor box.

Loss Function

With the final detection output, we can calculate the loss against the ground truth labels now. The loss function consists of four parts (or five, if you split noobj and obj): centroid (xy) loss, width and height (wh) loss, objectness (obj and noobj) loss and classification loss. When putting together, the formula is like this:

Loss = Lambda_Coord * Sum(Mean_Square_Error((tx, ty), (tx‚Äô, ty‚Äô) * obj_mask)

+ Lambda_Coord * Sum(Mean_Square_Error((tw, th), (tw‚Äô, th‚Äô) * obj_mask)

+ Sum(Binary_Cross_Entropy(obj, obj‚Äô) * obj_mask) + Lambda_Noobj * Sum(Binary_Cross_Entropy(obj, obj‚Äô) * (1 -obj_mask) * ignore_mask)

+ Sum(Binary_Cross_Entropy(class, class‚Äô))

It looks intimidating but let me break them down and explain one by one.

xy_loss = Lambda_Coord * Sum(Mean_Square_Error((tx, ty), (tx‚Äô, ty‚Äô)) * obj_mask)

The first part is the loss for bounding box centroid. tx and ty is the relative centroid location from the ground truth. tx‚Äô and ty‚Äô is the centroid prediction from the detector directly. The smaller this loss is, the closer the centroids of prediction and ground truth are. Since this is a regression problem, we use mean square error here. Besides, if there‚Äôs no object from the ground truth for certain cells, we don‚Äôt need to include the loss of that cell into the final loss. Therefore we also multiple by obj_mask here. obj_mask is either 1 or 0, which indicates if there‚Äôs an object or not. In fact, we could just use obj as obj_mask, obj is the objectness score that I will cover later. One thing to note is that we need to do some calculation on ground truth to get this tx and ty. So, let‚Äôs see how to get this value first. As the author says in the paper:

bx = sigmoid(tx) + Cx

by = sigmoid(ty) + Cy

Here bx and by are the absolute values that we usually use as centroid location. For example, bx = 0.5, by = 0.5 means that the centroid of this box is the center of the entire image. However, since we are going to compute centroid off the anchor, our network is actually predicting centroid relative the top-left corner of the grid cell. Why grid cell? Because each anchor box is bounded to a grid cell, they share the same centroid. So the difference to grid cell can represent the difference to anchor box. In the formula above, sigmoid(tx) and sigmoid(ty) are the centroid location relative to the grid cell. For instance, sigmoid(tx) = 0.5 and sigmoid(ty) = 0.5 means the centroid is the center of the current grid cell (but not the entire image). Cx and Cy represents the absolute location of the top-left corner of the current grid cell. So if the grid cell is the one in the SECOND row and SECOND column of a grid 13x13, then Cx = 1 and Cy = 1. And if we add this grid cell location with relative centroid location, we will have the absolute centroid location bx = 0.5 + 1 and by = 0.5 + 1. Certainly, the author won‚Äôt bother to tell you that you also need to normalize this by dividing by the grid size, so the true bx would be 1.5/13 = 0.115. Ok, now that we understand the above formula, we just need to invert it so that we can get tx from bx in order to translate our original ground truth into the target label. Lastly, Lambda_Coord is the weight that Joe introduced in YOLO v1 paper. This is to put more emphasis on localization instead of classification. The value he suggested is 5.

Diagram from YOLOv3: An Incremental Improvement

wh_loss = Lambda_Coord * Sum(Mean_Square_Error((tw, th), (tw‚Äô, th‚Äô)) * obj_mask)

The next one is the width and height loss. Again, the author says:

bw = exp(tw) * pw

bh = exp(th) * ph

Here bw and bh are still the absolute width and height to the whole image. pw and ph are the width and height of the prior box (aka. anchor box, why there‚Äôre so many names). We take e^(tw) here because tw could be a negative number, but width won‚Äôt be negative in real world. So this exp() will make it positive. And we multiply by prior box width pw and ph because the prediction exp(tw) is based off the anchor box. So this multiplication gives us real width. Same thing for height. Similarly, we can inverse the formula above to translate bw and bh to tx and th when we calculate the loss.

obj_loss = Sum(Binary_Cross_Entropy(obj, obj‚Äô) * obj_mask) noobj_loss = Lambda_Noobj * Sum(Binary_Cross_Entropy(obj, obj‚Äô) * (1 ‚Äî obj_mask) * ignore_mask)

The third and fourth items are objectness and non-objectness score loss. Objectness indicates how likely is there an object in the current cell. Unlike YOLO v2, we will use binary cross-entropy instead of mean square error here. In the ground truth, objectness is always 1 for the cell that contains an object, and 0 for the cell that doesn‚Äôt contain any object. By measuring this obj_loss, we can gradually teach the network to detect a region of interest. In the meantime, we don‚Äôt want the network to cheat by proposing objects everywhere. Hence, we need noobj_loss to penalize those false positive proposals. We get false positives by masking prediciton with 1-obj_mask. The `ignore_mask` is used to make sure we only penalize when the current box doesn‚Äôt have much overlap with the ground truth box. If there is, we tend to be softer because it‚Äôs actually quite close to the answer. As we can see from the paper, ‚ÄúIf the bounding box prior is not the best but does overlap a ground truth object by more than some threshold we ignore the prediction.‚Äù Since there are way too many noobj than obj in our ground truth, we also need this Lambda_Noobj = 0.5 to make sure the network won‚Äôt be dominated by cells that don‚Äôt have objects.

class_loss = Sum(Binary_Cross_Entropy(class, class‚Äô) * obj_mask)

The last loss is classification loss. If there‚Äôre 80 classes in total, the class and class‚Äô will be the one-hot encoding vector that has 80 values. In YOLO v3, it‚Äôs changed to do multi-label classification instead of multi-class classification. Why? Because some dataset may contains labels that are hierarchical or related, eg woman and person. So each output cell could have more than 1 class to be true. Correspondingly, we also apply binary cross-entropy for each class one by one and sum them up because they are not mutually exclusive. And like we did to other losses, we also multiply by this obj_mask so that we only count those cells that have a ground truth object.

To fully understand how this loss works, I suggest you manually walk through them with a real network prediction and ground truth. Calculating the loss by your calculator (or tf.math) can really help you to catch all the nitty-gritty details. And I did that by myself, which helped me find lots of bugs. After all, the devil is in the detail.

Implementation

If I stop writing here, my post will just be like another ‚ÄúYOLO v3 Review‚Äù somewhere on the web. Once you digest the general idea of YOLO v3 from the previous section, we are now ready to go explore the remaining 90% of our YOLO v3 journey: Implementation.

Framework

At the end of September, Google finally released TensorFlow 2.0.0. This is a fascinating milestone for TF. Nevertheless, new design doesn‚Äôt necessarily mean less pain for developers. I‚Äôve been playing around TF 2 since very early of 2019 because I always wanted to write TensorFlow code in the way I did for PyTorch. If it‚Äôs not because of TensorFlow‚Äôs powerful production suite like TF Serving, TF lite, and TF Board, etc., I guess many developers will not choose TF for new projects. Hence, if you don‚Äôt have a strong demand for production deployment, I would suggest you implement YOLO v3 in PyTorch or even MXNet. However, if you made your mind to stick with TensorFlow, please continue reading.

TensorFlow 2 officially made eager mode a first-tier citizen. To put it simply, instead of using TensorFlow specific APIs to calculate in a graph, you can now leverage native Python code to run the graph in a dynamic mode. No more graph compilation and much easier debugging and control flow. In the case where performance is more important, a handy tf.function decorator is also provided to help compile the code into a static graph. But, the reality is, eager mode and tf.function are still buggy or not well documented sometimes, which makes your life even harder in a complicated system like YOLO v3. Also, Keras model isn‚Äôt quite flexible, while the custom training loop is still quite experimental. Therefore, the best strategy for you to write YOLO v3 in TF 2 is to start with a minimum working template first, and gradually add more logic to this shell. By doing so, we can fail early and fix the bug before it hides too deeply in a giant nested graph.

Dataset

Aside from the framework to choose, the most important thing for successful training is the dataset. In the paper, the author used MSCOCO dataset to validate his idea. Indeed, this is a great dataset, and we should aim for a good accuracy on this benchmark dataset for our model. However, a big dataset like this could also hide some bugs in your code. For example, if the loss is not dropping, how do you know if it just needs more time to converge, or your loss function is wrong? Even with GPU, the training is still not fast enough for you to quickly iterate and fix things. Therefore, I recommend you to build a development set which contains tens of images to make sure your code looks ‚Äúworking‚Äù first. Another option is to use VOC 2007 dataset, which only has 2500 training images. To use MSCOCO or VOC2007 dataset and create TF Records, you could refer to my helper scripts here: MSCOCO, VOC2007

Preprocessing

Preprocessing stands for the operations to translate raw data into a proper input format of the network. For the image classification task, we usually just need to resize the image, and one-hot encode the label. But things are a bit more complicated for YOLO v3. Remember I said the output of the network is like 52x52x3x(4+1+num_classes) and has three different scales? Since we need to calculate the delta between ground truth and prediction, we also need to format our ground truth into such a matrix first.

For each ground truth bounding box, we need to pick the best scale and anchor for it. For example, a tiny kite in the sky should be in the small scale (52x52). And if the kite is more like a square in the image, we should also pick the most square-shaped anchor in that scale. In YOLO v3, the author provides 9 anchors for 3 scales. All we need to do is to choose the one that matches our ground truth box the most. When I implement this, I thought I need the coordinates of the anchor box as well to calculate IOU. In fact, you don‚Äôt need to. Since we just want to know which anchor fits our ground truth box best, we can just assume all anchors and the ground truth box share the same centroid. And with this assumption, the degree of matching would be the overlapping area, which can be calculated by min width * min height.

During the transformation, one could also add some data augmentation to increase the variety of training set virtually. For example, typical augmentation includes random flipping, random cropping, and random translating. However, these augmentations won‚Äôt block you from training a working detector, so I won‚Äôt cover much about this advanced topic.

Training

After all these discussions, you finally have a chance to run ‚Äúpython train.py‚Äù and start your model training. And this is also when you meet most of your bugs. You could refer to my training script here when you are blocked. Meanwhile, I want to provide some tips that are helpful for my own training.

NaN Loss

Check your learning rate and make sure it‚Äôs not too high to explode your gradient. Check for 0 in binary cross-entropy because ln(0) is not a number. You can clip the value from (epsilon, 1 ‚Äî epsilon). Find an example and walk through your loss step by step. Find out which part of your loss goes to NaN. For example, if width/height loss went to NaN, it could be because the way you calculate from tw to bw is wrong.

Loss remains high

Try to increase your learning rate to see if it can drop faster. Mine starts at 0.01. But I‚Äôve seen 1e-4 and 1e-5 works too. Visualize your preprocessed ground truth to see if it makes sense. One problem I had before is that my output grid is in [y][x] instead of [x][y], but my ground truth is reversed. Again, manually walk through your loss with a real example. I had a mistake of calculating cross-entropy between objectness and class probabilities. My loss also remains around 40 after 50 epochs of MSCOCO. However, the result isn‚Äôt that bad. Double-check the coordinates format throughout your code. YOLO requires xywh (centroid x, centroid y, width and height), but most of dataset comes as x1y1x2y2 (xmin, ymin, xmax, ymax). Double-check your network architecture. Don‚Äôt get misled by the diagram from a post called ‚ÄúA Closer Look at YOLOv3 ‚Äî CyberAILab‚Äù. tf.keras.losses.binary_crossentropy isn‚Äôt the sum of binary cross-entropy you need.

Loss is low, but the prediction is off

Adjusting lambda_coord or lambda_noobj to the loss based on your observation. If you are traininig on your own dataset, and the dataset is relative small (< 30k images), you should intialize weights from a COCO pretrained model first. Double-check your non max suppression code and adjust some threshold (I‚Äôll talk about NMS later). Make sure your obj_mask in the loss function isn‚Äôt mistakenly taking out necessary elements. Again and again, your loss function. When calculating loss, it uses relative xywh in a cell (also called tx, ty, tw, th). When calculating ignore mask and IOU, it uses absolute xywh in the whole image, though. Don‚Äôt mix them up.

Loss os low, but there‚Äôs no prediction

If you are using a custom dataset, please check the distribution of your ground truth boxes first. The amount and quality of the boxes could really affect what the network learn (or cheat) to do. Predict on your training set to see if your model can overfit on the training set at least.

Multi-GPU training

Since the object detection network has so many parameters to train, it‚Äôs always better to have more computing power. However, TensorFlow 2.0 doesn‚Äôt have great support over multi-GPU training so far. To do that in TF, you‚Äôll need to pick a training strategy like MirroredStrategy, as I did here. Then wrap your dataset loader into a distributed version too. One caveat for distributed training is that the loss coming out of each batch should be divided by the global batch size because we are going to `reduce_sum` over all GPU results. For example, if the local batch size is 8, and there‚Äôre 8 GPUs, your batch loss should divide a global batch size of 64. Once you summed up losses from all replica, the final result will be the average loss of a single example.

Postprocessing

The final component in this detection system is a post-processor. Usually, postprocessing is just about trivial things like replacing machine-readable class id with human-readable class text. In object detection, though, we have one more crucial step to do to get final human-readable results. This is called non maximum suppression.

Let‚Äôs recall our objectness loss. When is false proposal has great overlap with ground truth, we won‚Äôt penalize it with noobj_loss. This encourages the network to predict close results so that we can train it more easily. Also, although not used in YOLO, when the sliding window approach is used, multiple windows could predict the same object. In order to eliminate these duplicate results, smart researchers designed an algorithm called non maximum supression (NMS).

Photo by Python Lessons from Analytics Vidhya

The idea of NMS is quite simple. Find out the detection box with the best confidence first, add it to the final result, and then eliminates all other boxes which have IOU over a certain threshold with this best box. Next, you choose another box with the best confidence in the remaining boxes and do the same thing over and over until nothing is left. In the code, since TensorFlow needs explicit shape most of the time, we will usually define a max number of detection and stop early if that number is reached. In YOLO v3, our classification is not mutually exclusive anymore, and one detection could have more than one true class. However, some existing NMS code doesn‚Äôt take that into consideration, so be careful when you use them.

Conclusion

YOLO v3 is a masterpiece in the rising era of Artificial Intelligence, and also an excellent summary of Convolution Neural Network techniques and tricks in the 2010s. Although there‚Äôre many turn-key solutions like Detectron out there to simplify the process of making a detector, a hands-on experience in coding such sophisticated detector is really a great learning opportunity for machine learning engineers because merely reading the paper is far from enough. Like Ray Dalio said about his philosophy:

Pain plus reflection equals progress.

I hope my article could be a lighthouse in your painful journey of implementing YOLO v3, and perhaps you can also share the delightful progress with us later. If you like my article or my source code of YOLO v3, please ‚≠êstar‚≠ê my repo and that will be the biggest support for me.

References
This article is your one-stop solution to every possible question related to activation functions that can come into your mind that are used in deep learning models. These are basically my notes on activation functions and all the knowledge that I have about this topic summed together in one place. So, without going into any unnecessary introduction, let‚Äôs get straight down to business.

Contents

What is an activation function and what does it do in a network? Why is there a need for it and why not use a linear function instead? What are the desirable features in an activation function? Various non-linear activations in use Notable non-linear activations coming out of latest research How (and which) to use them in deep neural networks

What is an activation function?

Simply put, an activation function is a function that is added into an artificial neural network in order to help the network learn complex patterns in the data. When comparing with a neuron-based model that is in our brains, the activation function is at the end deciding what is to be fired to the next neuron. That is exactly what an activation function does in an ANN as well. It takes in the output signal from the previous cell and converts it into some form that can be taken as input to the next cell. The comparison can be summarized in the figure below.

Source: cs231n by Stanford

Why is there a need for it?

There are multiple reasons for having non-linear activation functions in a network.

Apart from the biological similarity that was discussed earlier, they also help in keeping the value of the output from the neuron restricted to a certain limit as per our requirement. This is important because input into the activation function is W*x + b where W is the weights of the cell and the x is the inputs and then there is the bias b added to that. This value if not restricted to a certain limit can go very high in magnitude especially in case of very deep neural networks that have millions of parameters. This will lead to computational issues. For example, there are some activation functions (like softmax) that out specific values for different values of input (0 or 1). The most important feature in an activation function is its ability to add non-linearity into a neural network. To understand this, let‚Äôs consider multidimensional data such as shown in the figure below:

A linear classifier using the three features(weight, Systolic Blood Pressure and Age in this figure) can give us a line through the 3-D space but it will never be able to exactly learn the pattern that makes a person a smoker or a non-smoker(the classification problem in hand) because the pattern that defines this classification is simply not linear. In come the artificial neural networks. What if we use an ANN with a single cell but without an activation function. So our output is basically W*x + b. But this is no good because W*x also has a degree of 1, hence linear and this is basically identical to a linear classifier.

What if we stack multiple layers. Let‚Äôs represent n·µó ∞ layer as a function f‚Çô(x). So we have:

o(x) = f‚Çô(f‚Çô‚Çã‚ÇÅ(‚Ä¶.f‚ÇÅ(x))

However, this is also not complex enough especially for problems with very high patterns such as that faced in computer vision or natural language processing.

In order to make the model get the power (aka the higher degree complexity) to learn the non-linear patterns, specific non-linear layers (activation functions) are added in between.

Desirable features of an activation function

Vanishing Gradient problem: Neural Networks are trained using the process gradient descent. The gradient descent consists of the backward propagation step which is basically chain rule to get the change in weights in order to reduce the loss after every epoch. Consider a two-layer network and the first layer is represented as f‚ÇÅ(x) and the second layer is represented as f‚ÇÇ(x). The overall network is o(x) = f‚ÇÇ(f‚ÇÅ(x)). If we calculate weights during the backward pass, we get o`(x) = f‚ÇÇ(x)*f‚ÇÅ`(x). Here f‚ÇÅ(x) is itself a compound function consisting of Act(W‚ÇÅ*x‚ÇÅ + b‚ÇÅ) where Act is the activation function after layer 1. Applying chain rule again, we clearly see that f‚ÇÅ`(x) = Act(W‚ÇÅ*x‚ÇÅ + b‚ÇÅ)*x‚ÇÅ which means it also depends directly on the activation value. Now imagine such a chain rule going through multiple layers while backpropagation. If the value of Act() is between 0 and 1, then several such values will get multiplied to calculate the gradient of the initial layers. This reduces the value of the gradient for the initial layers and those layers are not able to learn properly. In other words, their gradients tend to vanish because of the depth of the network and the activation shifting the value to zero. This is called the vanishing gradient problem. So we want our activation function to not shift the gradient towards zero. Zero-Centered: Output of the activation function should be symmetrical at zero so that the gradients do not shift to a particular direction. Computational Expense: Activation functions are applied after every layer and need to be calculated millions of times in deep networks. Hence, they should be computationally inexpensive to calculate. Differentiable: As mentioned, neural networks are trained using the gradient descent process, hence the layers in the model need to differentiable or at least differentiable in parts. This is a necessary requirement for a function to work as activation function layer.

Various non-linear activations in use

Sigmoid: The sigmoid is defined as:

This activation function is here only for historical reasons and never used in real models. It is computationally expensive, causes vanishing gradient problem and not zero-centred. This method is generally used for binary classification problems.

Softmax : The softmax is a more generalised form of the sigmoid. It is used in multi-class classification problems . Similar to sigmoid, it produces values in the range of 0‚Äì1 therefore it is used as the final layer in classification models.

: The softmax is a more generalised form of the sigmoid. It is used in . Similar to sigmoid, it produces values in the range of 0‚Äì1 therefore it is used as the final layer in classification models. Tanh: The tanh is defined as:

If you compare it to sigmoid, it solves just one problem of being zero-centred.

ReLU: ReLU (Rectified Linear Unit) is defined as f(x) = max(0,x):

This is a widely used activation function, especially with Convolutional Neural networks. It is easy to compute and does not saturate and does not cause the Vanishing Gradient Problem. It has just one issue of not being zero centred. It suffers from ‚Äúdying ReLU‚Äù problem. Since the output is zero for all negative inputs. It causes some nodes to completely die and not learn anything.

Another problem with ReLU is of exploding the activations since it higher limit is, well, inf. This sometimes leads to unusable nodes.

Leaky ReLU and Parametric ReLU: It is defined as f(x) = max(Œ±x, x)

the figure is for Œ± = 0.1

Here Œ± is a hyperparameter generally set to 0.01. Clearly, Leaky ReLU solves the ‚Äúdying ReLU‚Äù problem to some extent. Note that, if we set Œ± as 1 then Leaky ReLU will become a linear function f(x) = x and will be of no use. Hence, the value of Œ± is never set close to 1. If we set Œ± as a hyperparameter for each neuron separately, we get parametric ReLU or PReLU.

ReLU6: It is basically ReLU restricted on the positive side and it is defined as f(x) = min(max(0,x),6)

This helps to stop blowing up the activation thereby stopping the gradients to explode(going to inf) as well another of the small issues that occur with normal ReLUs.

The idea that comes into one‚Äôs mind is why not combine ReLU6 and a LeakyReLU to solve all known issues that we have with previous activation functions. Popular DL frameworks do not provide an implementation of such an activation function but I think this would be a good idea.

Notable non-linear activations coming out of latest research

Swish: This was proposed in 2017 by Ramachandran et.al. It is defined as f(x) = x*sigmoid(x).

It is slightly better in performance as compared to ReLU since its graph is quite similar to ReLU. However, because it does not change abruptly at a point as ReLU does at x = 0, this makes it easier to converge while training.

But, the drawback of Swish is that it is computationally expensive. To solve that we come to the next version of Swish.

Hard-Swish or H-Swish: This is defined as:

The best part is that it is almost similar to swish but it is less expensive computationally since it replaces sigmoid (exponential function) with a ReLU (linear type).

How to use them in deep neural networks?

Tanh and sigmoid cause huge vanishing gradient problems . Hence, they should not be used.

. Hence, they should not be used. Start with ReLU in your network . Activation layer is added after the weight layer (something like CNN, RNN, LSTM or linear dense layer) as discussed above in the article. If you think the model has stopped learning, then you can replace it with a LeakyReLU to avoid the Dying ReLU problem. However, the Leaky ReLU will increase the computation time a little bit.

. Activation layer is added after the weight layer (something like CNN, RNN, LSTM or linear dense layer) as discussed above in the article. If you think the model has stopped learning, then you can replace it with a LeakyReLU to avoid the Dying ReLU problem. However, the Leaky ReLU will increase the computation time a little bit. If you also have Batch-Norm layers in your network, that is added before the activation function making the order CNN-Batch Norm-Act . Although the order of Batch-Norm and Activation function is a topic of debate and some say that the order doesn‚Äôt matter, I use the order mentioned above just to follow the original Batch-Norm paper.

. Although the order of Batch-Norm and Activation function is a topic of debate and some say that the order doesn‚Äôt matter, I use the order mentioned above just to follow the original Batch-Norm paper. Activation functions work best in their default hyperparameters that are used in popular frameworks such as Tensorflow and Pytorch. However, one can fiddle with the negative slope in LeakyReLU and set it to 0.02 to expedite learning.

THAT‚ÄôS ALL FOLKS üòÉ

I have tried to solve every possible question related to activation functions, however, if I have missed something please comment down below.

You can look at more things related to deep learning on my Github and follow me on Linkedin.

Some of my previous articles:
Creating data pipelines by writing spark jobs is nowadays easier due to the growth of new tools and data platforms that allow multiple data parties (analysts, engineers, scientists, etc.) to focus on understanding data and writing logic to get insights. Nevertheless, new tools like notebooks that allow easy scripting, sometimes are not well used and could cause a new problem: extensive data pipelines are written as simple SQL queries or scripts, neglecting important development concepts as writing clean and testable code. Thus, design principles that ensure that code is maintainable and extensible might be broken, leading to further problems in an environment where our products should be dynamic.

We will expose a process that contains a set of steps and patterns that will help you in creating better spark pipelines using a basic spark pipeline as an example.

Step 1: Define first your pipeline structure

The first and most important step (even more than code cleanliness) in every pipeline is the definition of its structure. Thus, the pipeline structure should be defined after a process of data exploration that provides the phases need to produce the expected outputs from the inputs.

Let‚Äôs work on a basic example and define a pipeline structure. Therefore, we have three datasets:

usersSourceExampleDS that contains users‚Äô information.

genderSourceAExampleDS reads from the source ‚ÄòexampleA‚Äô that contains genders by a specif name in a country.

genderSourceBExampleDS reads from the source ‚ÄòexampleB‚Äô and contains another list of genders by a name. However, in this case, it does not discriminate by the country and adds a computed probability.

Then, the pipeline aim is to produce a dataset where the column gender is added to the usersSourceExampleDS as follow:

When the suffix has an explicit gender, for example, Mr or Ms add the gender right away in the column gender.

If the suffix does not have the gender then search the name on the genderSourceAExampleDS and add a new column source_a_gender . Then, search the name on the genderSourceBExampleDS and add the column source_b_gender.

and add a new column . Then, search the name on the and add the column Finally, when source_a_gender is not null set this value to gender, otherwise use the source_b_gender only if the probability is greater than 0.5.

Also, some metrics like male percentage and female percentage are produced into the metrics storage system.

Pipeline structure

The following phases are defined:

Data Reading: reads from the data sources. In this case, data is stored in S3 .

reads from the data sources. In this case, data is stored in . Data Pre-processing: As we can see in the data, there is not unique ID to join or search data, then texts within the columns name, country and suffix are used. However, these columns have invalid data to be removed (NaN), multiple letter cases, acronyms, and special characters that are pre-processed in this phase.

As we can see in the data, there is not unique to join or search data, then texts within the columns name, country and suffix are used. However, these columns have invalid data to be removed (NaN), multiple letter cases, acronyms, and special characters that are pre-processed in this phase. Data Enrichment: After data is clean and ready to be used, it is transformed and parsed within this phase. Thus, new columns are added after some business validations.

After data is clean and ready to be used, it is transformed and parsed within this phase. Thus, new columns are added after some business validations. Data Metrics: This phase contains tasks related to aggregations and calculations over the transformed data.

This phase contains tasks related to aggregations and calculations over the transformed data. Data Writing: Finally, this phase handles writes of the genderized results to S3 and the metrics to an external tool.

Is it quite simple? Yes, We know what you are thinking. This pipeline might be written easily using spark. You open your spark context, read the datasets, parse the datasets, finally join to get the final the results and write them to the output datastores. This looks like this.

Writing pipeline with SQL queries

This script works fairly well; it uses some extensive spark SQL and repeats the same logic for 3 different sources though. Besides, let‚Äôs imagine the following scenario:

This is only the first version of the pipeline and multiple new genderized sources are going to be added in the future.

Multiple new shared pre-processing and transforming tasks are going to be added.

Some cleaning functions have to be tested in isolation. As an example, the function that removes acronyms. Also, the output of each intermediate step should be tested.

New extensions and tests will be automatically configured with CI/CD.

As a result, the previous source code becomes useful for some cases where an effort for extensibility is needless; else becomes non-extensible, untestable and unmaintainable. Consequently, the following steps are recommended.

Step 2: Guarantee a proper project structure

Define a project structure that suits your pipelien phases, this will make your pipeline a data product.

A) Define a project structure for phases defined previously, this looks like this:

B) Define your project dependencies avoiding circular/unused dependencies and define clearly the dependency scope between test, provided and compile.

C) Start adding a general helper within each package where small and general functions are going to be defined. Define the rest of the functions in a companion object of the main class for now.

D) Define which version of the spark API you will use: RDDs, datasets, or dataframes depending on your requirements.

E) When you have multiple jobs using this same logic, think about creating a general spark-utils library that is shared for all your company jobs.

Step 3: Ensure Clean Code Rules Compliance

Creating jobs that behave as expected is not enough. Currently, companies cope to reduce technical debt and creating self-documented pipelines that are easy to read and maintain for all the data parties.

There are myths to be discussed here: ‚ÄúI write code as a data scientist‚Äù or ‚Äúdata engineers are too strict to read code‚Äù. We should start writing code for humans and ensure a minimum of cleanliness that guarantees our data teams‚Äô productivity. Thus, important rules to take into account are listed below.

